[{"title":"Continuous delivery","indexable_content":"Deployment The deployment pipeline The commit stage Shared sandbox environment Specialist testing environments Production environment add improvements fix bugs test expectations about your end product configuration management (how you maintain consistency with your product’s performance and functionality) the automation of your build, deploy, test and release processes commit stage shared sandbox environment specialist testing environment production environment the commit stage the shared sandbox environment any necessary specialist testing    Test all iterations of your software, either through public user testing or automated testing (using separate software to perform the testing).  Continuous delivery is about producing regular iterations of your software that’s ready to be released (deployed), though you don’t have to release these iterations to the public. Producing regular iterations makes it easier for you to: If your software isn’t ready to be used then it’s not creating any real value. Treat it as stock piling up – and stock is waste. The Lean software development philosophy tells us to: Test all iterations of your software, either through public user testing or automated testing (using separate software to perform the testing). Automate your deployment process so you are forced to fully understand it. Then any issues with moving code from your version control system into production (when it’s gone live) can be dealt with early on. Automating it early also means that code will be tested and any bugs fixed so that releases become frequent, low-risk, almost boring events. Don’t plan production release slots far in advance. You can’t be certain what will be ready in 6 months’ time. Make your release planning simpler – make sure it’s flexible enough that any feature or update to your software can be deployed when it’s ready. Then, if a feature needs to miss a release slot, it can easily be rescheduled for tomorrow or next week, rather than in 6 months, or more if the next slot is already full. Another advantage is being able to quickly respond to security patches or similar changes in underlying  libraries or frameworks used by your application. You can quickly make a change and watch the update flow through the various gates in your deployment pipeline, confident that nothing has been broken. What happens to code between it being written by a developer and deployed to production is known as the deployment pipeline. Understand your end-to-end deployment pipeline. Knowing how it works and how each element works together will have implications for: The deployment pipeline has 4 stages: When your developer checks into version control (where all code, including previous versions, are stored), a range of tests should be run against the latest version of the code. Any quick, easy-to-identify defects, like compile errors or unit test failures will be found at this stage. If the tests pass, the code progresses to the next stage and should be considered for release into production. Send the code to a shared sandbox (testing) environment. This is the first environment where the application is deployed and run. It’s also the first stage where it can be visually inspected for quality by anybody on the team. Make the sandbox environment as similar to the production (live) version as far as is practical. For example, if production uses Postgres, the sandbox should also use Postgres and not another database like MySQL or sqlite. The purpose is to find any defects in the code. If a defect is found, stop the version of the code at this stage. If it passes the sandbox environment it can move on to further specialist testing environments. You may need to perform additional testing for specialist requirements, like load and performance testing, penetration testing, or accessibility testing. The amount of environments you’ll need will depend on the requirements and conditions of your individual projects. When you are satisfied with the quality of the code, move it to the live production environment. Your code is ready to go live if it has passed: Deploy to production the same way as you deploy to any other environment – use the same scripts, same configuration management tooling, and the same version of the code.  This means you’re not releasing code for the first time – you’re performing a task that’s been validated at each stage throughout the deployment pipeline.","description":"Continuous delivery is about producing regular iterations of your software that’s ready to be released (deployed), though you don’t have to release these iterations to the public.","link":"/service-manual/agile/continuous-delivery.html"},{"title":"Features of agile","indexable_content":"Sprints Standard meetings Daily stand-up Sprint planning Sprint review Retrospectives User stories Testing in agile the length of the project – use shorter sprints for shorter projects, giving you greater visibility of what’s going on, the opportunity to iterate and the flexibility to adaptively plan team members that are new to the Scrum process:     shorter sprints means those that are new to Scrum get used to the regular meeting formats and processes faster      shorter sprints means those that are new to Scrum get used to the regular meeting formats and processes faster how often you release code to production  - if it’s only released at the end of each sprint decide how often do you want to ship production-ready code get the development and working environments set up agree some of the design principles (technical, product, interaction, content) prepare the product backlog for the first sprint daily stand-ups sprint planning sprint reviews retrospectives what I worked on/produced yesterday what I am working on today (and help I might need) what’s blocking me (ie stopping me from finishing a user story card) understand the story and acceptance criteria agree on the number of user stories they’ll aim to achieve within each sprint agree on the tasks needed to complete it what we will do? how we will do it? Some common features of the agile development methods we’ve used at GDS. Sprints are part of an agile process that uses Scrum, a framework for teams developing a product. Sprints are about planning what you and your team will achieve and when you will achieve it by. Each member of your team should have a task to complete within each sprint. Each sprint is of equal length and usually lasts between 1 to 2 weeks, but you can use longer or shorter sprints in your project. When deciding the length of your sprints, consider: Start the process with a sprint 0. This sprint is about preparing your team for the first sprint. Use a sprint 0 to: While there are other agile methodologies that don’t rely on sprinting, such as Kanban, it’s common for teams to start with Scrum. Agile processes have 4 different types of regular meetings: Have daily meetings with your team lasting no more than 15 minutes. It’s best if you do it standing up, in a semicircle, in front of your project wall. This will help you to keep it short and allows your team members to point at user story cards on the wall to keep things on topic. Each member of your team should answer: Ask people to keep it brief and don’t be afraid to remind them of this. If people try to solve issues during the stand-up, stop the conversation and arrange a huddle after the stand-up to discuss in a smaller group. Sprint planning is done at the start of each sprint. It requires user stories to have been written in advance with acceptance criteria. The product owner should read out the stories and explain the acceptance criteria in order of priority. It’s the job of the team to: A good description of sprint planning is on the Agile Learning Labs website, explaining its 2 parts: This meeting can be hard to get right with large teams. Some people want to dig deep and question every story; others want to keep moving and don’t want to go into detail. Persevere! This is a chance for team members to demonstrate the work they’ve produced during the sprint. You can include stakeholders in this meeting and also use it to inform stakeholders which user stories haven’t been completed and why. These meetings are important opportunities to iterate the team’s working process. Learn how to run retrospectives. User stories are essential to the development of your service and the user experience, so it’s important that you know how to write them well. Learn how to write a user story. Testing in agile is about constantly improving the quality of your service. Learn about Agile testing methods.","description":"Some common features of the agile development methods we’ve used at GDS.","link":"/service-manual/agile/features-of-agile.html"},{"title":"Quality","indexable_content":"Defining quality Everyone is responsible for quality Testing Team roles and quality assurance specialists A note on technical debt accessibility for (the widest possible range of) users, across an appropriate range of devices how simple it is to interact with the service the ability of the service provider to quickly provide appropriate support whether the storage and handling of any data is suitable and secure the strength and security of essential software and the infrastructure making sure the code is well-tested seeing if it responds quickly when dealing with lots of users at once (load testing) if it can be scaled rapidly to handle unusual amounts of traffic the ability of the team to quickly add or change features to respond to changing requirements or settings testing in an agile environment accessibility testing testing code load and performance testing vulnerability and penetration testing testing what is being built doing the basics, so assumptions about accessibility can be regularly tested taking the time to consider failure scenarios and how to respond penetration testing - having people outside of the team carry out this task helps challenge assumptions and identify weaknesses, giving a valuable outside perspective hiring quality assurance specialists – they can make sure all quality-related activities are coordinated, and that your team gets the training and resources needed to make a high-quality service by:     having a clear remit and working with your team to build quality into everything they do, instead of just adding a gate review to the development process       being a short-term role, leaving your team capable of managing quality as part of their standard development and iteration of your service      having a clear remit and working with your team to build quality into everything they do, instead of just adding a gate review to the development process being a short-term role, leaving your team capable of managing quality as part of their standard development and iteration of your service Put quality at the forefront of your project if you want people to use and enjoy your service. Quality is the responsibility of every member of your team – the people who create the service create the quality of it. Every person on your project should be part of increasing quality and fixing issues. Quality will mean different things to different members of the team. Fundamentally it is about the experience of the service’s users, from the start to the end of the transaction, which may include: ‘Technical debt’ is a common term in software development, and definitions vary. Generally, technical debt is used to describe compromises made to complete a product’s development, instead of creating a clean, expandable technical design. These compromises mean there’s still work to be done, even though the overall product is finished. It’s impossible to develop your software without building up some technical debt, so make sure your team have some way of sharing an understanding of the technical debt in their system. A large amount of technical debt will slow down future development.  Know how much of it you have and you can prioritise work to reduce it, and can continue to iterate rapidly in the future. Service quality isn’t just a testing issue or the responsibility of a few individuals. The quality of a system is defined by the people who create it.  Your team should be able to see any problems in the quality of your system. Every person on the project should be taking action to increase quality and fix issues. You won’t know how great your product is or if it meets your criteria without testing it – in both normal and unusual conditions. Dylan Robert’s book “Learning From First Responders” is worth reviewing as an example of testing software and teams under unusual stresses (in this case, the final stages of the 2012 presidential campaign). This manual gives further information on testing: The quality of any digital service is the responsibility of the entire team, but the final responsibility lies with the service manager. Make sure your service manager works with the rest of your team. Otherwise you risk your team not understanding what measures they need to put in place to guarantee quality. Your team will need to consider quality when writing user stories, and allow time and resources to be used for: Using specialist skills and facilities is a good way to make sure testing is thorough, and is typically used for:","description":"Put quality at the forefront of your project if you want people to use and enjoy your service.","link":"/service-manual/agile/quality.html"},{"title":"Running retrospectives","indexable_content":"Retrospectives The facilitator Working agreements Outcomes of a retrospective Template Why we do this Further reading Setting the scene: 00:00 to 00:05 (5 mins) Actions from the previous retrospective: 00:05 to 00:10 (5 mins) The good: 00:10 to 00:20 (10 mins) Discussion: 00:20 to 00:30 (10 mins) The bad: 00:30 to 00:45 (15 mins) Discussion: 00:45 to 01:05 (20 mins) Actions: 01:05 to 01:20 (15 mins) gather data generate insights decide what to do plan the retrospective make sure that everyone gets a chance to contribute keep the retrospective on track make sure actions are created and assigned manage time so that it does not run over everyone contributes no one speaks over the other (except for the facilitator) no phones or laptops are allowed – everyone should be concentrating on the discussion be concrete and measurable (eg ‘write 10 more unit tests for the redirector’, or ‘speak to Jamie about arranging a project retrospective’; not ‘write more tests’, or ‘we should understand the lessons learned from this project’) have a date by which they should be completed be assigned to a specific person (and not to ‘the team’) not be assigned to someone who is not present they still need to be done why haven’t they been completed – set a new deadline if necessary, but don’t keep carrying actions over what should we keep doing? why did those things go well and what can we learn? are there any actions we can draw out? can we work out why these things went badly? can we work out what we need to do to improve matters, or prevent specific things happening again? can we draw out specific actions that someone here can take that will help? make small improvements often, ideally before problems start to fester identify working practices that make you more efficient, productive or at least happier The Agile Retrospective Wiki – it has some very useful resources, including plans for different types of retrospectives. Agile Retrospectives: Making Good Teams Great. A central principle of agile is quick feedback loops – you demonstrate something to the user as soon as possible so you can see how well it suits their needs. Retrospectives are the way we apply this to our own teams to find out what’s working and what isn’t, so a team can continuously improve.  A retrospective is a meeting at the end of a sprint where your team get a chance to talk about what went well and badly in that sprint, and take some actions to improve matters. It can also cover a larger scope, eg a full project retrospective. A retrospective takes this form: This is a chance for everyone in your team to contribute to improving process/productivity.  All retrospectives must be facilitated. The facilitator’s role is to give everyone a chance to talk about their concerns and give positive feedback.  At the same time, they make sure the meeting remains a structured, productive meeting and doesn’t become overly negative. Ideally, your facilitator will be someone outside of your team so your whole team can contribute, but it’s not essential. The facilitator needs to: You’ll find it helpful to have some working agreements for a retrospective. These can be stated if necessary, eg in the first retrospective your team has. Working agreements could be that:  During the discussion you will uncover some successes you can celebrate, as well as some problems that you can fix or things you can improve. Make a list of actions that will address these. Aim to have done them within the next sprint or iteration. Some problems may take longer to fix, in which case you should try to make an action that will start the process of improving it by your next retrospective. Actions should: Retrospectives should follow up on the actions of previous retrospectives to make sure they’ve been completed. If they’re consistently not getting done, you may have too many.  You can use this template for your retrospectives. It’s based on a team of 8 to 10 and covers a 2-week sprint. 90 minutes is a reasonable amount of time to use for this scope and amount of people. Each of the activities is timeboxed (has a set time it will run for), and it’s your facilitator’s job to make sure that your team stick to this. Build in about 10% ‘shuffle time’ to move between activities to make sure it doesn’t overrun. Explain the scope and, if necessary, purpose. If your team don’t know each other and/or are shy, include brief introductions. Make sure they’ve been completed. If any haven’t, ask if:  Give your team around 10 minutes to write down all the things that went well in the previous 2 weeks on post-it notes. If anonymity is important to encourage free expression, collect them in and add them to the wall yourself. If not, have the team take turns adding their own post-its to the wall, possibly saying a few words about each. Don’t allow this to develop into a discussion at this point – you just want to gather data. Group the post-its into common themes. If there are too many to discuss in the time you have then have the team prioritise, eg by voting with stickers. Discuss each of the main areas in turn: Give the team around 15 minutes to write post-its for anything that went badly. Again, group the post-its, prioritise if necessary, and discuss the main areas:  Spend some time looking at the actions identified; assign them to people present and set realistic deadlines for completion. Total: 80 minutes with 10% shuffle time. It’s OK to finish early if people have said what they need to. It’s not OK to overrun – if there is too much to say, have the team prioritise the top areas for discussion and/or book more time for the next retrospective. Having a regular retrospective means you can: Agile development practices will help you work better, and the retrospective allows you to fine-tune the process and your environment to your needs. You may find these resources useful: Retrospective plan photo by Anna Shipman. Voting with stickers photo by Pete Herlihy. All other photographs by Paul Downey.","description":"A central principle of agile is quick feedback loops – you demonstrate something to the user as soon as possible so you can see how well it suits their needs. Retrospectives are the way we apply this to our own teams to find out what’s working and what isn’t, so a team can continuously improve.","link":"/service-manual/agile/running-retrospectives.html"},{"title":"What agile looks like","indexable_content":"Understand your users “What do you want next Friday? What have we learned last week?” Small, agile teams Fail fast Continuous planning Common situations to look out for give value to their users and stakeholders regularly shorten feedback loops that could be longer if using a waterfall methodology (where you only move on to the next phase when the phase you’re working on is complete) think about what features are the next most important to produce direct their efforts on creating usable software product manager – a role probably performed by the Service manager) responsible for delivering return on investment, usually by creating products that users love delivery manager (aka Scrum master or project manager) – the agile expert responsible for removing blockers (things slowing a team down), they also act as a facilitator at team meetings team members – self-organising and multi-disciplinary, they produce user stories, carry out the product manager’s vision and are responsible for estimating their output and speed produce better software solutions encourage better quality controls spread knowledge across the team improve quality improve visibility reduce cost to market releasing working software to your users regularly – it allows you to get feedback quickly and hear or see what they think; if the product is wrong you can easily change direction and iterate demonstrating value to your sponsor with regular releases – if your software is rarely released you run the risk of creating a ‘too-big-to-fail’ service that shouldn’t be released, but must be released anyway  checking your teams’ progress – if your teams’ speed is still inconsistent after the initial 4 to 6 sprints, then something needs fixing (possibly unknown complications or poor estimation with timings) using test-driven development (writing tests before you develop the features to be tested) to highlight issues with quality early on – establish the issues, baseline metrics, and monitor throughout the project release level – identify and prioritise the features you must have, and would like to have by the deadline iteration level – plan for the next features to implement, in priority order (if features are too large to be estimated or delivered within a single iteration, break them down further) the progress of the previous sprint any new facts and requirements your core team isn’t full-time or is working on multiple projects – your team is the unit of delivery and you need 100%, so push back on managers and stakeholders if this is happening you don’t have a dedicated team area – sit your team together, preferably in your own room, with space on the walls to draw ideas and stick up cards and post-its     rearrange your workspace or use tools in innovative ways to improve your teams’ working environment and increase productivity – you’ll challenge some longstanding working practices, but this is very important      rearrange your workspace or use tools in innovative ways to improve your teams’ working environment and increase productivity – you’ll challenge some longstanding working practices, but this is very important there’s no continuous integration/development environment – if your teams aren’t insisting on this from the beginning you’ve probably got the wrong team:     iterative software development is, in many areas, dependent on the ability to continuously deploy and run automated tests      iterative software development is, in many areas, dependent on the ability to continuously deploy and run automated tests you have a separate quality assurance (QA) department – if your team pass software they’ve developed over to a QA department, they’ve got the wrong attitude to delivering production-ready software; embed a quality culture into the team Agile can be a liberating way of working. It won’t stop you from using existing skills and knowledge, but it will require your team, users and stakeholders to start working together in new ways.  Prioritise features for users over everyone else – including your big, scary stakeholders – and ask for their feedback early and often. Really listen to your users. Even when they tell you things you don’t want to hear or disagree with. If possible, use data from real people that are using your product and let it influence the direction of your project. Constantly put users first.  Iterate often. Build something that strives to achieve the most valuable user need and show it to the users, listen to their feedback and improve it. Keep doing this until you have something so useful that they wouldn’t be without it. It might sound like over-simplifying the complexity of software development and project management, but agile development is all about “what do you want by next Friday?” The process of producing incremental, production-ready software allows your team to: Run a retrospective at the end of each delivery cycle or sprint to review what worked or what could be improved. Your team will continue to learn through delivery cycles and improve throughout the project.  Small teams of between 5 to 10 people are often more productive and predictable than larger teams. Forget man-days (the amount of work produced by an average worker in a day) and think about your team as a unit. A good team includes members with all of the skills necessary to successfully produce software. A fully functioning team has 3 main roles: Encourage your team members to pair up, as working together is beneficial. 2 people working on 1 thing will: A good team means you’re able to estimate your output, or speed, very effectively and consistently. You can then plan much more accurately.  Regularly releasing little pieces of code will: Agile techniques don’t guarantee success – you can still fail! But these techniques do allow you to spot problems earlier on and resolve them. You can resolve issues, and stop issues from happening, by: Don’t be afraid to fail or experiment. Learn to fail, and create a culture that learns from failure.  It’s a myth that you don’t plan on agile projects.  The freedom of agile projects does not come free: you have to plan. You just plan differently and continuously. Base your agile planning on solid, historical data, not theories or opinions. Your plan must continuously demonstrate its accuracy: nobody on your agile project should take it for granted that the plan is workable. Your teams should plan together, on at least 2 levels: Review these plans after every sprint and adjust them based on:  If your team is new to agile, be wary of familiar situations and reactions from having to do things differently. These situations have a bad smell about them and will undermine your project and its chances of success: This is by no means a complete list, but these are most common things to watch out for.","description":"Agile can be a liberating way of working. It won’t stop you from using existing skills and knowledge, but it will require your team, users and stakeholders to start working together in new ways.","link":"/service-manual/agile/what-agile-looks-like.html"},{"title":"Writing user stories","indexable_content":"User story cards Working outside-in using goals Taking a rain check – the promise of a conversation Acceptance criteria in user stories Stories only work in an agile team How to get user stories Further reading Structure Goal Actor Narrative the person using the service (actor) what the user needs the service for (narrative) why the user needs it (goal) relevant team members subject matter experts stakeholders title actor narrative goal why do they want to use your service? what are they trying to achieve? what need has motivated them to seek out your service? in what context do they use it – at home/work/on a mobile phone/whilst caring for a child? how often do they use it? faster more accurate than written documentation allows developers to build up a detailed mental model of the user goals, workflows, constraints, and the many issues which must be taken into account when building a software system story writing workshops – more commonly at the start of a project, the development team and stakeholders will get together to write stories user interviews with real users – ideally, you will set up a user panel which the development team have ongoing access to user representatives embedded within your team – this may include the service manager or product owner observation – watch real users using your service Mastering the Requirements Process, 3rd Ed, Suzanne Robertson & James Robertson, 2012 Agile Alliance, “Agile – What is it?”, http://www.jconne.com/agile1whatisit/, accessed 7/1/2013 Twelve Principles of the Agile Manifesto – http://agilemanifesto.org/principles.html User Stories Applied, Mike Cohn, 2004. Free chapter on “Writing User Stories” available at http://www.mountaingoatsoftware.com/books/user-stories-applied A user story briefly explains: User stories are an essential part of the agile toolkit. They’re a way of organising your work into manageable chunks that create tangible value, and can be discussed and prioritised independently. User stories are a technique which relies on other agile practices, including continuous delivery, and face-to-face communication with user representatives. It’s not sufficient to simply ‘set’ a user representative: that user representative needs to be in the same place as the team and available to the team a sufficient amount of time. User stories can be added to a product backlog at any point in the sprint cycle by any person in the team. It’s up to the product owner to coordinate and prioritise them, and to select stories for each sprint at the start of each sprint cycle. Discuss the stories ahead of sprint planning with:  A user story is represented through a story card that has a title and a few lines of text. Your story cards can be virtual, as well as actual cards. On a large product/service keep your stories in a digital format, and then turn them into physical cards as part of sprint planning. When writing a user story, make sure the story is well-formed. Don’t skip the part explaining why there’s a need for a service just because it can be difficult. You might want to include a list of acceptance criteria. These should be a reminder for things to test or check which may have come up during conversation, but they should not be used as a way of defining the scope of a story. If stories are too big then split them into smaller stories as they stand more chance of producing shippable (ready to use) code. Story cards follow a standard structure: They don’t capture every detail, but you should have a more in-depth discussion about a user story at the appropriate time. Building useful software systems is hard, so you need to make sure you’re solving the right problem. Agile methodologies emphasise an outside-in approach – what outcome is your service user trying to achieve? If you dive into the solution without a good understanding of your users, you risk solving the wrong problem, or coming up with solutions which don’t really work for your users in the real world. That’s why the most important part of a user story is the goal. Use this to help you decide whether the story is “done” or delivered, ie does the work meet the goal of the user? When writing stories with your development team, always start by thinking about and discussing your users’ goals: Suzanne and James Robertson have excellent advice on this in the book Mastering the Requirements Process (3rd edition). Being specific about the actor will help you to break down interactions into logical chunks. Sometimes the actor will be a user of your service, or the actor will be an administrator, technician or manager in your organisation. Make sure you already have a good understanding of your users from your initial project work or existing research. If not, take the time to develop that understanding. Use this as a reminder of the main interaction that needs to be addressed as part of the user experience. Remember that the story card does not need to spell out every detail. Agile teams prefer face-to-face conversation over detailed documentation. Face-to-face is: The story card is just a placeholder, a promise to have a conversation when the time is right. Use the story card and some brief initial conversations to estimate the amount of time a story needs to be completed and then put it into an agile backlog. When development work actually starts, consult the users or user representative to fill out the story details. A user story is the sum of all of these conversations, sketches and whiteboard diagrams – not just the card. You don’t need to write down or archive your conversations, but translate them directly into automated tests and working code. Using user stories in this way allows you to avoid ‘analysis paralysis’, the painful condition of trying to guess the details of some far-future goal. Acceptance criteria can be used to determine when a story is done. These follow a conversation between developer and user or user representative and are considered before coding begins. Only include these on the back of the story card if the team finds them useful for recording user assumptions they might later forget. Sometimes writing acceptance criteria on the story card is useful where the user or user representative is not immediately available but is no substitute at all for face-to-face conversation. The success of a user story is dependent on regular face-to-face communication between developers and users or user representatives. Your service manager and other user representatives need to be available to developers every day, and have enough time to think through and answer questions. Don’t underestimate how time-consuming this work can be! Stories can come from many places, but the most common sources include: See Chapter 4 of User Stories Applied.","description":"A user story briefly explains:","link":"/service-manual/agile/writing-user-stories.html"},{"title":"Assisted digital action plan","indexable_content":"Discovery: understanding your assisted digital users and their needs Alpha: designing assisted digital provision Beta: testing and measuring your service Live: continuous development Beta: before GOV.UK Beta: on GOV.UK meet the Service Standard at each development phase understand your assisted digital user volumes and needs develop assisted digital support that meets user needs and provides good value for money How many of your users do you estimate will require assisted digital support and what is the proportion of total users? What are the demographic characteristics of your assisted digital users? What are your existing support channels for assisted digital users and how are you likely to provide support in future? This should be in line with GDS assisted digital guidance and the Government Approach to Assisted Digital. What do initial assisted digital user journeys and personas look like for your service? What is your plan for developing your understanding of assisted digital user needs and provision during the alpha phase? confidence with using computers and the internet experience access to an online device trust in sharing personal information online full physical or cognitive ability your existing users who’ll need assisted digital support in the future people in your organisation who work with assisted digital users organisations who support people offline to access services organisations who are already providing formal and informal support to your users How many of your users do you estimate will require assisted digital support and what is the proportion of total users? What barriers do your assisted digital users need to overcome (eg skills, access, confidence, motivation) to use the digital service independently? What is your plan to test your assisted digital support before the digital service moves to GOV.UK? Your tests should be in line with GDS guidance and the Government Approach to Assisted Digital. The results should help you develop your assisted digital support so that it’s ready to be fully tested during beta. What are the user journeys for your service’s assisted digital user personas (including ID Assurance if required)? Include estimated volumes for relevant channels:     web chat       telephone support       face by face support on the high street       face by face support at home (outreach)      web chat telephone support face by face support on the high street face by face support at home (outreach) What research have you conducted and what feedback have you received on your user journeys from users and/or experts? How will you fund your assisted digital support so that it’s sustainable or is the support free to the user? How will you gather user insights and use them to iterate your assisted digital support in beta? The specific needs of users who may require assisted digital support (including accessibility). This must be established by evidence-based research. Your plan to fully test assisted digital support in beta. This should include:     timelines       details of how assisted digital support will be provided, by channel       forecast volume, during testing and when live – this should also be split by channel       forecast costs in live, by channel       showing assisted digital support that is joined up and consistent with relevant central government transactions       showing how digital inclusion will be incorporated into your support       evidence that all assisted digital support meets legal requirements, including language requirements, Data Protection Act, accessibility etc      timelines details of how assisted digital support will be provided, by channel forecast volume, during testing and when live – this should also be split by channel forecast costs in live, by channel showing assisted digital support that is joined up and consistent with relevant central government transactions showing how digital inclusion will be incorporated into your support evidence that all assisted digital support meets legal requirements, including language requirements, Data Protection Act, accessibility etc How you’ll gather user insights in beta (in testing and/or pilots) and use them to iterate your assisted digital support for live. What have you done to test your assisted digital support? What evidence do you have that your assisted digital support meets users’ needs and provides good value for money? Show that:     volumes and costs for different channels are in line with estimates       users are aware of assisted digital support and can access it easily       your assisted digital support is consistently and clearly branded as a government service       wait times are low for all support channels       a high volume of users successfully complete the transaction       the volume of assisted digital users has decreased       the service is trusted       feedback from users and experts is positive, with good end-to-end user experience       your users have a joined-up and consistent assisted digital support across central government transactions       all assisted digital support meets relevant legal requirements, including language requirements, Data Protection Act, accessibility      volumes and costs for different channels are in line with estimates users are aware of assisted digital support and can access it easily your assisted digital support is consistently and clearly branded as a government service wait times are low for all support channels a high volume of users successfully complete the transaction the volume of assisted digital users has decreased the service is trusted feedback from users and experts is positive, with good end-to-end user experience your users have a joined-up and consistent assisted digital support across central government transactions all assisted digital support meets relevant legal requirements, including language requirements, Data Protection Act, accessibility What will you do to iterate your assisted digital support in live, based on what you have learnt from your testing? What is your plan for ongoing evaluation and iteration of assisted digital support? Include how you will measure performance for the following:     user satisfaction       increase in users’ digital skills and/or confidence to use digital services unaided       decrease in the volume of people using assisted digital support      user satisfaction increase in users’ digital skills and/or confidence to use digital services unaided decrease in the volume of people using assisted digital support All new digital services must demonstrate appropriate [assisted digital user research](/service-manual/assisted-digital/assisted-digital-user-research, and show that suitable support is in place to meet their needs, to meet the Digital by Default Service Standard. The information in this document will help you to: The discovery phase will give you a high-level understanding of assisted digital user needs and what your assisted digital support will look like. GDS’s guidance on assisted digital user research will support you in developing your service throughout all stages. Do some work to understand the demographic makeup (eg gender, age, disability status, region) of who needs your assisted digital support. The Digital Landscape Research provides a cross-population view of who is offline in the UK. You should also look at current users of offline channels for your department’s services. Once you have an initial understanding of who your assisted digital users are, start to understand their needs. Work with people already supporting your users to complete digital transactions. They might be staff in your department, organisations who help people develop digital skills, or intermediaries such as charities, lawyers, libraries, Citizens Advice Bureau, JobCentre Plus, or other commercial agents. GDS regularly meet a panel of user experts and have published information on work carried out with these groups on the GDS blog. Existing assisted digital support may be informal and uncoordinated. Think about how you will make sure there’s formal, funded and sustainable assisted digital support for all users who need it. This support should cover all relevant channels, such as web chat, telephone, face by face (on the high street or in the user’s location). You should start to develop user stories and personas based on assisted digital user needs. These will help you visualise specific needs and user journeys. By the end of the discovery phase, you should have developed a plan to answer these questions: Your understanding of the number of users who may need support and their specific needs will be at an early stage and may have some gaps. As you design your service in alpha, you’ll be able to fill these gaps and iterate your initial answers. The objective of the alpha phase is to design a support model with which to test the appropriateness of your support and your understanding of user needs. At this point, you should formalise your thinking on assisted digital and create an assisted digital plan. This will state how you will provide assisted digital to the standards set out in the GDS assisted digital guidance. The guidance includes key performance indicators (KPIs) for your assisted digital support, so you will need to include ways to measure your service. During the alpha stage, you should understand your assisted digital users’ current ability, confidence and barriers to completing your service independently. You need to find out if they lack: You’ll need to continue developing user personas and user journeys based on assisted digital user needs. You should involve: You’ll need to put suitable plans in place to support your assisted digital users if your service requires Identity Assurance. You should also consider any impacts on or overlaps with other central government transactions, to make sure that the user journey is consistent and joined up across services, where relevant. Assisted digital support should follow shared models which are currently being developed and tested by GDS and exemplar services. By the end of the alpha stage, you’ll need to be able to answer these 7 questions to be allowed to proceed to beta: During the beta phase, you will fully test and complete end-to-end assisted digital support so you have accurate metrics and measurements in place. You should continue to do user research and sufficiently test your assisted digital support with users in the early stage of beta. You should involve experts and intermediaries in your testing. GDS can support this by introducing you to organisations we work with. You should test the whole of your assisted digital user journey, looking at each point where the user interacts digitally in the transaction and testing the planned support at that point. Before your digital service moves onto GOV.UK, you’ll need to demonstrate: When your digital service is in beta and on GOV.UK, it’s vital that your plans are fully tested on real assisted digital users before your service proceeds to the live stage. Testing should cover all channels that are relevant for your assisted digital users and should be in line with GDS assisted digital guidance. You should establish ways to get ongoing feedback from users about your assisted digital support and to monitor key performance indicators (KPIs). This will help you to provide evidence that your service will meet user needs, is high quality and provides good value for money. Use this information to iterate the assisted digital support and continue to ask for feedback from users, providers and experts. This will make sure the support is suited to user needs throughout the beta and live phases. You should share your findings with GDS and other service managers. By the end of the beta stage, you’ll need to be able to answer these questions to proceed to live: Your digital transformation project must meet the Service Standard before it goes live. Your project can go live only after you’ve thoroughly tested your assisted digital support and can prove that it meets user needs and provides good value for money. This is not the end of the process. The service should now be improved continuously, based on user feedback, KPIs and further user research. You should have plans in place to provide training or guidance for your assisted digital users so that many are able to complete transactions independently in future, through digital inclusion. You may also need to close down services if your former offline channels are being replaced. You should be able to show your service’s assisted digital information on your Performance Platform dashboard.","description":"All new digital services must demonstrate appropriate [assisted digital user research](/service-manual/assisted-digital/assisted-digital-user-research, and show that suitable support is in place to meet their needs, to meet the Digital by Default Service Standard.","link":"/service-manual/assisted-digital/action-plan.html"},{"title":"Researching assisted digital users","indexable_content":"Researching assisted digital users – a guide for service managers Assisted digital users What services need to know about their assisted digital users Accessing information on assisted digital users Relating user insight to assisted digital support design Researching users who are offline or have low digital skills across government in the long term Conducting new research Using existing research Understand the complexity of your service Understand your users’ levels of digital skills Develop an understanding of which channels for assisted digital support your users may need who are my assisted digital users what is a good service for my assisted digital users won’t ever be able to use the digital service independently and will always need assisted digital support could use the digital service independently, but will need assisted digital support at the beginning to build their confidence in using the service should use the digital service (ie have the digital skills, but currently choose not to use the digital channel) and don’t need assisted digital support the estimated number of assisted digital users for your service the demographic characteristics of your assisted digital users (eg gender, age, disability status (physical and/or cognitive), language, region, other) what channels your assisted digital users currently use (eg face by face, telephone) why they currently use these channels (eg for trust, familiarity, known expertise) which other government services your assisted digital users use where your assisted digital users currently go for support with using your service what parts of your service users find difficult to complete (pain points) what barriers your assisted digital users need to overcome (eg skills, access, confidence, motivation) to be able to use the digital service independently your assisted digital users’ support needs and preferences if your assisted digital users need assurance that they’re using an official assisted digital service the number of your assisted digital users that have accessibility needs for the assisted digital service specifically the details of your users’ accessibility needs ongoing or recent research on the offline users of your service looking at existing data and insight on your offline users using the existing channels that currently support your services, eg voluntary and community sector organisations, in-house suppliers, contact centres using locations where vulnerable users may feel more comfortable about answering questions, eg libraries, community centres, religious centres, and places where support groups meet speaking with staff and volunteers as well as the end-users of your services considering employing the services of user research recruitment agencies to recruit to a specific but relevant research brief ‘applying/ renewing a passport at the Post Office’ ‘applying for Carer’s Allowance’ Digital Landscape Research (GDS, 2012) Oxford Institute Survey (PDF, 2013) Office of National Statistics (2013) Ofcom Disability Consumer Report (2013) Third Sector research, eg Age UK (2014), Citizens Advice BBC Media Literacy: Understanding Digital Capabilities (PDF, 2012) share the Digital Inclusion Scale with colleagues working on user research and digital services in your department use the scale to understand the complexity of your service and your users’ levels of digital skill (see below) provide feedback on the usefulness of the scale to the GDS Assisted Digital team what documentation the user needs to complete the service if the user needs to have an email address to use it the difficulty level of the questions the user needs to answer to complete the service the level of digital skill (and other requisite skills - eg literacy or being able to understand technical language/policy) needed to complete your digital transaction why departments should conduct ongoing research on users who are offline or have low digital skills what you should be finding out through this research how to conduct this research (based on the recommendations of our long-term research proposal) how you can use it, to improve your assisted digital and digital services Services need to conduct and collate research on their assisted digital users as part of developing their requirements for assisted digital support. Doing this research is the department’s responsibility.  User research will allow services and departments to answer these questions: An assisted digital user is someone unable to use a digital by default service independently. Assisted digital support will be provided for these users so that they can use digital by default services. Assisted digital users include people who are offline, and people who are online but genuinely can’t complete a digital by default transaction by themselves.  18% of UK adults are offline (ie never or rarely use the internet) and GDS’ Digital Landscape Research provides a demographic breakdown of who is online and who is offline. Go ON UK estimates that 11 million people in the UK aged 15 and over don’t have Basic Online Skills (eg sending an email, performing a simple search). Every service will have a different proportion of users who are offline or who are unable to complete a digital transaction independently. Understanding your users’ needs is vital for putting in place appropriate assisted digital support. Consider the digital skills of your users. This will help you understand, out of those not using digital channels, which users: Remember, assisted digital and digital inclusion are separate, but related. All digital by default services need to know: Information on your assisted digital users should come from: Combining new research with analysis of existing research will let you build up the most accurate picture possible of your assisted digital users. Identify opportunities to engage with your users by: Our research so far has indicated that service providers and assisted digital users don’t talk about ‘government services’ as such, and aren’t always aware of which central government departments administer them.  We’ve found it’s more relevant to ask users about services more colloquially, for example: Read through your existing research to pull out the answers to the questions above, and to identify which questions existing research is unable to answer.  Look for existing external research relevant to your service to support your research. There’s a lot of useful research resources on offline users, for example: It will be important to understand the Digital Inclusion Scale and the definitions, when they are published this month. From then you will be able to: It’s important to understand: You should build on our initial analysis with your own more specific and detailed knowledge, expertise and research on your service. Understand your users’ levels of digital skills by using your data and insight about them to map them onto the Digital Inclusion Scale, when available.  Your user data and insight should also be used to create personas which represent the different assisted digital user types for your service. The assisted digital team has developed personas that apply across government services, which you can also use to better understand your users (contact the assisted digital team for more information) You can then assess how much support your assisted digital users will need to fill the gap between their skills and the skills needed to complete your specific service. Use the demographic data and insight you collect on your users to develop an understanding of which channels for assisted digital support your users may need and why. You should then map the journeys for each of your user personas through your proposed assisted digital channels. This will allow you to check that your proposed provision will meet your users’ needs. Low levels of literacy (verbal and written) may impact on the ability to communicate effectively. Face by face support can help people overcome these barriers by providing a safe and comfortable environment in which they can express themselves. It’s sensible for research on users who are offline or have low digital skills to be coordinated across government and to be aligned between assisted digital and digital inclusion in the long term. GDS is working to develop a long-term proposal for ongoing research on users who are offline or have low digital skills. We will also develop more detailed guidance for departments, which will be made available on the Government Service Design Manual, on:","description":"Services need to conduct and collate research on their assisted digital users as part of developing their requirements for assisted digital support. Doing this research is the department’s responsibility. ","link":"/service-manual/assisted-digital/assisted-digital-user-research.html"},{"title":"Blogging about a service's progress","indexable_content":"Publish, don’t send Get to the point Little and often Illustrate everything Use the style guide… …but remember you’re a person Always get someone to read it first Blogs are a simple, effective way of keeping a record of the work a team is doing on a service.  Updates can be as short as you like, allowing you to tell readers about subtle changes to a service’s design or new features added to a tool, while also offering you a platform for longer pieces of writing that describe significant changes to policy or approach. The Inside GOV.UK blog and the MOJ Digital blog are great examples of how teams are already talking about their work in the open. A good blog post can do the work of dozens of emails. Finding the right way of sharing a piece of information once – describing a change to how a feature works for example, or talking about upcoming work – can save a huge amount of effort in finding new ways of saying roughly the same thing. More importantly, it means there’s a consistent frame of reference for talking about a subject. If people have been told slightly different things then they might come away with a different understanding of how something works – not something likely to make your life simpler, clearer and faster. Whatever you’re writing about, make sure the most important information is in the title and the opening sentence. Readers may only see the summaries of your posts on the home page, or their feed reader, or in the email alerts, and it’s important they have a clear understanding of what you’re doing. Short updates can still go into a bit of detail about what you’re doing and why you’re doing it, whether that’s changing the colour of a button or re-writing a whole section of guidance. Keeping posts short makes them simpler to write, and also means it’ll be easier to get input from the people you work with. Though the frequency with which you blog will often vary you should aim to publish a new post at least once a week. A well-chosen illustration or photo can add important context or information for your readers. The GOV.UK blogging platform is fairly flexible. You can embed tweets, slide decks and videos as well as images, all media that make your message just a little bit easier to comprehend. The GOV.UK style guide is an invaluable tools for writers. It’s full of carefully researched tips for clear, concise prose which seeks to make your writing easier to understand. In particular, make use of subheadings and bullet points within your blog posts to make them quicker to read. And make use of the ‘words to avoid’ list – it’s there to make sure writers stay clear no matter the audience. The style guide doesn’t mean you can’t be warm, candid or personal. You should be all of these things.  It’s important that you’re accountable for the things you write. If your name appears on a post then you should be prepared to respond to comments and engage in dialogue both on your blog and on other social networks. It’ll improve your work, and ultimately improve the experience for your users It’s a given that writers will be professional and mindful of things like the Civil Service Code when they blog, but you should always get someone to read your work over before you hit PUBLISH.  For starters, they’ll probably spot a few mistake that have slipped your notice, but they might also be able to offer a different perspective on the things you’re writing about. ","description":"Blogs are a simple, effective way of keeping a record of the work a team is doing on a service. ","link":"/service-manual/communications/blogs.html"},{"title":"Increasing digital take-up","indexable_content":"Background How to increase your digital take-up What not to do Why you should do this 1. Understand how and why people use your service by: 2. Get users to try digital for the first time by: 3. Communicate effectively by: 4. Manage staff and partner engagement by: 5. Reward digital users by: 6. Overcome legislative barriers by: 7. Identify and minimise security risks by: 8. Identify users’ triggers for using non-digital channels and satisfy these needs through your digital service by: 9. Track and measure channel shift by: 10. Set your channel shift targets by: mapping the end-to-end user journey of those who currently use your services via offline channels identifying what it is in the offline process that makes it more convenient for service users to continue to use your service this way identifying the barriers that stop current users from wanting to use your digital service knowing which intermediary organisations your users trust to give them information / reassurance about the service rapidly implementing changes to your service based on effective user research; Leisa Reichelt, head of user research at GDS, has provided a step-by-step description of how her team delivers timely, relevant and actionable user research identifying all the ways that your service is currently promoted to first- time users, and ensuring that the digital service is promoted through these channels making your current offline users aware of how your digital service will meet their needs, like convenience, speed, reduced cost etc training frontline staff on how to make current users of offline channels aware of your digital service, how it meets their needs, and support users to try the digital service for the first time training and supporting your delivery partners in providing practical support to first-time users of your service identifying all your delivery partners and making sure that at every point of contact (between them and your users) they inform users about the digital option in a positive way outlining the benefits of the digital service relevant to your user groups when communicating with them:     with the help of your user researchers work out which messaging styles work best with each user group       the Cabinet Office Behavioural Insights Team has interesting case studies on various ways to encourage behaviour change      with the help of your user researchers work out which messaging styles work best with each user group the Cabinet Office Behavioural Insights Team has interesting case studies on various ways to encourage behaviour change making sure that all delivery partners are aware of how your service meets users’ needs and actively promote this to the users they work with addressing any myths about barriers to using your digital service and making users and partners aware when real barriers have been removed using both digital and non-digital channels ensuring that all your communication with users encourages action eg create a memorable URL that goes straight to the service, not just to a home page (this may be subject to advertising, marketing and communications spend controls) incorporating the URL for your service into all your written correspondence eg letters, email footers, hard and soft copy, out-of-hours phone messages; put phone numbers and physical addresses in less prominent places in any communications getting information on your digital service into partner publications / websites that your target users might go to for other reasons checking if the guidance for using your service is set out in jargon-free language that your users find comprehensible – be sure to user-test it identifying the benefits of the digital service for your delivery partners and highlighting these in your engagement with them *making sure that these organisations and intermediaries are aware of the digital channels available for accessing your service, and are convinced of the benefits to the user mapping all the offline customer journeys for your service and identifying every ‘touch point’ ie through whom, how and where its accessed making sure that all points of contact are used to promote digital, ie frontline staff, partner organisations, voluntary groups etc making sure that staff and partners are familiar with the service through demos and help with digital transactions – you need to train staff well so that they are able to help users and don’t lead them back to more traditional channels preparing staff (and unions) as early as possible for the adjustments to working practices that channel shift will cause investigating whether you can offer extra advantages or benefits if people use services digitally, such as:     charging users less for digital transactions (eg Companies House’s Annual Returns submission service)       extending submission deadlines (eg Self Assessment tax returns)       providing additional service features, such as application progress tracking, and text reminders of deadlines – the use of personalised text messages for reminders has proven to be a successful behaviour change tool (PDF, 549KB); implementing this approach will require the cooperation of your department’s information assurance team, so it’s important to engage them early      charging users less for digital transactions (eg Companies House’s Annual Returns submission service) extending submission deadlines (eg Self Assessment tax returns) providing additional service features, such as application progress tracking, and text reminders of deadlines – the use of personalised text messages for reminders has proven to be a successful behaviour change tool (PDF, 549KB); implementing this approach will require the cooperation of your department’s information assurance team, so it’s important to engage them early identifying legal requirements which disrupt digital delivery and hinder digital take-up (eg data sharing restrictions, requirements for wet signatures) and develop a plan to resolve them discussing these with your policy team to work out non-bureaucratic solutions In this case study (PDF, 99KB), the DVLA explain how they overcame legislative, bureaucratic and data protection barriers preventing take-up of their Electronic Vehicle Licensing Service and the lessons learnt. setting out in plain English the steps you have taken to protect your users from fraud and inappropriate data sharing educating users about how your services and procedures should work so that your users are less susceptible to phishing activities and third parties which mis-sell government services making sure that your department’s single point of contact (SPOC  – see below for details) is aware of your service and has put in place measures to support users’ reports of mis-selling or phishing: every department with public-facing transactions should have a SPOC that co-ordinates the investigation and resolution of end-users’ reports about scams or phishing activity monitoring usage of digital transaction to identify where in the end-to-end user journey customers are dropping out, and using your customer research to understand why they’re failing to finish the digital transactions limiting the need for your users to revert to non-digital channels (eg phone or face-to-face) to do things like checking the progress of their application checking, for example, that your digital service acknowledges successfully completed applications and allows progress to be tracked online getting access to usage level data, so you can make useful comparisons against a task carried out in 2 different ways, eg number of registration requests completed via offline channels as well as via your digital service determining cost per transaction, by channel – combining the cost per transaction for all your offline channels makes it harder to understand how usage is changing and its cost implications for your service setting the right metrics for measuring digital take-up measuring the number of complaints or requests for support that your digital service is generating eg calls to your call centre agreeing the methodology for measuring savings due to channel shift – try as much as possible to select tangible metrics and minimise the use of estimates This case study (PDF, 94KB)  from Hertfordshire County Council about their Library Renewals Service explains how they developed a channel shift tracker tool to track and measure channel shift; and how its monthly dashboard informs strategic management decisions. getting sufficient demographic information about your user base, so you can set ambitious and achievable targets for channel shift – the Digital Landscape Research sets out useful information on internet usage levels by demographic considering projected savings from digital service provision and using these as a guide for setting your targets; it’s likely that you’ll have to prioritise increasing digital take-up for some of your digital services over others (both existing and new or recently redesigned) to maximise savings have unstructured routes into your service (eg email – online forms are much better) make other channels easier (eg pre-populating paper forms but not online ones) request unnecessary information unquestioningly maintain service standards for traditional channels; review them once your digital service is up and running enter into outsourcing contracts which discourage providers from promoting channel shift (eg paying per call handled) It’s essential to increase the take-up of government digital services so more users can benefit from improved government services. Increased take-up will also make it possible for assisted digital support to be focused on those who are unable to use the digital service. Digital take-up is one of the four key performance indicators your service will need to benchmark, measure and monitor in order to meet the Digital by Default Service Standard. Digital Landscape Research found in 2012 that 82% of the population are online and over 60% of people have used online banking, yet the take-up of most government digital services was well below those figures. 10 steps have proven to be successful in ‘shifting’ people away from non-digital channels, eg over-the-counter services and telephone helplines, to using digital services. These steps will help you identify and fix some of the barriers to take-up of your digital service, including: usability, legal barriers, security fears, lack of trust and awareness of your service among users. These 10 steps will generate a higher take-up of digital services. We’ve included case studies which outline the experiences of several organisations that have implemented these steps, the impact this has had on their services and the lessons they’ve learned. Use this information as a step-by-step guide for increasing your digital take-up. Every action is important and should be considered and seriously attempted. Hertfordshire County Council, as explained in this case study (PDF, 94KB), developed an understanding of how and why people use its Online Free School Meals service, and used this knowledge to increase take-up. Another case study from Hertfordshire County Council (PDF, 93KB) looks at some of the steps they took to get users to try its Online Schools Admissions service for the first time. While this case study from Chelmsford City Council (PDF, 74KB) shows how a simple how to guide sent out with the usual service renewal letter switched 40% of service renewals to the online service. In this case study (PDF, 103KB), the Driving Standards Agency (DSA) explains some of the steps it took to communicate the benefits of its Driving Test Booking Service to its service users. It also highlights some of the lessons it learned along the way. In this case study from Chelmsford City Council (PDF, 81KB), the critical factor is the timing of the communications. The important aspect is ensuring that communications get to the right people at the right time, so that they are encouraged to use the online service. This case study (PDF, 103KB), about the Land Registry and their e-Document Registration Service highlights the value of involving other stakeholders to help to communicate the benefits of using digital services. This case study (PDF, 150KB) about the HMRC Cross Tax Strategy outlines the ways they work with staff and partner organisations to encourage the use of their digital channels. This case study (PDF, 145KB)  from the HMRC Cross Tax Strategy explains how HMRC offered financial incentives for online filing and why they later switched to non-fiscal benefits. In this case study (PDF, 131KB), from Companies House, customer insight is used to ensure that the online service fully meets customer needs and to develop a communications strategy highlighting the benefits of online filing (including fee differentials) for service users. This case study (PDF, 76KB), about the Land Registry e-Document Registration Service shows how they took action to reassure service users that the digital service met current standards to address potential security risks. Your digital take-up will not be as successful if you: You need those who can use digital services to do so as soon as possible. It will cut costs and improve assisted digital support. The Government Digital Strategy requires departments to review their channel shift plans every year.","description":"It’s essential to increase the take-up of government digital services so more users can benefit from improved government services. Increased take-up will also make it possible for assisted digital support to be focused on those who are unable to use the digital service.","link":"/service-manual/communications/increasing-digital-takeup.html"},{"title":"Privacy note template for services","indexable_content":"Your personal information What information [department name] holds about you How [department name] protects your personal information Sharing your information [Department name] staff handling your information Asking to see your personal information How to make a complaint What information DWP holds about you [example] Asking to see your information [example] [information 1] [information 2] your personal details, eg name, address, telephone number your financial details, eg bank account number your partner’s personal details your employment details [any other information that your department holds, give examples if the list is too long]  tell you why the information is needed, eg [examples] only ask for what’s needed  make sure nobody has access to it who shouldn’t tell you if the information is shared with other organisations and if you can say no to this – see the [full privacy notice] only keep the information for as long as it’s needed not make it available for commercial use, eg marketing, without your permission give accurate information tell [department name] about any changes, eg to your address Departments should base their customer-facing, plain English privacy notice on this template and add links to it from existing related statements, like their personal information charter and full privacy notice. This page explains what kind of personal information [department name] holds about you, how it’s protected, and how you can find out about it.  You can also read the full privacy notice. [Service action] + [department name] will, for example, hold the following information about you: When you apply for Carer’s Allowance, the Department for Work and Pensions (DWP) will, for example, hold the following information about you: [Department name] complies with the Data Protection Act 1998.  [Department name] will:  You must: Your information can be shared with other teams in [department name] or other organisations where this is needed to process your application. There are some cases when your information can be shared for other reasons, eg to prevent crime.  [Department name’s] staff are trained in handling information and understand how important it is to protect personal and other sensitive information.  You can ask to see the personal information [department name] holds about you. Fill in the [personal information request form] or write a letter to: [address] [Information about costs when making a request.] Sometimes [department name] can withhold information, eg to protect national security.  You can ask to see the personal information DWP holds about you. Fill in the personal request form or write a letter to: DWP Personal Information Requests Sample Street 1 London   There’s no charge.  Sometimes DWP can withhold information, eg to protect national security.  If you’re unhappy with the way [department name] handled your personal information, you can write to: [contact information] You’ll get a confirmation that [department name] has received your complaint within 5 days and a full answer within 20 days. [Department name] will tell you if there’s going to be a delay.  If you’re unhappy with the answer or need any advice, contact the Information Commissioner’s Office (ICO). The ICO can investigate your complaint and take action against anyone who has misused personal data.","description":"Departments should base their customer-facing, plain English privacy notice on this template and add links to it from existing related statements, like their personal information charter and full privacy notice.","link":"/service-manual/content-designers/privacy-note-template-for-services.html"},{"title":"Content style guide for services","indexable_content":"Style guide for services GOV.UK style Name of your service Consistency and clarity Logic flow Addressing the user Writing questions Help text and error messages Who is eligible and who isn’t Headings Labels Start and done pages Sign in or log in Be consistent and specific Don’t assume the user has previous knowledge Think about the user’s learning curve Start with the majority of users User’s perspective Only ask what you need to know Once is enough Addressing the user as ‘you’ Services with multiple users Referring to your organisation Making your questions easy to understand Telling the user why you’re asking a question Number of questions Help text Error messages Mandatory and optional fields Names Titles Addresses Nationality Date of birth Bank details National Insurance number Dates Progress bars Confirm or ‘submit’ page End or thank you page Start and done pages explain things that are specific to your service when users register or log in for the first time give the option to switch off explanations within the service  simple questions: users will know the answers without help, eg ‘What’s your date of birth?’ lookup questions: users don’t need help but they have to look up the information first, eg ‘What’s your bank account number?’ complex questions: users need to think before they can answer them and will probably need help, eg ‘Did you not use your car at any time in the last 30 days? Only answer if it was for 5 days in a row.’ you have a spouse or partner. you’re registered for Self Assessment. your household income is £30,000 or more. are you married or in a civil partnership? are you registered for Self Assessment? is your household income £30,000 or more? Name and address – you must enter this Bank details – your sort code is incorrect etc upfront eligibility checker, eg a short smart answer  short list of the main eligibility rules on the start page keeping eligibility in mind when ordering your questions: try to cover the main eligibility related questions early on (but avoid turning this into a random bunch of questions) a dynamically generated order of questions: users only see questions relating to their specific eligibility group, eg users who aren’t working shouldn’t see any questions about employment validate questions within your service so that they kick out  users who aren’t eligible, eg ‘Enter a UK postcode’, validation: ‘You can’t register without a UK address’ long bullet lists of eligibility criteria on the start page links to ‘see guidance notes’ within a service that send users to long-winded documents when it’s not clear what’s the eligibility checker and what’s the actual application  eligibility checkers asking the same questions that will be asked in the service later on as users find this very frustrating first name middle name (if needed) last name any other names (eg maiden name)  given name(s) family name any other names (eg maiden name) Mr Mrs Miss Ms Other [when selected this switches to a free text field] day month [free number text]  Using the government standards for style, logic and content design will help you build services that meet user needs. Building services from the user’s perspective and then developing services based on users’ needs is essential to your project’s success. This guide is itself in the alpha phase. The guide’s continued success depends on service managers telling us what works with their users. Please share your ideas and research insights on the mailing list for service designers Follow the GOV.UK style guide for language and style, eg tone of voice, plain English guidance, and any particular points of style like abbreviations, capitalisation etc. Make sure users can easily find your service through search engines like Google. Optimise the title of your service.  Read more on search engine optimisation. Language and labelling must be consistent throughout your service, eg don’t switch from ‘last name’ to ‘family name’.  Tell users exactly what you want them to do. Don’t invent words to ‘sum up’ the user action.  Example: Tell HMRC about a change to your company car.  Instead of: File an HMRC change request. Avoid any jargon and ‘insider’ words. Users won’t know what you’re talking about. When writing copy for your service always ask yourself ‘what does this actually mean?’ and ‘what information do I want the user to give me?’. When you get close to the subject matter it’s sometimes easy to use ‘insider language’. User research will show you what users understand and where they stumble.  If you’re building a service that users log on to regularly, eg every week or month, they will soon know what you’re talking about.  For these services you can use shorter names but you must still be specific. For example, you can use ‘Other taxes’ instead of ‘Other taxes you can register for’. But don’t use ‘Other services’.  Call things in your service what they are to make the learning curve easy for users. For example, use ‘Other taxes you need to register for’ instead of ‘Other services’. You can also: Find out how the majority of users will use the service and focus on this path first, then add niche scenarios where needed.  Approach the logic from a user’s perspective rather than how the information is processed later on.  User research will give you insight into how the user would use the service.  Before you start building your service, write out a list of the information you need from the user and why you need it. For example, ‘need to know their income so that we can subtract income over £100 a week from benefit paid’.  This will force you to think through each piece of information you’re asking the user to give you. Remember, just because it’s in your current service, doesn’t mean you need it.  Make sure you only ask for information once. This seems obvious but in a long service it’s easy to miss and users can lose confidence in a service that lacks consistency and logic. For example, don’t ask for the user’s birth date and then later in the service for their age. Always address the user as ‘you’ and use ‘your’ for headings and labelling where appropriate, eg ‘Your account’ or just ‘Account’ instead of ‘My account’ or ‘You agree to the terms and conditions’ instead of ‘I agree to the terms and conditions’.  As a general rule don’t use ‘my’ or ‘I’.  If the user has to give information about other people, be clear whose details you’re asking for, eg ‘Your address’ and ‘Your partner’s address’. Give a ‘Same address as you’ option to populate these fields dynamically.  Where multiple users have access to the same service, use the name of the person logged in, eg ‘Susan’s account details’.  If users have roles and permissions, spell out what these are.  Example: Susan’s account details You’re the accountant for Ladida Ltd. You can send tax returns on behalf of Ladida Ltd. At the start of the service make clear which organisation the user is dealing with, eg ‘Apply for funding from Defra’. After that you can refer to your organisation as ‘we’.  If the circumstances are ambiguous or there is more than one party involved, use the name of your organisation throughout, eg ‘HMRC will get back to you in 7 working days’.  Test with users if they understand who ‘we’ is. Email your insights to digital-service-designers@digital.cabinet-office.gov.uk The same rules apply to writing questions or steps.   Keep questions short and straightforward. After you’ve written them take a step back and ask yourself ‘what do I actually want the user to do?’. Check your questions for any jargon. Words you use on your project to describe things usually don’t mean anything for users.  Ask users in research to describe what they’re doing in your service. Use the words they use.  Usually you’ll have 3 types of questions in your service: Your questions don’t have to be questions, you can also use statements, eg ‘What is your date of birth?’ or ‘Date of birth’. This depends on the format of your service. However, you have to be consistent and keep reading to a minimum. Don’t use ‘What’s your date of birth’ followed by ‘Contact details’ on the same page.  When you’re asking for sensitive or additional information, tell the user why using help text. For example, when you’re asking about their partner’s details. Users often don’t understand what this has to do with to their own application. Example: What’s your partner’s income? [help text] Your household income is your and your partner’s income. To calculate it correctly we have to ask about your partner’s finances.  User research will show you which questions users have an issue with. Email your insights to digital-service-designers@digital.cabinet-office.gov.uk Keep the number of questions to a minimum. Remember, users are most likely reluctant to use your service in the first place. Each question adds to their frustration and opens the door for mistakes to be entered.  Often it’s possible to get the same amount of information with fewer questions.  For example, instead of asking users 10 questions about their personal circumstances you can group these into a checklist.  Example: Your circumstances Use: Which of these apply to you? Choose all that apply [from this checklist]: The checklist is simpler for users than 3 separate questions: Keep it short and concise and speak to the user in their language. Users are unlikely to read anything longer than 3 lines. Focus on user action. Don’t give background information, eg ‘This used to be called X but in 2008 it was changed to Y’.  Example: Enter the years you’ve paid National Insurance. [help text] These are the years you’ve worked and paid tax or the years when you’ve received benefits like Jobseeker’s Allowance.  Instead of: National Insurance contributions can consist of Class 1 payments for employed, Class 2 payments for self employed or voluntary Class 3 payments. There may be certain years when you’ve received benefits that cover some of your National Insurance contributions. If this is the case, you must also count those years.  Only if you really can’t fit help text on the page or when it’s an edge case you should use help links, eg ‘What does this mean?’ or ‘What’s X?’. However, this isn’t a dumping ground for long ‘lazy’ help text.  Don’t send the user out of the service to read long guidance documents. They won’t. On the top of the page tell the user where they went wrong and how they can fix it: There was a problem with the information you entered: Give the user a message in each field where an error occurred.  Name and address [Enter a name] [Enter an address] There isn’t a ‘one size fits all’ answer for this. How you present eligibility criteria depends on your service.  Different service exemplars have tried out these things: Things that don’t work very well: If you’re trying out other ideas, email them to digital-service-designers@digital.cabinet-office.gov.uk All headings must be sentence case, eg ‘Your tax account’ not ‘Your Tax Account’. However, where a proper noun appears in the heading this is upper case, eg ‘Your National Insurance contributions’. Check the section on capitalisation in the GOV.UK style guide. Headings must give the user a clear indication where they are in the service. The name of your service should be on every page. Avoid words like ‘more’ or ‘further’ in headings, eg ‘About you’ followed by ‘More about you’. In this example ‘Personal details’ followed by ‘Contact details’ would be useful. Use sentence case: the first word is always upper case, the rest is lower case, eg ‘Date of birth’ instead of ‘Date of Birth’.  Exceptions are personal names and proper nouns, eg National Insurance number, Child Benefit etc.  Don’t invent proper nouns, eg don’t say ‘Check your Identity Profile’.  Mandatory fields don’t need special labelling. Don’t use asterisks *.  Generally, there shouldn’t be any optional fields because you either need the user to give you the information or you don’t.  If there are optional fields they should be marked (optional) immediately after the field label text so that screen readers catch them before getting to the field.  Example: Maiden name (optional) [free text field] Use one freetext field: Name.  Use the following format as applicable if backend or business requires that you separate names: You can use this format if your service is used by many users outside the UK: If your research shows that users don’t understand this labelling, eg users whose first language isn’t English, email insights to digital-service-designers@digital.cabinet-office.gov.uk You don’t have to use titles in your service. If you do, use this format: You can give examples for ‘Other’ that fit with your service.  The preferred option is to have a postcode lookup with a ‘Your address not listed’ option. Users should also be able to edit the address they choose from the lookup.  You should also have an ‘Enter address manually’ option for users who don’t want to use postcode lookup.  NB: postcode is one word.  Use the following labelling for ‘Your address is not listed’: Name or First name (if backend needs separate fields) Last name (if backend needs separate fields) Address Address Address City Postcode Country (where appropriate) Telephone: Email: If you find in research that users don’t understand this labelling, email insights to digital-service-designers@digital.cabinet-office.gov.uk This is an alphabetical country drop down. You can have ‘United Kingdom’ as the first option.  If you test other ideas, email insights to digital-service-designers@digital.cabinet-office.gov.uk This is a drop down with the following labelling: You can test other formats, eg one free text box with help about the date format. Email your insights to digital-service-designers@digital.cabinet-office.gov.uk. Use the following labelling and single free text boxes for user input: Bank or building society details Account holder Bank or building society Sort code  Account number Roll number for building societies (if needed)  For international services use: Account holder Bank or building society IBAN BIC Use a single free text box labelled: National Insurance number.  Remember that ‘number’ isn’t capitalised and don’t call National Insurance number ‘NINO’. When asking for date ranges make sure the validation forces the user to enter consecutive dates or show an error message. You can add days of the week to help the user with this.  Example: Enter the dates when you were abroad From [day] [month] [year] to [day] [month] [year] Validation example: 09/03/2013 to 12/11/2012 Error message: Your end date must be after the start date. When asking for past or future dates make sure validation starts from today’s date. Show an error message if user input isn’t in the past or future.  Periods of time when used in questions or help text should follow the format in this example:  8 July to 9 August. Send any other labels you’re using to hinrich.von-haaren@digital.cabinet-office.gov.uk. Make sure these actually reflect what users have to do on each page. Avoid using general terms like ‘other’ or ‘more information’.  You can have a confirm page in your service but you don’t have to. The purpose of this page is for the user to check anything they’ve done before they send it.  Example: Check your updated income details before making the change.  Your income for this tax year: £25,000 [button] Make the change   The button label should tell the user what it is they’re going to do, eg ‘Change your tax details’ or ‘Send your claim form’. Don’t just use ‘Submit’ or ‘Confirm’.  You can use a generic label if the action is too long, eg ‘Send’ or ‘Apply’ or ‘Make the change’.  Users must actively say that they’ve understood what they’re signing up for if terms and conditions are part of your service. This can, for example, be an opt-in box or the user can type in a confirmation.   Adapt this format of the end page for your needs: Thank you.  You’ve [user action].  Example: You’ve sent your Carer’s Allowance claim.  What happens next [include things like when the user will hear back, contact number for questions] [include links to other related services or things within the service user can do] [button with link to GOV.UK done page] Finished  or  [link to GOV.UK done page] Sign out Make sure the copy on this page matches what you asked the user to do at the beginning of the service, eg Remove a benefit, end page: You’ve removed Jobseeker’s Allowance.  Use ‘sign in’ instead of ‘log in’. GDS will write and upload the start and done pages on GOV.UK. Raise a ticket in Zendesk and send a summary of what your service does and what the user will get at the end of it, plus the 3 main pieces of information the user needs to get through the service, eg their National Insurance number, date of birth, bank account number. Also include a list the most important keywords for search engine optimisation because pages within the service won’t be indexed in GOV.UK search.  Use the GOV.UK SEO guidance for headings, introductions and metadata descriptions.","description":"Using the government standards for style, logic and content design will help you build services that meet user needs.","link":"/service-manual/content-designers/transactions-style-guide.html"},{"title":"Assessments at GDS","indexable_content":"Arranging an assessment with GDS Attending the assessment Outcomes from an assessment a brief description of the service and its intended users the current development phase details of the service(s) the new one will replace or complement an up to date link to the most recent version of the service (a development environment link is fine) a high level technical architecture diagram details of any outstanding security or privacy accreditation Assessments for services that have (or are likely to have) more than 100,000 transactions a year are completed by GDS. Planning ahead is very important as it can take up to 4 weeks to complete the assessment process. You can start the assessment process by asking GDS Approvals (gdsapprovals@digital.cabinet-office.gov.uk) to arrange a Digital by Default Standard Service assessment. GDS will agree a date and time within 4 weeks and confirm the attendees from the service. The service team should give: You should give a brief overview of the service including a live demonstration. Video screens and cables will be provided. The assessment panel will ask you to answer questions to show how the service meets the standard. You will normally bring other team members along to represent different aspects of the service and help answer questions. Often they will be a technical architect, a designer, a user researcher or a delivery manager, but that’s up to you. The assessment can last up to 3 hours depending on the service’s complexity, and will assess against all 26 points in the standard. You should expect to receive a result from an assessment within 3 working days. This will be an email to you and the responsible department’s digital leader. There are two possible outcomes from an assessment: pass and not passed. You will have an opportunity to fact check the report before it is published on the GDS Data Blog, usually within 5 working days. When a service passes a beta assessment, you will be able to launch the service publicly with beta branding on GOV.UK. When the service passes a live assessment, beta branding may be removed from the service. Any warnings on the GOV.UK start page will be removed.","description":"Assessments for services that have (or are likely to have) more than 100,000 transactions a year are completed by GDS. Planning ahead is very important as it can take up to 4 weeks to complete the assessment process.","link":"/service-manual/digital-by-default/assessments-at-gds.html"},{"title":"Awarding the standard","indexable_content":"Assessments for larger services Assessments for smaller services Providing evidence Assessment throughout development during alpha development during beta development (which grants access to the GOV.UK domain) just before live release contributes to a government-wide repository of best practise provides advice for other teams redesigning or building services is an easy way to share ideas within and across departments All services within the scope of the standard will be assessed against the criteria of the standard. This includes the 25 exemplar services. The department or agency that owns the service must make sure it meets and maintains the standard, whether or not it is designed, built and operated by teams which are in-house, external or mixed. For services that process (or are likely to process) over 100,000 transactions a year, you will demonstrate the service and answer questions from a GDS panel. Services should be assessed at least three times before going live: You must show that the service and team have met the standard. The panel will apply common sense and a degree of flexibility to their assessments, giving teams feedback and help on how to improve a service when it looks like it won’t meet the standard. For services that process (or are likely to process) fewer than 100,000 transactions a year, the you must arrange an assessment through the department’s digital leader. A GDS-trained assessor will lead the assessment. An assessor or panel from the department will review the service with you and report back to the responsible digital leader who will certify the service. Publishing good, frequent information about how services are being built is really important. It not only demonstrates commitment to meeting the standard, but also: Use a public blog to collect and publish progress. Publishing means that people from inside and outside government can follow the development of a service, allowing teams to quickly get advice and test their assumptions with a wider community. It also means that there is a published audit trail of progress to support the decisions of the panel. Redesigning or transforming a service should take 18 to 24 months, though there are no fixed deadlines for meeting the standard. As a result, having feedback throughout the process is really important, reducing the risk of lengthy projects going off track. To do this, and support agile design, progress against the standard will be monitored as part of spending control processes. Departments currently need Cabinet Office approval for digital spending. To simplify this, a dedicated account manager will work with each department to help prioritise their projects. When a department makes a submission to their account manager for spending approval on services within the scope of the standard, GDS will assess whether the work completed so far meets the standard. If the service redesign or build is at an early stage, GDS will make meeting the standard a condition of approval. If the service redesign or build is at an advanced stage, GDS will make recommendations for additional spending to meet the standard. GDS will state when the team should return for further approval. A team will go through this process to unlock spending approval at each phase of a service. The public blog will be used to record progress, which will help GDS to give departments support and feedback on whether they are on track to meet the standard well ahead of an assessment.","description":"All services within the scope of the standard will be assessed against the criteria of the standard. This includes the 25 exemplar services.","link":"/service-manual/digital-by-default/awarding-the-standard.html"},{"title":"Failing to meet the standard","indexable_content":"Failure during service design Failure to pass a Digital by Default assessment Ongoing performance during the service’s design after a review at GDS prior to going live when live, if it falls below the performance targets approved at launch A service may fail to meet the standard at three points: If a service isn’t being designed inline with the standard, or there is no public written record of progress, GDS will raise the issue with you and the digital leader and suggest action. Progress towards meeting the standard will be assessed at every spending control point the service passes through. If the department can’t give sufficient evidence that the work completed so far is in line with the standard, further spending on the project won’t be approved. If the business case is rejected, GDS will explain what evidence the service team must give for funding to be released. If a GDS assessment panel doesn’t pass a service, it won’t be awarded the standard and won’t appear on, or be linked to from, GOV.UK. The lead assessor will give feedback to you and the digital leader on what needs to be rectified. Depending on the stage of development, your team will be invited back to be re-assessed fully or against the failed criteria. If this needs only relatively minor changes, or a small amount of additional evidence, this can be done through correspondence. If the service fails to pass an assessment twice, the Cabinet Office will write to the responsible minister explaining why the service has not yet been awarded the standard, and what remedial action is required. GDS will use assessments to review services that are failing to maintain the standard, and will call in service teams to explain drops in performance or satisfaction.","description":"A service may fail to meet the standard at three points:","link":"/service-manual/digital-by-default/failure-to-meet-the-standard.html"},{"title":"Maintaining the standard","indexable_content":"continuously update and improve the service on the basis of user feedback, performance data, changes to best practice and service demand contribute to best practice developed across government to be shared through the service manual show high levels of user satisfaction are maintained in the digital and assisted digital service show high levels of transaction completion are maintained in the digital and assisted digital service show cost per transaction is decreasing in line with plans submitted ahead of the service’s launch show digital take-up is increasing in line with plans submitted ahead of the service’s launch, and assisted digital support is targeted at the people who really need it make all new source code open, reusable and publish it under appropriate licenses (or else give a convincing explanation of why this can’t be done for specific subsets of the source code) The launch of a service on GOV.UK is only the beginning. Once it has been released, users become the arbiters of its quality. Having designed the service to be iteratively developed using performance data and user research, service teams will have all the tools and techniques they need to continuously improve what they offer. Performance against the 4 key indicators will be tracked and publicly displayed. To keep to the standard, your team must:","description":"The launch of a service on GOV.UK is only the beginning. Once it has been released, users become the arbiters of its quality. Having designed the service to be iteratively developed using performance data and user research, service teams will have all the tools and techniques they need to continuously improve what they offer. Performance against the 4 key indicators will be tracked and publicly displayed.","link":"/service-manual/digital-by-default/maintaining-the-standard.html"},{"title":"Scope of the standard","indexable_content":"The criteria Why the standard doesn’t apply to everything completely new and/or being redesigned? processing (or likely to process) more than 100,000 transactions every year? the responsibility of a central government department, agency or non-departmental public body? While the information and guidance within the manual will be useful for teams in all services, the standard itself will not apply to all government services. As described in the Government Digital Strategy, only high-volume transactions being released after April 2014 will need to meet it. To establish whether your service needs to meet the service standard, ask yourself if your service is: If you have answered yes to all of these then the service must meet the standard before it can go live. If you have answered no to one or more of these questions then your service does not have to meet the standard. Government services cover a vast variety of activity, from the mainstream to the highly specialised.  Applying the same standard to services dealing with a handful of transactions every year that was applied to those handling millions is unlikely to be helpful. ","description":"While the information and guidance within the manual will be useful for teams in all services, the standard itself will not apply to all government services. As described in the Government Digital Strategy, only high-volume transactions being released after April 2014 will need to meet it.","link":"/service-manual/digital-by-default/scope-of-the-standard.html"},{"title":"Self-assessments and certification","indexable_content":"Arranging an assessment with your digital leader Outcomes from an assessment Assessments for services that have (or are likely to have) fewer than 100,000 transactions a year are completed by an assessor within the service’s responsible department. The department will be responsible for the assessment of a service and the process to arrange this will vary by department. The digital leader will then certify the service conforms to the Digital by Default service standard. You should contact your digital leader who will assign a lead assessor trained by GDS. You must give a brief overview of the service including a live demonstration. The lead assessor will expect you to answer questions to show how the service meets the standard. You will normally bring other team members along to represent different aspects of the service and help answer questions. Often they will be a technical architect, a designer, a user researcher or a delivery manager, but that’s up to you. The responsible digital leader will consider the recommendations from the lead assessor to certify the service with GDS. The assessment will be filed with GDS and a report will be created. You will have the chance to fact check the report before it is published on the GDS Data Blog, usually within 5 working days. A certified service may launch publicly with beta branding on GOV.UK after the beta assessment. When the service passes a live assessment, beta branding may be removed from the service. Any warnings on the GOV.UK start page will be removed.","description":"Assessments for services that have (or are likely to have) fewer than 100,000 transactions a year are completed by an assessor within the service’s responsible department. The department will be responsible for the assessment of a service and the process to arrange this will vary by department. The digital leader will then certify the service conforms to the Digital by Default service standard.","link":"/service-manual/digital-by-default/self-certification.html"},{"title":"Asset hosts","indexable_content":"Asset hosts To provide the fastest possible experience for our users it’s usually a good idea to serve assets (stylesheets, javascript, images) from a separate domain name. That will enable the web browser to fetch more elements of the page in parallel, and also make it straightforward to remove cookies and other extraneous information from the HTTP responses for assets. You should not use the asset host that we use for www.gov.uk to load your assets. The files provided there are only guaranteed to work for www.gov.uk and could change without notice in ways that would break other services that are using them. Instead you should host any assets you rely on within your service. We recommend a hostname such as:","description":"To provide the fastest possible experience for our users it’s usually a good idea to serve assets (stylesheets, javascript, images) from a separate domain name. That will enable the web browser to fetch more elements of the page in parallel, and also make it straightforward to remove cookies and other extraneous information from the HTTP responses for assets.","link":"/service-manual/domain-names/asset-hosts.html"},{"title":"Handling email","indexable_content":"Handling email Getting emails to users ensure there is a mail exchanger (MX) record set up for the domain from which you send email enable Sender Policy Framework (SPF) on the sending domain consider using Domain Keys Identified Mail (DKIM) on the sending domain, it can provide additional guarantees about message delivery and help recipients to distinguish genuine mail from forgery Emails to users of your service should be sent from a human-monitored email address that originates from the domain servicename.service.gov.uk (and not the dept/agency or any other domain name). Users are interacting with the service and that is where they will expect communications to come from. In order to protect users from spam email providers put in place a variety of checks. It is often a good idea to use a trusted specialist third-party to dispatch email as they will have tools and expertise to help ensure that you pass those checks. As a minimum you should: Before releasing your service you should test your email delivery. As a minimum you should use your service with registered email addresses from a range of popular email providers and ensure that emails arrive as you expect.","description":"Emails to users of your service should be sent from a human-monitored email address that originates from the\ndomain servicename.service.gov.uk (and not the dept/agency or any other domain name). Users are interacting\nwith the service and that is where they will expect communications to come from.","link":"/service-manual/domain-names/email.html"},{"title":"How service.gov.uk domain names are managed","indexable_content":"How service.gov.uk domain names are managed How is this different to how direct.gov.uk domains have been managed? Why do we do this? There are service providers on GCloud who will provide DNS Hosting with a web-interface for you to manage your DNS records. Many infrastructure hosting providers also provide DNS services for the use of their customers. As a last resort (or for larger services) you may wish to ask the supplier to provide DNS services by any means they consider reasonable (including utilising options 1 and 2 above). Once a domain name for a service has been agreed, the GDS Infrastructure Team allocates service.gov.uk domain names by delegating control to the team running the service. The team should provide the details of nameservers and GDS will delegate {service-name}.service.gov.uk to those nameservers. At that point the service team will be able to manage the detailed configuration of which servers the domain name points to, and so on. The domain direct.gov.uk was managed by the DirectGov Service Desk directly (and inherited by GDS). All domains were administered by DirectGov and each subdomain did not have its own DNS servers. This meant that services would need to give details of the names and IP addresses of their webservers to DirectGov, who would directly create the necessary A and CNAME records. The DirectGov Service Desk were also a critical part of any DNS change. The new model for service.gov.uk domains requires that you provide your own DNS servers which can respond to requests for {something}.{servicename}.service.gov.uk. It is not necessary that you run and manage your own separate DNS servers: It is likely that as you build and iterate your service you will need to make a number of changes to its configuration. You might introduce load balancers or content delivery networks, you may move hosting providers, or your provider might need to change the IP addresses that your servers are assigned. In any of those cases it is important that your team is able to quickly respond to the situation and make any relevant changes with as few intermediaries as possible. By delegating control GDS ensures that control is in the hands of the service team and not blocked by a central authority.","description":"Once a domain name for a service has been agreed, the\nGDS Infrastructure Team allocates service.gov.uk domain names by delegating control to the team running\nthe service. The team should provide the details of nameservers and GDS will delegate\n{service-name}.service.gov.uk to those nameservers. At that point the service team will be able to\nmanage the detailed configuration of which servers the domain name points to, and so on.","link":"/service-manual/domain-names/how-they-work.html"},{"title":"HTTPS","indexable_content":"HTTPS Strict Transport Security Verification of SSL Certificates Create a file with a special code supplied by the SSL vendor on your website Create a special DNS record with a code supplied by the SSL vendor Sending an email to the owner of the service.gov.uk domain Many services will collect personal information from users. It’s very important that this information can’t be intercepted by malicious third parties as it travels over the Internet. Therefore, all services accessed through service.gov.uk domains (including APIs) MUST only be accessible through secure connections. For web-based services this means HTTPS only (often referred to by the acronyms TLS or SSL, which both refer to the protocol underpinning these secure connections). Services MUST NOT accept HTTP connections under any circumstances. Strict Transport Security or HSTS is an extension to the HTTPS protocol that tells web browsers that they should make extra efforts to verify the security of a connection: they should assume for a specified period that all connections with this server should be via HTTPS and shouldn’t accept mixed content (where some content on a page is served via HTTPS and some via HTTP). This provides an extra level of assurance about the integrity of the connection. Once a service manager has verified that their HTTPS setup is working fine they SHOULD enable HSTS on the production domains (www., admin. and assets.), by setting an HTTP response header such as representing a commitment to HTTPS-only traffic for 14 days. Once the service manager is confident that HSTS is configured correctly, you SHOULD increase the commitment to months or years: In order to provide your service over HTTPS you will need to purchase a certificate (or certificates) from a recognised vendor. SSL vendors vary but regardless of which service you choose to use, you’ll need to validate with the vendor that you own the domain. The verification methods that are commonly in use (in order of preference) are: The least preferred method of sending an email to the domain owner relies on the GDS Infrastructure Team seeing the verification email and responding. If this is necessary, then first send an email to the address that you intend to use for verification from your own email address warning that an SSL verification is needed for your service. The GDS Infrastructure Team can validate requests sent to the following addresses: They are unable to validate requests sent to any @service.gov.uk address.","description":"Many services will collect personal information from users. It’s very important that this information can’t be\nintercepted by malicious third parties as it travels over the Internet.","link":"/service-manual/domain-names/https.html"},{"title":"Service subdomain names","indexable_content":"Subdomain names and certificates *.preview.servicename.service.gov.uk *.staging.servicename.service.gov.uk *.servicename.service.gov.uk being able to do HTTP 1.1 virtual hosting for HTTPS without relying on Server Name Indication (SNI). This is important because of legacy software which does not support SNI. It may not be possible to enumerate all of the names that will be served from a domain, so by allowing wildcards, we can meet that need. (You may have the public-facing www, admin and assets, but also non-public logging, monitoring and other services that still require TLS) having a different certificate for preview versus staging versus production means that we can potentially restrict who has access to the certificate. This is better from an operational security perspective. We should not use the same certificate on production as in development/test environments. When you are operating a service at servicename.service.gov.uk, you will need testing and development environments. Given that these must use HTTPS, we suggest purchasing wildcard certificates with this naming convention: The reasons why we prefer wildcard certificates include:","description":"When you are operating a service at servicename.service.gov.uk, you will need\ntesting and development environments. Given that these\nmust use HTTPS, we suggest\npurchasing wildcard certificates with this naming convention:","link":"/service-manual/domain-names/service-subdomain-names.html"},{"title":"Getting a domain name and start/end page","indexable_content":"Getting a domain name and start/end page Service name, eg {service-name}.service.gov.uk DNS servers to delegate to (ask your technical team to see the guidance on DNS delegation) Date you need it by (at least 5 working days’ notice) To make sure that the right user journey (appropriate start/end pages, clear domain names) are set up for a new service it’s important that you engage with the GOV.UK team within GDS early. You can start the service domain name process by emailing gdsapprovals@digital.cabinet-office.gov.uk who will discuss with you the best name for your Service Domain and the start pages on GOV.UK. While you are waiting for this process, you should start looking at where you will host the DNS, as GDS will delegate control of your Service Domain to you. You should read more about delegating DNS for Service Domains here which will prepare you for the next step. Once your service has an agreed name, you will need to supply the following information to your GDS contact who will put you in touch with the GDS Infrastructure Team to arrange delegation. If you are intending to use the “Managed DNS” product offered by Dyn, then you will need to give as much notice as possible, as Dyn is currently used to manage the main service.gov.uk DNS domain. Dyn requires a signed letter of authorisation from GDS and work on their systems in order to manage sub-zones of service.gov.uk via other customer accounts – this may take additional time to arrange.","description":"To make sure that the right user journey (appropriate start/end pages, clear domain names) are\nset up for a new service it’s important that you engage with the GOV.UK team within GDS early.","link":"/service-manual/domain-names/setting-up.html"},{"title":"Accessibility testing","indexable_content":"Carrying out accessibility testing When not to use it Accessibility audits Types of participants Cost Timescales cognitive and learning disabilities, eg dyslexia or attention deficit disorders visual impairments, eg total and partial blindness, colour blindness or poor vision auditory disabilities which can also affect language motor skills impairments, eg those affected by arthritis, strokes or RSI Accessibility testing is very similar to usability testing, in that it is about ensuring that a product or service is easy to use for its intended audience. That audience includes users who access the service via a range of assistive technologies, such as screen readers, voice recognition software, trackball devices and so on. It’s important to consider a range of disabilities when you are testing any product or service, including those with: The Equality Act 2010 came into force in 2010 and is intended to tackle discrimination against members of society who may be unfairly treated due to age, disability, race, religion or sexual orientation. We have a duty to make adjustments for disabled persons so that we do not exclude them from the services we provide and must make every reasonable attempt to ensure access. The Royal National Institute of Blind People (RNIB) has a useful guide to the UK law and how it affects the accessibility of your site or service. Most accessibility testing is typically conducted after an accessibility audit has been conducted. Accessibility testing with participants with a range of needs is best conducted in the participants’ own homes. This is because they will often have things set up to suit their individual needs and the whole process is less stressful for them, eg travel, environment. It is often difficult to conduct accessibility testing early on in the process of service design. The service must be fairly robust in order for it to be evaluated by people using assistive technologies, eg a screen reader will read out the contents of a web page so the code needs to be well structured. For accessibility testing to be worth doing, real content needs to be in place rather than dummy text so that it can be assessed by those with any cognitive or learning difficulties. Interactive elements such as calls to action, hyperlinks, forms etc must be in place if motor skills are being assessed. Full lab-based accessibility testing is not necessary for every project. An accessibility audit may be a more efficient and cost-effective way to review a service, depending on its typical user needs. Accessibility audits are an alternative to standard accessibility testing. An accessibility audit involves an accessibility expert reviewing the site or service, highlighting all accessibility issues and making recommendations for fixing them. They would typically use assistive software used by disabled web users (eg a screen reader) to effectively carry out the audit. See the W3C accessibility guidelines for further information. Accessibility audits are cheaper and quicker than accessibility testing but rely primarily on the expertise of the person conducting them. Disabled participants should be included as part of a wider user testing recruitment process. The numbers will be small, but should aim to capture a range of disabilities and assistive technologies. This is a harder to reach audience so the cost of doing so can be relatively expensive. Recruitment is best conducted through specialist organisations or agencies like the RNIB. Additional costs can be incurred if these participants are travelling to your testing location and/or require specialist assistance with carers or travel. Recruitment via an agency can take up to 2 weeks, depending on the target audience. Conducting testing sessions can take between 2-3 days depending on the number of participants. This may vary depending on whether the sessions are lab-based or structured sessions in a ‘home environment’. Analysis and reporting should take up to a week. These estimates are dependent on the project’s scope, and the availability of ‘robust’ testing assets.","description":"Accessibility testing is very similar to usability testing, in that it is about ensuring that a product or service is easy to use for its intended audience. That audience includes users who access the service via a range of assistive technologies, such as screen readers, voice recognition software, trackball devices and so on.","link":"/service-manual/making-software/accessibility-testing.html"},{"title":"Analytics tools","indexable_content":"High level requirements Privacy Vendor comparison Configuring analytics tools Further reading the total cost of ownership as well as cost in comparison to turnover of service the volume of data being sampled who owns the data (it should be your organisation!) the cost of additional profiles and/or custom variables the admin system users have access to whether it is hosted by the vendor or in-house whether it tracks offline channel usage whether it provides a comprehensive set of standard reports (including social interactions and multimedia capturing) it must provide an open API with no export restrictions if it can measure transactions through funnel analysis and measure goals support and training the cookies it requires do not collect and process any personal information (the terms and conditions of your analytics provider will probably expressly forbid you from doing this) turn off any data sharing (some suppliers may collect data anonymously for internal benchmarking) anonymise IP addresses that your analytics provider collects, by removing the last octet of the address obtain and review your vendor’s privacy and security policy you should own and be able to export analytics data (pay attention to terms and conditions for any free products) does the solution meet the EU privacy directive and the European Commission’s Directive on Data Protection? where is collected data held? do data centres meet EU/British data security standards? how long is data held for? what will happen to the data on termination of the contract- can you export it? data aggregation and sharing (does the vendor mine your data for cross-customer benchmarking/trends or to provide usage data to any advertising channels) what access your vendors employees have to your data (make sure there are adequate administration tools to control appropriate access for your own staff) have you installed web analytics software? have you configured your web analytics software with the appropriate conversion funnels? do you have the capability to run user satisfaction surveys? do you have the capability to do A/B testing and multivariate testing? About analytics Web Analytics Tools Comparison: A Recommendation Old, but thoughtful Analytics tool comparison Enterprise Web Analytics: A Buyer’s Guide Occam’s Razor by Avinash Kaushik has a wealth of useful, easily digestible information on analytics Web Analytics Demystified and the Digital Analytics Association have free whitepapers. Conversion Funnels What is a Conversion Funnel? Blog post by Morgan Brown with a good discussion on user flows and conversion funnels There are various web analytics tools available to help you measure how people are using your service. You will need to assess how well a particular tool meets your needs before deciding which one to use. This guidance describes some of the criteria you should consider and reviews some of the main analytics tools against them. When deciding which analytics tool is most appropriate for your service, you should consider the following: For each of these criteria, you should identify which are fulfilled as part of the standard quoted package and what is charged for any additional features. The privacy and security of data is of the utmost importance. Make sure your analytics solution and processes take the following into account: There are a range of digital/web analytics vendors in the market-place, together with open source solutions. A search for ‘analytics tools comparison’ provides a number of useful resources where you can compare the capabilities and strengths of different services. There are some examples in Further reading below. Install and configure analytics tools that meet your needs. Where possible, use platforms that enable the data to be piped automatically into other systems. Using APIs (Application Programming Interfaces) will stop you having to input data manually and allows for aggregation across multiple platforms. You will need to answer the following: A selection of sites that provide vendor comparison information: For information on digital analytics: For transactions, it’s important to understand funnels:","description":"There are various web analytics tools available to help you measure how people are using your service. You will need to assess how well a particular tool meets your needs before deciding which one to use.","link":"/service-manual/making-software/analytics-tools.html"},{"title":"APIs","indexable_content":"Guidance Consuming and using APIs Further reading Build an API by building with the API Just use the web Give each thing a bookmarkable URL Use HTTP methods as Tim intended Representations are for the consumer Names reinforce conventions Document by discovery… and example Explicitly set expectations Be public by default Practice service evolution Code Integration Testing Service agreements and resilience JSON for convenient processing in most programming languages JSONP and JSON with CORS for client-side JavaScript CSV for importing into spreadsheets Atom for feeds iCalendar for events vCard for name and addresses KML and geoRSS for geographical data m3u for playlists API Craft is a reasonably active public forum for discussing publishing APIs The Open Web Application Security Project (OWASP) maintains a large repository of security information applicable to building APIs, including a REST Security Cheat Sheet The White House are developing API standards which are largely compatible with this guide Martha Lane Fox’s report called for government to act as a “wholesaler, as well as the retail shop front, for services and content by mandating the development and opening up of Application Programme Interfaces (APIs) to third parties.” This section is a set of guiding principles for exposing a digital service as an API. When building an API there is always a danger of building the wrong thing in the wrong way for the wrong people. This is especially a risk in the absence of a developer community driving the needs behind the API. The simplest way to ensure your API is useful and consumable is to build a website using your own API. Building a website leads to considering how to best model content and data in terms of bookmarkable resources, and ensures data is presented in human as well as machine-readable representations. Becoming a consumer of your own APIs not only validates your API, but exposes services on the web. Consider an API to be a part of a website. Provide links to machine-friendly formats from human readable pages, and enable agents to easily construct URLs which link to human-friendly representations of pages. Use standard formats for content, and follow established web patterns for authentication. Building a service to enjoy mass adoption and support from a wide, disparate community of developers and programming environments while being able to reach a worldwide audience is difficult. While proprietary and open technologies abound for machine-to-machine communication, none of them have the web’s interoperability, reach and ability to scale. Standards are powerful agreements, and nowhere are agreements more quickly established and adopted than on the web. Using HTTP (Hypertext Transfer Protocol) and URLs (uniform resource locator), the core technologies of the web, along with emergent standards such as JSON and OAuth changes a website from a retail shop window into a wholesaler, meeting our design principle to build digital services, not websites. Expose data as a set of resources, offering a clean URL for each thing, and each collection of things. Only use query strings for URLs with unordered parameters like options to search pages. Consider creating URLs for different granularity of resources. For example, /members.json could return a list of names, whilst /members.json?detail=full could return detailed information about each member in a list. These principles enable network effects which arise through linking and allow information published beyond the web, sent in alerts email, SMS, XMPP and other messages, to link back to the canonical content on the web. Ensure all HTTP GET requests are safe and actions which change state are conducted using a POST, PUT or DELETE method. Use PUT and DELETE with caution, as they are commonly blocked by firewalls, intranet proxies, hotel Wi-Fi and mobile operators: always offer a POST alternative. Avoid HTTP methods which are not well defined, such as PATCH. Offer content for each thing as a human-readable HTML, with links to content in alternative machine-readable representations: Where possible, also offer other formats most suited to a specific domain, such as: This advice builds on our more general guidance on data and content publication formats. Include hyperlinks to alternative representations as link headers as well as in content. Consider also encoding metadata inside HTML content using semantic markup: Microformats, RDFa or schema.org. The representations supported by an API for input will vary depending upon the complexity of the action, but where possible should include application/x-www-form-urlencoded to allow the construction of simple POST forms. Use names for fields, formats and path segments in a URL path consistently across your API. Establish conventions others may easily follow and anticipate. Where possible, reuse names widely used elsewhere on the web, as with the Microformats naming policy. Building a website which exposes the data through links, and services through HTML forms encourages exploration and leads to discovery through hypertext. Provide documentation for your API using examples. Collect how people are using your API, especially link to any open source projects for projects, wrappers and programming language libraries. Provide simple ways to experiment, as with The Guardian API explorer. Be clear in the web pages and other documentation as to the security, availability, rate-limiting, expected responsiveness of the platform and the provenance of data, so consumers can plan their commitment to using your API. Lower the barriers to others using your data: don’t demand registration or API keys for public data. Open data increases the number of people able to use your data and service, and leads to feedback loops where consumers become motivated to resolve issues at source, feeding back issues and correction to your service and its data. Where content is sensitive, or requires authentication, use HTTPS encryption (Hypertext Transfer Protocol Secure) and a standard authentication such as basic authentication or OAuth, depending upon the sensitivity of your content. Build for forwards compatibility by gracefully handling content that is unexpected. The robustness principle – Postel’s Law – explains the ability for the web and internet to evolve, though you shouldn’t ignore protocol errors, corrupted, or invalidly formatted content. Preserve backwards compatibility with existing consumers of your API, by sending expected fields and employing sensible default values for missing fields. Eschew changes to the semantics of content, eg don’t change a title field from meaning the title of the page, to meaning the prefix for a name to the person’s job title. Where a revolutionary change is unavoidable, communicate a breaking change by changing the URL. When changing URLs, continue to honour old consumers, possibly use a redirection. Cool URIs don’t change. Don’t do everything yourself (you can’t). Sometimes the functionality your service needs will be provided by other parts of your organisation, other government departments or by reliable third parties via APIs. Most modern digital services are built on top of a wide range of APIs. This allows each part of the service to focus on its core responsibility rather than constantly reinventing the wheel. When consuming APIs you should be careful to keep the integration with your code clean and distinct. This is important to ensure that you can swap between providers or update to new versions of an API without making substantial changes to your core code. At GDS we encourage the use of adapter code that’s entirely focused on interfacing with the system and mapping code. This will provide the linkage between your code’s domain model and the concepts and services provided by the API. You should consider carefully how you intend to test your integration with the service. In day-to-day development you’ll want to be able to test your code without making (computationally or potentially financially) costly calls out to third party services, so you should come up with a way of providing mock versions of those APIs. For full system tests, however, you’ll want to test the full flow including the third-party service so an automated mechanism should be built for that. Many of the GOV.UK publishing applications send emails to provide alerts for content designers. When running tests we don’t want to send lots of fake emails so we swap the normal email adapter for one that logs the emails it would have sent. This lets us test our code is doing the right thing without depending on external services. Parts of our Performance Platform code involve significant interactions with Google Analytics. It wouldn’t be practical to test this by sending events to Google, waiting for them to be processed, and then reviewing the results. Our developers therefore built a mock service that can be run alongside tests and provides a dummy version of Google’s API that lets us check the right data is being sent. Our publishing systems make use of a single sign-on service. In most of our tests the interaction with that service are mocked so the applications’ tests can be run in isolation, but we also have a suite of smoke tests that run in our preview environment and use dummy accounts to ensure that the full authentication and authorisation flow is working. The Licence Application Tool integrates with a number of third-party payment services. It makes use of test accounts with those services to verify it is able to communicate with them and is sending the right data to complete payments. By depending on a third party API you could very easily be tying your service’s availability to that of the third party. In some cases that may be acceptable, but often you will want to ensure that you have a fallback plan in place. The details of that fallback will vary according to your service. It may be that you’ll need to offer the user the opportunity to use an alternative service, or queue the action to take place later. That could be an automated queue with software that monitors it and retries transactions, or it could be a manual queue where someone follows up to collect further details. You should be clear with your users about what’s happening. If a third-party payment provider isn’t available you might queue the transaction to try again later. That will mean you can’t offer users the same guarantee that their payment will be processed correctly and you should tell them so.","description":"Martha Lane Fox’s report called for government to act as a “wholesaler, as well as the retail shop front, for services and content by mandating the development and opening up of Application Programme Interfaces (APIs) to third parties.”","link":"/service-manual/making-software/apis.html"},{"title":"Choosing technology","indexable_content":"You can change your mind Start with capabilities, not implementations Cost Consider people Testing and deployment Lock-in When to make new software versus using existing software Level playing field Coding in the open Why we do this Further reading Reasons not to share software create an unacceptable risk to the security of our systems or processes that cannot be mitigated with reasonable efforts contravene existing contractual arrangements directly threaten our national security the ability to install and experiment with open source software environments to easily publish prototype services on the web convenient access to a wide variety of network connected devices for testing websites unrestricted access to collaboration tools like GitHub, Stack Overflow and IRC source code patches bug reports feature requests sponsorship of developers and support staff engaging in community discussion groups giving public attribution to projects  maximise developer productivity minimise total cost of ownership avoid lock-in make it easy for the government to share software that it creates    Please note, this is for guidance purposes only, and should not be taken as legal advice.  At different points in your project you’ll have to choose one technology over others. That might be a programming language, database, library, operating system or some small tool that helps the development team work more efficiently. In some cases you may also have to choose between proprietary and open source software. Please note, this is for guidance purposes only, and should not be taken as legal advice. The most important consideration is to work on the assumption that most of your technology choices can change, especially during the early stages of development. You might select a programming language that you know to be easier and quicker to prototype in for the early stages of the project, and then move to another one that is easier for large teams to use for the final product. Or you might start out with an open source product to allow you to get started quickly, before going on to buy a commercial (proprietary) product which provides some required feature (or vice-versa). On day one of your project you simply won’t know enough about the domain or the user need to select the right technology. It’s OK to make an educated guess at this stage, as long as everyone understands that’s what’s happening. Then find time to challenge the selection as you learn more about the problem at hand. It’s very easy to immediately jump to a specific product when making technology choices. This tends to be based either on past experience or on fashion. Try to take a step back and think about the capabilities of the technology you’re after. Are you looking for a relational database? Or a document database? A key/value store? Or maybe a graph database? Argue first about the capabilities, rather than any specific implementation. Once you have defined a set of capabilities, work out how you’ll test solutions against those capabilities. You should aim to test any software that appears appropriate in an environment that’s very similar to your production environment, ie at production scale under real-world conditions (including worst-case user load). This will allow you to make sure you understand its specific characteristics, confirm that its claimed capabilities are genuine, and identify any trade-offs or potential issues to consider with a given piece of software such as conceptual fit with your domain model vs. performance under heavy load. When choosing technology make sure you consider the total cost, as well as any upfront fees. Try to take into consideration costs for things like staff, support or licensing costs (where applicable), the productivity of ongoing service improvement work, and any exit costs (especially around migration of data to a future replacement system) that might be caused by the use of non-open standard formats or protocol extensions. Make sure that you understand the cost implications of any unusually high user loads in production systems. Try to involve the whole team in technology choices so the development team (including web ops people) can share their insights into the options available and how they’ll affect the overall system. This process will get their ‘buy in’ to the technology choices made. Technology preferences vary, and technology choice can divide opinion. All things being equal, picking technologies that developers and operations staff like will typically result in improved productivity. The services that we write will need to be deployed, early and frequently. Any components that we choose should be easy to deploy and upgrade as part of an automated pipeline. Frequent, automated testing is essential in agile development. Software that makes testing difficult should be avoided. Technology lock-in happens when previous decisions regarding technology limit future decisions, possibly so that only one real choice exists. For example, if you select a database that only runs on one operating system you have no choice about the operating system you will use. If the costs of that operating system jump you have no simple way of reducing that cost quickly or cheaply. Over time, and after many decisions, you can find yourself in a situation where all your technology decisions are tightly coupled and you are locked-in to one vendor, or one way of doing things. This can have unforeseen financial costs (for example an overnight cost increase). It might also limit how quickly you can iterate on your product in the future (if, for instance, the ideal technology choice isn’t compatible with your current vendor or technology). Aim to have a clear understanding of the cost or implications of moving away from a technology when you commit to it, and avoid technology lock-in whenever possible. This may mean choosing not to use certain features of a particular piece of technology, like non-standard extensions to protocols, APIs or programming languages. This will avoid lock-in and preserve the ability to move to another technology at a later date. In general, you should avoid making long-term commitments to any particular technology, product or supplier until you fully understand the problem you’re trying to solve. Even then, you should make sure you maximise your future development options and avoid technology lock-in if possible. Where there is an existing software solution which solves your problem, you should certainly consider using it. You’re more likely to use existing software than to make new software when you have a commodity need. You may even be facing a niche problem that’s peculiar to government which has already been solved by another part of the government – or indeed another government – and released as open source software.  Development tools, build tools, utility libraries, databases and monitoring tools are all examples of software where many projects will have the same need, and it makes little sense to reinvent the wheel. For software that serves a rare or niche need you’re less likely to find a tool which serves and will continue to serve your needs. In this situation you will likely have to create new software. Software that’s developed to meet the needs of the government – whether it’s developed by government employees, contractors or by a supplier – should be shared wherever possible under a permissive, – compatible open source licence (eg MIT/X11 or 3-clause BSD) so that it can be widely used and improved.  This allows the software to be used and improved by anyone in the world who has a similar need. It’s important that other governments in particular have the opportunity to reuse the software you’ve created, because everyone deserves to have digital services so good that people prefer to use them. For example GovSpeak and unicorn herder are small components which were developed as a part of GOV.UK. They are now used by several different organisations, and have received a number of public contributions. You should always share software that has been written by the government and/or its suppliers (this includes source code and documentation) unless doing so would: In practice, sharing usually means uploading the source code and documentation to a public source code repository, keeping it updated with subsequent changes that you make (or accept from other people), and putting in place appropriate information security assurance to reduce and mitigate the risk of an exploit appearing in publicly viewable software. In some cases you might want to store the ‘master’ version of your software on an internal source code control system and replicate the latest version to a public repository. Sometimes it’s not possible to share software that was developed for the government by a third party because the third party retains ownership of the ‘intellectual property’ (IP) embodied in that software. Contracts that allow third parties to retain ownership of IP in software that’s been developed for the government, and/or that restrict the government’s ability to share this software under a permissive, GPL-compatible open source licence, should be avoided. On other occasions the team may take a risk-based decision not to share some of the software they have created, for instance to avoid exposing in public the details of a particular risk-assessment algorithm or process. It’s good engineering practice in any case to encapsulate software, and so often in this situation a large portion of the software for a given system or service can still be shared in public. The default assumption should be in favour of coding in the open and sharing software widely, but if you have serious concerns about sharing source code in public, then Communications-Electronics Security Group (CESG) can provide you with advice based on your specific situation. See also the joint Cabinet Office / CESG statement on Open Source Software and Security. With the growth of free/open source software, many high quality technology products (databases, operating systems, programming languages, development tools etc) are freely available for government and its suppliers to use and improve. But a large market still exists for commercial software products: the availability of open source software doesn’t automatically mean that you can’t choose a proprietary technology if it meets your needs. However, it remains the policy of the government that, where there is no significant overall cost difference between open and non-open source products that fulfil minimum and essential capabilities, open source will be selected on the basis of its inherent flexibility. For more information on how to ensure a level playing field between proprietary and open source software, please see the Open Source Procurement Toolkit and the section of this manual that covers use of open standards. You should ensure that any decision you make to use existing software, whether open source or proprietary, doesn’t stop you from sharing any new software that you create (or your suppliers create for you) under a permissive, General Public License (GPL) compatible open source licence. A successful open source project will garner contributions from a large number of sources, both inside and outside of a single organisation. Allow developers time to review contributions, and answer issues and discussion raised by others using the software. Larger open source projects often evolve an extension model to enable others to continue to use the service in a variety of often unexpected and possibly undesirable ways while keeping the core project coherent under the editorship of a small, trusted group of committers. Make sure developers have: Take every opportunity to contribute back to open source projects you use. Contributions may be in the form of: Cite the open source code you use, as in the GOV.UK colophon – you can read more about this approach on the GDS blog entry about coding in the open. Keys, passwords and other secrets must always be stored safely and securely away from source code following Kerckhoffs’s principle. This separation of project code from deployed instances of a project is good development practice regardless of whether or not the software itself is shared in public. For projects with a high-impact level, particularly with a small number of participating developers, it’s advisable to have a private space to discuss security issues and develop a patch. This practice will prevent flagging a vulnerability before a fix has been deployed. Choosing technology is important, but it’s probably not quite as important as you think. What’s important are the users of that technology and being able to create quality at a sustainable pace and suitable cost. When making technology choices, and importantly as you develop your product and constantly reassess your selections, try to make decisions that: Open standards considerations","description":"At different points in your project you’ll have to choose one technology over others. That might be a programming language, database, library, operating system or some small tool that helps the development team work more efficiently. In some cases you may also have to choose between proprietary and open source software.","link":"/service-manual/making-software/choosing-technology.html"},{"title":"Testing code","indexable_content":"Approaches Types of testing When to write tests Test early and often Acceptance testing Unit testing Dan North “Introducing BDD” Wikipedia on BDD We use automated testing to ensure that our code does what is intended, to protect against misuse of that code, and to provide assurance that iterating that code for better design or new features doesn’t break existing behaviour. Automated testing is an important part of our overall approach to quality but only one part of it. There are various approaches to writing automated tests. In particular there are differences in when people expect to write tests, and in the ways that they’re expressed. Many practitioners insisting that automated tests should always be written before the code they seek to test (to ensure careful design and ‘just enough’ code) while others are happier writing tests after the fact. Tests that are written before the code offer a number of advantages and that approach should be encouraged, but the most important thing is that the whole team works to ensure there are automated tests, that those tests are understood as an asset of the product and that they help you ensure the quality of your code. It is common to talk about behaviour-driven development (BDD) as an alternative approach to test-driven development. BDD is an approach to automated testing that focuses on expressing tests in the “ubiquitous language” that the whole team should share when discussing problems. There are various tools that have been created to facilitate BDD but it is an approach that can be implemented using most traditional tools. Any code written for your service should have a suite of tests operating at two levels: Requires broad tests that run through high-level functionality end-to-end, making sure that the pieces of the system come together to provide the right service. A developer should be able to describe the steps in any acceptance test to the product/service manager in a way that makes sense to them and matches how they expect the service to be used (or abused!) Focused on the specific details of the code ensuring that each discrete unit of code does what is expected of it. They allow the developers to verify that complex calculations are performed correctly, to ensure that code handles bad input properly, and that optimisations to the code don’t break its behaviour. We aim to write a first set of tests at the start of working on a feature. An acceptance test that describes the end-to-end behaviour ensures that everyone involved understands the objective of a piece of work, and can demonstrate progress through the story at hand. Unit tests can then be written to understand the implementation of the code. Tests are often described as ‘happy path’ or ‘sad path’. Happy path tests verify that the system can be used as intended, while sad-path tests verify that it handles errors (whether bad input from a user, a vital API being unavailable, or some other issue) gracefully. We start with happy path tests and a few simple sad path tests and then add more sad path tests as our understanding of the code and its dependencies develops. Tests should also be written whenever a bug is discovered. A test to reproduce the bug should be written before it is fixed, allowing you to verify that the bug has been fixed and ensure that it isn’t reintroduced later. Developers are expected to run tests regularly, especially before sharing new code, they are verified as part of the code review process, and they are also run regularly in a shared continuous integration system to ensure the whole team has a chance to see how they’re performing.","description":"We use automated testing to ensure that our code does what is intended, to\nprotect against misuse of that code, and to provide assurance that iterating\nthat code for better design or new features doesn’t break existing behaviour.","link":"/service-manual/making-software/code-testing.html"},{"title":"Configuration management","indexable_content":"Management tools Why we do this Further reading Infrastructure as code Build for portability Use the same tools for development and production testability reusability executable documentation common and constrained language to describe a problem domain Infrastructure as Code Your system is likely to be much larger than a single application, relying on other supporting infrastructure components. Even a simple application probably requires some configuration, to provide database credentials or a web service endpoint for instance. In order to build robust, scalable and portable systems this configuration data should be well managed. Configuration management tools help with documenting and maintaining the configuration and dependencies of a software system. Although this could be done using hand-made software, it’s common to use existing tools. Three examples of existing open source configuration management tools are CFEngine, Chef and Puppet. One approach to managing configuration is to describe the configuration and the software dependencies in code. This brings with it all the advantages of programming in general, including: Once described in code the infrastructure configuration is executed against the servers, networks and software in question. Moving software systems between providers can be difficult and time-consuming. Even with compatible providers and simpler procurement rules it’s possible to lock yourself in through technical inertia alone. Configuration management encourages a deep understanding of the configuration of the system and this can be used to move software easily between providers. A common problem in software systems is seen when code written by a development team works on their machine or a test environment but not on the production environment. A common cause of this is differences in configuration – different versions of software, different types of database or application server. This can be avoided by using the same tools for both development and production environments. Existing approaches to managing configuration are often manual, process heavy, slow and error prone. Ultimately people are bad at carrying out detailed monotonous tasks. And installing and configuring software across tens or hundreds of servers (if done by hand) is definitely monotonous. Even if this could be done to provide everything correctly configured on day 0, over time configuration drifts if not kept in check. One traditional approach to this problem is to make configuration changes hard, thereby limiting the number of them. When trying to build agile and flexible software systems rapid change is needed and manual processes break down.","description":"Your system is likely to be much larger than a single application, relying on other supporting infrastructure components. Even a simple application probably requires some configuration, to provide database credentials or a web service endpoint for instance.","link":"/service-manual/making-software/configuration-management.html"},{"title":"Cookies","indexable_content":"Cookies explained Privacy and Electronic Communications Regulations Using cookies Types of cookies Cookie information and warnings Cookie scoping and attributes Further reading First party cookies Third party cookies Exempt cookies get explicit, informed consent from the user before storing cookies on a user’s computer be satisfied that the user understands that their actions will result in cookies being stored (implied consent) be satisfied that the cookie is “absolutely essential” to the operation of the website (eg cookies used for operating a shopping cart) cookies for storing logged in status cookies for storing user preferences some types of analytics cookies cookies from social media sharing services cookies from advertising campaign management services cookies from embedded document sharing services cookies from some analytics services    “GOV.UK uses cookies to make the site simpler. Find out more about cookies.”  This short guide tells you what to keep in mind when including cookies into your services, and how and why you must notify users about cookies on your service. Cookies are small data files that are sent from a website and stored on a user’s computer. They are used to store information that can be retrieved later in the visit or in future visits to the website. Many uses of cookies are harmless, but sometimes they are used to track users and their browsing habits across multiple websites and target them with relevant advertising. In May 2011, the Privacy and Electronic Communications Regulations were updated to require website operators to gain consent before storing or retrieving data from a user’s computer (or other device). This change directly affects the use of cookies and other similar technologies such as HTML5 local storage. Before using cookies the website operator needs to either: Responsibility for complying with these regulations lies with the website operator. The Information Commissioner’s Office provides guidance on cookies. This guide covers how to use cookies on government services, but the principles also apply to other technologies such as HTML5 local storage. You should minimise the use of cookies on services, store as little information as you require for as short a time as necessary to provide a good service to users. If your service requires cookies to be stored then you need to make sure that they can be explained simply and clearly, in a way that the majority of users can understand. You must notify users that cookies are being stored. These are cookies that are set by the website that the user is currently viewing. They are under the control of the website operator and can only be accessed by the website. Data stored in the cookie is not shared with other websites. Examples of first party cookies include: These types of cookies are minimally intrusive as the website owner has complete control over what data is stored within them, how long the data is stored for and what the data is used for. These are cookies set by external services used on the website. The cookies are under the control of the third party service and can be accessed on any website that makes use of the service. These cookies are not controlled by the website operator and can be used to track a user from one site to another. Examples of third party cookies include: These types of cookies are intrusive as the website owner usually has no control over what data is collected or how it is used. A number of uses of cookies are exempt from the requirement to gain consent. These include cookies that are used for load balancing or cookies that are “absolutely essential” to the use of a website (eg used to store shopping cart contents). While these cookies are exempt from the Privacy and Electronic Communications Regulations, you should still notify users that these cookies are in use. All services on the service.gov.uk subdomain must include a cookie information page. This page must contain information about the cookies used throughout the site, followed by an explanation of each cookie’s purpose and how long it stored for. You can see an example of how to do this on the GOV.UK cookies page. Each service must include a link to this page on the footer of the website. The information page must also include a link back to the main GOV.UK cookies page. Services must also tell users on their first visit that cookies are used and regularly remind them of this. This is particularly important when the service relies on implied consent. GOV.UK does this with a blue information banner that is displayed at least once every 3 months with this message: “GOV.UK uses cookies to make the site simpler. Find out more about cookies.” Where explicit consent is required, services must notify their users before the cookie is set. You should do this with the sets a cookie text linked to the appropriate details on the cookie information page. Cookies must be scoped to their originating domain name only eg www.servicename.service.gov.uk not .gov.uk. Cookies should not be used on domains that host only static assets (they introduce a browser overhead that slows down the response time for users without providing any benefit). Cookies must be sent with the Secure attribute and should, where appropriate, be sent with the HttpOnly attribute. These flags provide additional assurances about how cookies will be handled by browsers. This blog post by GDS developer Dafydd Vaughan explains how cookies were used on the beta version of GOV.UK.","description":"This short guide tells you what to keep in mind when including cookies into your services, and how and why you must notify users about cookies on your service.","link":"/service-manual/making-software/cookies.html"},{"title":"Deploying software","indexable_content":"Principles for software deployment Techniques and tools to achieve these ideals Little and often Quality software Optimise for cycle time Repeatable, auditable deployments Zero downtime deployments Single artefact Ordered environments Repeatable deployments of infrastructure configuration Repeatable deployments of code Management of environment-variable configuration Zero downtime deployments Smoke tests Emergency deployments Deploying configuration management code Avoiding configuration management code Secrets Database migrations Service dependencies Making writes asynchronous little and often quality software optimise for cycle time repeatable, auditable deployments zero downtime deployments a .jar file for JVM languages for languages without compilation artefacts it may even be a tag in the source control system an entire virtual machine image with the application pre-deployed. construct your artefacts as operating system packages (.debs or .rpms) and install using your infrastructure configuration management tool from a local package repository (apt or yum) use a push-based system to deploy such as fabric, capistrano, or similar create a new immutable server for each deployment at a coarse-grained level, secrets cannot be accessed outside of the environment which uses them at a fine-grained level, secrets are known only by those machines in an environment which need to know them. We’ve identified some common principles for software deployment which we’ve applied in a number of different projects, with different technology stacks and needs. These principles underpin a software deployment process which meets user needs. Those principles are: Deploying software should be a low-risk activity. By deploying software frequently and in small increments, the risk is reduced in a number of ways. See Regular Releases Reduce Risk from the GDS blog for more on this. Deploying software frequently makes life better for the product managers in your organisation. Frequent deployments allow the product managers to get things right in a timely fashion: both fixing bugs and releasing new features.  Roo Reynolds, GOV.UK mainstream product manager, said that “Deploying once a week would be frighteningly slow.”  The GOV.UK site design has changed radically 4 times since its public release in October 2012. This was enabled in part by frequent releases enabling rapid gathering of feedback and responding to change. The software that you deploy to production should be of a consistently high quality. The user impact of bugs is obvious; less obvious is that the earlier you identify bugs, the easier and cheaper they are to fix. The deployment itself should not be a risky process. By the time a version of the software is deployed to production, you should have confidence that it will work smoothly and seamlessly. How long does it take from a developer making a code change to that change hitting production? The shorter this time is, the faster a product can respond to change. The quicker you can release the next iteration, the faster you will converge on an ideal solution. You should know at any moment what version of your service is running in each environment. When a deployment hits production, you should be able to trace the changes that it introduced all the way back to the initial code commits in the source code repositories which went into that deploy. Combined with small, frequent releases, if any problem does hit production, you will be able to immediately narrow down the cause to a small number of commits.  Rolling back to a previous version is less onerous as less of the system has changed. And “rolling forward” – with a code change to fix the production issue – is achievable because the deployment process is automated and the lead time is short. An additional benefit of having a repeatable deployment process is that scaling and recovering from failure become easy. Suppose you want to add more application servers to host a particular application, either to respond to higher demand, or to replace failed instances.  Once you have provisioned the required machines, you can just re-run your deployment process on the new machines to deploy the software. Without a repeatable deployment process, adding machines becomes manual and error-prone. Many deployment processes incur a downtime cost. The more frequently you deploy, the more downtime you will experience as a result of deployment. This may be acceptable depending on the needs of your particular project,  or you may need to consider how your deployment process needs to change to achieve zero downtime. This isn’t a one-dimensional problem. Achieving zero downtime for read operations is easier than doing so for write operations. Whether or not your project has a business need for zero downtime deployments, it’s worth considering the tools and processes which make it possible, as the constraints of zero downtime deployments can result in better engineering practices generally. An antipattern in deployment processes is building a different application artefact for each environment. Examples of this might include controlling the presence of debugging symbols in the binary or variations in the use of optimisation flags. The problem is that the testing you do of code artefacts in preview environments may not be applicable to the artefact you deploy to production. The better alternative is to build a single artefact which gets deployed to all environments. With the same code running in each environment, you can deploy to production safe in the knowledge that this code has been tested in every other environment and has not been found wanting. Note that the exact nature of an artefact is intentionally vague. It may be You should have multiple environments to deploy to. At the very least, you will have a development environment running the latest version of the software, and a production environment being used by live users. You may also have other environments dedicated to exploratory testing, user testing, performance testing or a staging environment prior to production. The environments should be ordered so that a version of the software cannot be deployed to a later environment before it’s been deployed and tested in an earlier stage. That way, the software cannot be deployed to production without having been tested in every previous environment first. This does not need to be a strict linear ordering. Some sets of tests may be run in parallel – such as user acceptance testing and performance testing. However there is very often one single production environment which is later than all others, and one single entry point which precedes all others. One of the principles of good deployment is repeatable deployments. This does not just apply to application code. Applications don’t run in a vacuum, and often have particular requirements of the underlying system in which they run. The configuration of that system should be automated and repeatable. There are two main issues: ensuring that new builds of machines are repeatable, and ensuring that once built, machines do not suffer from configuration drift, in which small manual configuration changes are made over time, resulting in a system which is not in a reproducible state. Scripting the configuration of a new machine is not a difficult process. It will always start from a known state and can have a number of tasks to install packages, put configuration files in place and start services until the machine is in a good state. Managing configuration drift is more tricky, as to counteract manual changes to configuration, your system must be robust enough to take a machine from an unknown state to a known state. There are a number of tools for managing your infrastructure configuration, such as CFEngine, Chef, and Puppet. Each of these is designed such that they can be run repeatedly on a machine to alleviate configuration drift. If you’re using one of these tools, you need to provide a means to deploy new versions of the infrastructure code. There are two means of distributing infrastructure code: Using a server (Chef Server, Puppetmaster) In this kind of system,   you’ll have a central server that distributes configuration code   to each machine in your environment. Deploying new versions of code   requires only deploying to the server, that will then distribute it   to the clients. Serverless (Chef Solo, Masterless Puppet) Here, you’ll need to   distribute the configuration code to each individual node and ensure   that each node runs the code. An alternative strategy to avoid configuration drift is to use the immutable server pattern, in which once a machine is configured it’s never touched again. In order to deploy a new version of the software, an entirely new machine is provisioned and the old one discarded. Configuration drift is avoided because servers have short lifespans and are frequently replaced by new instances. This is a natural fit in virtualised environments and where the application artefact is a virtual machine template with the app pre-deployed, but can also be achieved using containerisation technology such as lxc. There are a number of options for deploying your code: You should think about how you’ll discover hosts that you deploy to. In a simple scenario, your deployment script may have a hard-coded list of application servers that it deploys to.  In this situation, there’s a risk that the hard-coded list of servers drifts to differ from the number of servers which actually exist in reality. This risk grows more likely with larger and more dynamic infrastructures.  There are more involved host discovery mechanisms, such as internal DNS, Zookeeper, or using a message-queue based system such as MCollective. Since you should be deploying the same artefact to each environment, both for infrastructure configuration management code and for application code, you’ll inevitably find a need to inject configuration which varies between environments, such as URLs of dependent services. For application configuration, your deployment mechanism should provide a way of injecting environment-specific configuration files into each environment. For infrastructure configuration, your infrastructure tool should provide some means of achieving this. For example, Puppet 3 provides Hiera, a hierarchical datastore for managing these values. Extra care must be taken when managing secrets such as database passwords or SSL keys. You want to ensure: For example, in a three-tier app with database, application and web servers, the database server does not need to know the  SSL (secure sockets layer) private keys for the site, nor does the web server need to know the database credentials. If you are using hiera, then hiera-gpg provides a solution to this problem. It allows the injection of values from GPG-encrypted files. Only those with an appropriate private key can access the contents. By creating a GPG key for each host in an environment, you can decide on a host-by-host basis which host can access which sets of secrets. If you are using chef, then chef data bags provide a similar solution. In projects which have high availability requirements, the process of deploying small code changes to production frequently may incur an unacceptable loss of service, if each deployment results in a short period of downtime. Therefore, it’s important to consider what engineering is necessary to enable deployments which do not result in any downtime at all. This is all subject to what your definition of downtime is. Maintaining uptime for read-based operations is relatively simple: a caching layer which can serve from stale can hide the absence of application servers; a database is easy to migrate from one master to another if it is placed into read-only mode first. Maintaining uptime for write-based operations is trickier, and requires up-front thought and design. If you know that you’ll have high uptime requirements for write-based or transactional operations, you’ll need to consider how that will affect your architecture and infrastructure. As applications evolve over time, so do the requirements that they place on their databases. Database migration scripts are short pieces of database code which transform the database in some way for the benefit of the application. To achieve zero downtime deployments, you should decouple application deployments from database migrations. If you’re performing zero downtime deployments, you’ll necessarily end up with multiple different versions of the application running concurrently. Conversely, the application will need to be tolerant to the eventuality of a database migration script running concurrently within the application lifetime. Note that database migrations should be subject to the same rigorous deployment pipeline as application code. They should be deployed to testing environments first, and only go to production once they have been applied and verified against all other environments. Services which depend on one another via an application programming interface (API) can experience similar deployment problems as applications which depend on a database. For example, a frontend application which communicates with a backend application over an API of some sort.  Once again, the answer is to decouple deployments of the applications to make sure that the frontend application is tolerant to additions to the backend API, and that similarly the backend API can add functionality without disrupting the frontend application’s operation. Another method of avoiding failures during deployments is to make write operations asynchronous by posting them to a message queue. That way, when the backend system which consumes from the queue is disabled during a deployment, the frontend does not start seeing errors; rather, it just sees an increase in the time taken to see a write reflected in further read operations. Once you have deployed your application, you should determine whether the application is working as expected. If it’s not working, the deployment can be cancelled or rolled back. The test used to determine this is often referred to as a “smoke test”. A good smoke test is simple and fast, and exercises not just the application but also all of its essential dependencies. For example, if an application needs a database to be present to operate effectively, the smoke test should exercise an application code path which will fail if the database is not present or returns an error. If and when the smoke test fails, you should know what your response will be. The simplest option is to manually roll back to a previous version of the application – which should be easy enough if you have a versioned artefact repository to draw the application from. A solution with more sophistication may automatically detect the smoke test failure and cancel the deployment or roll back to the previous version. An ideal solution would not even add the new version of the application to the production load balancer until it has been smoke tested and verified good. If the application fails the smoke test, it is simply discarded; no rollback is necessary, and no interruption in service happens. This works particularly well with the immutable server pattern. From time to time, there may come a situation where you wish to deploy to production right now. This may be due to a published security vulnerability in a library you are using, or because a bug has hit production which has broken the system for a number of users. It may be the case that you subvert your usual deployment pipeline to fix things, then back-port the change you made in production (or “hotfix”) to your development environment and push it through the normal deployment process once the crisis is over. Should this be the case, then your cycle time is too long. In the ensuing post-mortem analysis of what went wrong, you should ask questions about why the deployment pipeline was not streamlined enough to accommodate a rapid deployment of a fix to production.","description":"We’ve identified some common principles for software deployment\nwhich we’ve applied in a number of different projects, with\ndifferent technology stacks and needs. These principles underpin a\nsoftware deployment process which meets user needs. Those principles\nare:","link":"/service-manual/making-software/deployment.html"},{"title":"Development environments","indexable_content":"Required Desired Current availability Internet connectivity Self-service provisioning Suitable range of virtual machine options Run own operating system EU-based data centres Service Level Agreement (SLA) Development team access Provisioning API Create virtual machine templates Firewall and load balancer service Configurable private network Virtual Private Network test software choices to prove they are valid experiment quickly with new approaches produce and test software in a production-like architecture develop rapidly and iteratively continuously test and monitor software during development deploy updates to the system rapidly and iteratively (ie at least daily) continuously test and monitor software in production As software developers, the environments we use every day matter greatly. Below are a set of guidelines for development environments to enable the exemplar projects (service transformations committed to in the Government Digital Strategy) to: Although this document doesn’t describe the capabilities and characteristics of a production environment, there’s a general presumption that any production environment should enable the exemplar project development teams to: The essential capabilities of the development environment without which the development team will not be able to operate. A service that’s already operational and able to onboard customers very quickly (typically within 5 working days). Both incoming and outgoing internet connectivity. This should also help remote management. We should be able to remotely provision new machines ourselves to meet our needs as they arise, without the need to phone, fax or email anyone, and therefore require a self-service method of provisioning virtual machines and storage. Support for 64 bit architectures and a range of virtual machine sizes at least up to 4 cores, 16GB RAM and 300GB disk. The flexibility to run whatever operating system is deemed suitable for the project, rather than just a limited subset of those supported by a vendor. We would prefer to store data in the EU, and ideally within the UK, therefore we require development environments to be hosted only in EU-based data centres. A suitable SLA should be in place with the service provider (whether internal or external), with at least a 99.5% uptime guarantee. Approved development team members should have root access to manage virtual machines (eg to install and configure software). Optional capabilities which would make a marked difference to the production of the services. The provisioning of virtual machines, storage, load balancing etc to be available via an application programming interface (API). Any API should have a suitable authentication mechanism in place, and should be accessible to development team members via the internet (optionally through a virtual private network (VPN)). To speed up provisioning we would like to be able to store virtual machine templates from which new machines can be launched. If available a managed firewall and/or load balancer service may be used. We require the ability to manage internal networks, each consisting of specific groups of virtual machines. This should allow for some virtual machines to not be internet accessible. We may choose to expose parts of the service via a VPN. The infrastructure service should at a minimum not prevent this and may ideally provide a suitable managed service.","description":"As software developers, the environments we use every day matter greatly. Below are a set of guidelines for development environments to enable the exemplar projects (service transformations committed to in the Government Digital Strategy) to:","link":"/service-manual/making-software/development-environment.html"},{"title":"Information security","indexable_content":"Introduction to information security Information security in government Roles Business Impact Levels Agile Design Good Practice Guides (GPG) Risk Management Document Set (RMADS) IT Health Check (ITHC) Ongoing Tools Risk management Further reading physical controls: walls, locked doors, guards procedural controls: managerial oversight, staff training, defined emergency response processes regulatory controls: legislation, policy, rules of conduct technical controls: cryptographic software, authentication and authorisation systems, secure protocols for confidentiality: the potential impact if the information is seen by those who should not see it for integrity: the potential impact if the accuracy or completeness of the information is compromised for availability: the potential impact if the information becomes inaccessible GPG13 – Protective monitoring GPG8 – Protecting External Connections to the Internet GPG12 – Use of Virtualisation Products for Data Separation Identify the SIRO Work with an accreditor to identify the Business Impact Levels Confirm with the SIRO the target Business Impact Levels Confirm with the SIRO whether a formal accreditation is required Procure a CLAS consultant if needed for the accreditation work If possible establish a contact at CESG who can offer assistance and some technical oversight Produce supporting documentation; for example architecture documentation, risks and mitigations, operating processes, references to GPGs, controls Work with the CLAS consultant on completing the RMADS if required Arrange the ITHC Present to the SIRO to get final sign-off Security Engineering — Ross Anderson: a comprehensive textbook covering the theory and practice of building secure systems HMG IA Standard No. 1 – Technical Risk Assessment: the CESG guide to assessing risk in information systems Business Impact Level Tables: Business Impact Level Tables    Assurance is the broad set of activities involved in assessing and managing the risks associated with the system under development, while accreditation refers to a subset of the assurance work, involving a formal and independently verified process similar to ISO27001.  When building your service, you’ll need to ensure that appropriate steps are taken to ensure its security. Information security is a topic both broad and deep, drawing from fields ranging from economics and psychology through to mathematics and probability. This document cannot claim to provide a thorough review of the field. Instead, it aims to provide you with a brief introduction to information security, and will explain the communities and processes to help you build world-class secure services. The term information security refers to the theory and practice of defending data or information systems against unauthorised or unintended access, destruction, disruption or tampering. Security professionals frequently refer to 3 main concepts: In government, much is made of these 3 main “concepts of information security,” as will be explained below. Security systems typically attempt to address one or more of these concerns through: Not every system requires a full battery of security controls. Indeed, ‘completely secure systems’ don’t exist, and overly secure systems are often prohibitively expensive or thoroughly inconvenient for their users. You should aim to build services that are appropriately secure, and in practice you will be guided by an assessment of the risks associated with a lapse in the confidentiality, integrity, or availability of your service. Within government, there’s an established set of assurance and accreditation processes. These provide a structure and a shared language within which to discuss, analyse and address security considerations. If the processes work correctly, managers should have a clear and accurate understanding of what risks they’re accepting, and those providing the service should know what controls they’re going to employ to mitigate those risks. Assurance is the broad set of activities involved in assessing and managing the risks associated with the system under development, while accreditation refers to a subset of the assurance work, involving a formal and independently verified process similar to ISO27001. The important thing to note about building trustworthy and secure systems is that it’s a team game. Assurance and accreditation should not be a completely separate strand of work, or be seen as a hurdle to be jumped over (or sidestepped). Only by engaging with risk and making decisions based on a range of expert opinion will you end up with the best product. The rest of this document will introduce you to assurance and accreditation in government. The content will use quite a lot of acronyms; unfortunately these are in common usage and it’s very hard to engage with the existing documentation and processes without speaking the lingo. We include them here in the hope that they’ll provide a helpful reference which can be used when reading existing documentation. It’s important to understand the different roles involved within the process detailed below. One of the first things you should do on your project is to establish who plays each of these roles. Note that all of them require formal training and specialist skills. All projects, however small, should involve some level of assurance. This may be as simple as documenting the limited risks and proposing to the SIRO that the project does not require a formal accreditation. For anything involving sensitive data or of interest to lots of people an accreditation stage will be required, and this process is likely to include representatives from all of the above groups. Business Impact Levels, often shortened to Impact Levels (IL) are a set of numbers used to guide discussions of risk in government projects. Specifically they are numbers between 0 and 6 for each of the 3 main concepts mentioned above, and measure: More details about identifying these numbers can be found in this extract from HMG IA Standard No. 1. The role-holders listed above will work with the wider team to bring the appropriate concerns to bear in the process of designing the service. The team as a whole will need to make a range of decisions about topics such as what information needs to be captured, how it’s processed, whether it’s stored, and so on which will have a direct impact on the assurance/accreditation process. A close working relationship will be essential to make sure that business impact levels and other details are kept up to date as designs evolve and that risk management plays an appropriate role as a constraint in the design process. The Good Practice Guides (GPG) are documents published by CESG on specific topics of interest to various types of projects. These can act as a good starting point when looking to identify risks and put in place mitigations. Unfortunately many of these documents are Restricted. It’s advisable to establish a working relationship with CESG early on in the project to make sure you can access these documents. Examples include: The Risk Management Document Set or RMADS are the result of the formal accreditation work. This is likely a large set of documents, including the Baseline Control Set (BCS), system overview and supporting evidence, presented to the SIRO for sign-off as part of go-live conversations. The IT Health Check (ITHC) forms part of the formal accreditation. In essence it’s a penetration test carried out by a CESG approved supplier (specifically a CHECK certified individual). Read the guide about penetration and vulnerability testing for more details. The assurance and accreditation work described above is not just about getting a project to launch. It also covers the running of the resulting service. Over time, new threats may emerge, systems and processes may change, and assumptions may become invalid. Documentation should be kept up-to-date and additional penetration tests organised on a regular or as-needed basis. It is important to start understanding risks and engaging with the assurance and accreditation process as early in a project as possible. This checklist is a good starting place for milestones to add to a project plan: It’s important to understand the assurance and accreditation processes and tools are all about managing the risk associated with the running service. Security is part of this, but just one part. Nearly everything brings with it risks: technology choice, staffing, processes, access to Restricted documents, data aggregation etc. It’s important to understand those risks and put in place sensible and suitable mitigations. It’s unrealistic in most cases to aim for a system with no risks, and ignoring them is a recipe for future failure. The aim is a system where the risks are known and the team, working with risk professionals, have made careful decisions about how to deal with them.","description":"When building your service, you’ll need to ensure that appropriate steps are\ntaken to ensure its security. Information security is a topic both broad and\ndeep, drawing from fields ranging from economics and psychology through to\nmathematics and probability.","link":"/service-manual/making-software/information-security.html"},{"title":"User accounts and logins","indexable_content":"Find alternatives Where there aren’t alternatives Credentials Examples Also see significantly increasing your user support overhead (people forget how to sign in, lose their passwords etc) gathering personal data that you will need to constantly review and protect adding a relatively complex interaction for users to complete are you providing a login service for a small number of agents (administrative users, accredited partners etc) or for a broad range of citizens or businesses? do you already have all the data you need in order to establish trust with those users or will you need to match them against other services (online or offline)? specifically monitor use of the system for attempts to gain access, with identifiers such as unusually high number of failed login attempts over a short period of time, or a sequence of failed logins on a given account over a long period of time segment user data from other data you hold to avoid aggregating a large amount of identifiable information swap to a new identity system such as the identity assurance scheme without invasive changes to the rest of your codebase Wikipedia on password policies CESG Good Practice Guide 44: authentication credentials in support of HMG online services xkcd cartoon explaining password vs. passphrase    For some admin systems on GOV.UK we use the   zxcvbn   library that measures how hard it would be for a computer to crack a   passphrase using brute-force methods. That library is used to validate   new passphrases and insist on strong passphrases. It’s only one measure,   but it increases our confidence that our users are picking good   passphrases.  Our advice is that teams do not build login systems. Building a login system is a significant undertaking. While there are numerous open source libraries that make it trivial to add login functionality to your service, there are significant downsides: Many features that are often implemented using login systems can be completed in other (and potentially more useful) ways. Saving search results, for example, doesn’t require a login but just a way of helping users remember a specific URL. Instead of having them log in, you could provide a tool to help send the URL to an email address or instructions on creating a bookmark in their browser. Or perhaps you could just take their email address and let them know if the search results change? The precise details will vary according to what users need from your service, but if there’s an alternative to a login system, that should be preferred. If after careful review and design work there is no option but to build a login system you will need to consider a few questions: If building a service for a small number of clearly identified agents then it’s probably safe to proceed. You should ensure that any authentication and authorisation code written for your system is carefully separated from the application in such a way that you can: If you need to build a system for a broad range of citizens and businesses, or you need to do sophisticated matching with other systems in order to build trust in the identity of your users then you should explore the advice published by the ID Assurance team. You should help your users to pick strong, secure passwords or phrases and consider whether it’s appropriate to require 2-factor authentication for extra security. As a minimum passphrases should be eight characters long and include a mix of letters, numbers and symbols, but ideally they’ll be longer than that. We refer to passphrases as a phrase is usually easier to remember but harder to guess than a short collection of symbols or a single word. For some admin systems on GOV.UK we use the   zxcvbn   library that measures how hard it would be for a computer to crack a   passphrase using brute-force methods. That library is used to validate   new passphrases and insist on strong passphrases. It’s only one measure,   but it increases our confidence that our users are picking good   passphrases. All new government services should be served over HTTPS to ensure the communication between the user and the service is encrypted. This is especially important when logging in. The Electronic License Management System (ELMS) license application system on Business Link required a login to complete an application. In building a new version of the system for GOV.UK we removed that requirement and usage rates have increased considerably. There’s still a login system for approved users in local authorities who need to process those applications.","description":"Our advice is that teams do not build login systems.","link":"/service-manual/making-software/logins.html"},{"title":"Open standards and licensing","indexable_content":"Open standards overview Building on open standards Choosing which open standards to use Why we do this Guidance Further reading We place the needs of our users at the heart of our standards choices. Our selected open standards will enable suppliers to compete on a level playing field. Our standards choices support flexibility and change. We adopt open standards that support sustainable cost. Our decisions on standards selection are well informed. We select open standards using fair and transparent processes. We are fair and transparent in the specification and implementation of open standards. user and functional needs security and legal requirements economic efficiency of government as a whole interoperability preventing lock-in improving our flexibility and the ability for government to provide services based on users’ needs and avoid digital exclusion based on the technology choices we make putting in place a level playing field for open source and proprietary software, giving us the ability to move between different technologies without the risk of lock-in making it easier to share appropriate data across and beyond government boundaries to provide efficient services for users and for our delivery partners making the cost of our digital services more sustainable by reducing complexity and encouraging reuse Standards Hub - go here to get involved in the debate about which open standards we should choose and to find out about which ones we’re adopting. Completed challenges – go here on the Standards Hub to find out which standards profiles are compulsory for use in government. Open Standards Principles Open Standards: Open Opportunities consultation outcome    Please note, this is for guidance purposes only, and should not be taken as legal advice.  Considerations for using open standards and why they matter. Please note, this is for guidance purposes only, and should not be taken as legal advice. We define open standards for software interoperability, data and document formats, as those which demonstrate all of these criteria: Collaboration – the standard is maintained through a collaborative decision-making process that is consensus based and independent of any individual supplier. Involvement in the development and maintenance of the standard is accessible to all interested parties. Transparency – the decision-making process is transparent and a publicly accessible review by subject matter experts is part of the process. Due process – the standard is adopted by a specification or standardisation organisation, or a forum or consortium with a feedback and ratification process to ensure quality. Fair access – the standard is published, thoroughly documented and publicly available at zero or low cost. Market support – other than in the context of creating innovative solutions, the standard is mature, supported by the market and demonstrates platform, application and vendor independence. Rights – rights are essential to implementation of the standard, and for interfacing with other implementations which have adopted that same standard, are licensed on a royalty-free basis that is compatible with both open source and proprietary licensed solutions. These rights should be irrevocable unless there is a breach of licence conditions. There are 7 open standards principles that you should follow when thinking about which open standards to use: We describe the reasons for these principles and their implications in the Open Standards Principles, published in November 2012. The Open Standards Board makes recommendations to the Government’s Chief Technology Officer on setting compulsory open standards profiles using the Standards Hub process. If you are looking for an open standard for a particular function and we’ve not set a compulsory profile in that space, you should carry out a thorough assessment of the standards that exist elsewhere. Choose one that meets your needs and is consistent with our definition of an open standard. Where open standards have been identified you should use them. Each completed challenge has a standards profile that refers to specific open standards. If you are looking for an open standard and we’ve not set a compulsory standards profile in that space, you should carry out a thorough assessment of the existing standards and choose a standard that meets your needs and is consistent with our definition. You should also consider how the standard fits with: If you are considering use of a standard that doesn’t meet our definition of an open standard, you’ll need to apply for an exemption. You’ll also need to apply for an exemption if you want to use an alternative standard to one that performs the same function if we’ve already selected a compulsory standards profile for that purpose. By implementing open standards for software interoperability, data and document formats, we are: Build component-based digital services, based on open standards, to provide a flexible design to give you greater choice and enable your digital services to evolve. Expressing your user needs in terms of required capabilities, which are in turn based on open standards, helps you to make better choices for service delivery. It also means that there is no unintentional lock-in built into government digital services. Whether designing and building in-house or outsourcing, you must require solutions that comply with open standards, for software interoperability, data and document formats, where they exist and meet functional needs. If there’s a reason why you think using open standards is inappropriate, then an exemption may be agreed in advance on an exceptional case-by-case basis. Any exemption would be agreed with the government’s Chief Technology Officer (or through Departmental Accounting Officer procedures for cases below the Cabinet Office’s spend controls threshold for IT).","description":"Considerations for using open standards and why they matter.","link":"/service-manual/making-software/open-standards-and-licensing.html"},{"title":"Progressive enhancement","indexable_content":"First, just make it work Second, make it work better It isn’t about “JavaScript off” It isn’t only about JavaScript Further reading Progressive enhancement on Wikipedia Understanding progressive enhancement on A List Apart For a Future-Friendly Web on A List Apart Cutting the mustard on the BBC News responsive design blog When creating web pages, the only part of it that you can rely upon working is the HTML (and even that can fail, but without it there is no web page and everything else becomes moot). The attitude towards building for the web with this in mind is called progressive enhancement. This means in essence that each extra layer (images, styling, behaviour, video, audio) of the page should be seen as optional. If you build pages with the idea that parts other than HTML are optional, you will create a better and stronger web page. Any new page or feature should first be made to work with HTML only. No images, no CSS, no JavaScript, nothing but HTML. Interactive elements are only those capable of being implemented with forms and server-side processing. This gives a baseline experience which will work in practically every browser, allowing your site to work for as many people and devices as possible, including older, legacy browsers and devices. From this baseline, the extra layers can be added. The page can have images added, advanced styling can be applied. Interactions can be made smoother and faster without the need to refresh the entire page. Validation of submitted data can be performed before it hits the network. Charts and data tables can be turned into visualisations with interactive elements. However, it’s important that each addition is seen as just that — an addition. Something extra that the more modern browsers are capable of doing on top of an already accessible and usable experience with as much of the content and interactivity as possible available already. A common misunderstanding is that designing sites to work without CSS, JavaScript or anything else is that this is not about a conscious choice made by the person visiting the web page that we can ignore. Nor can we treat this as an error or mistake, and it isn’t a case of “just fix your browser”. There are many scenarios where the extra layers can fail to load, including temporary network errors and DNS lookup failures. The server that the resource is found on could be overloaded or down, and fail to respond in time or at all. On top of failures there’s deliberate filtering of the internet. Many large institutions (eg banks and financial institutions, some government departments) have corporate firewalls that block, remove or alter content from the internet. Mobile network providers have been known to resample images and otherwise alter content in order to make the load times faster and reduce bandwidth consumed. Also some antivirus and personal firewall software will alter and/or block content. And, of course, there are people who do turn off features in their browsers deliberately. We should respect their decision and still provide them with a usable and useful service. Another common misunderstanding is that progressive enhancement is only about JavaScript. Anything more than HTML is an enhancement and should be treated as such. Adding an image to a page requires suitable alternative text (and may also need a longer description) for people with visual impairment, and simply for when the image fails to load. Styling cannot be the only method by which information is shared. “Text in bold” is not enough when you can’t differentiate bold from normal text. “Red items are required” is not enough when you don’t have the colours applied, cannot see the colours or cannot distinguish them because of colour blindness. Video and audio without transcripts and/or subtitles are unsuitable for hearing impaired people. Information appearing in a video in a visual-only form (eg a question then answered by the people on screen in an interview) is unsuitable for people with visual impairments. Making an interactive element that requires a mouse (eg a hover effect, a drag and drop operation) is unusable by someone who only uses a keyboard or a touch-only device such as a smartphone or tablet.","description":"When creating web pages, the only part of it that you can rely upon working is the HTML (and even that can fail, but without it there is no web page and everything else becomes moot). The attitude towards building for the web with this in mind is called progressive enhancement.","link":"/service-manual/making-software/progressive-enhancement.html"},{"title":"Releasing software","indexable_content":"Releasing software Why we do this Regular releases reduce risk Further reading your approach to testing the quality of low level code – approaches like continuous integration, where code is tested constantly, and test driven design, can be helpful using the same tools and release processes for both the development and production environments - this way the software and tools will be well understood and will have been run thousands of times before the first public launch the people using your service don’t get new features and improvements quickly bundling up lots of new features makes the release more complicated by releasing smaller chunks regularly it’s much easier to see what is going to change, and if something goes wrong it’s much simpler to roll that change back and undo it doing something regularly makes the case for investing in automation easier, removing much of the potential for human error and making each release the same if you’re doing something several times a day you tend to get better at it Regular Releases Reduce Risk Blog post about the approach to releasing software onto GOV.UK Constantly improving online services means releasing changes to the underlying software. How often you want to do this will affect how you design and build the applications and presents a number of challenges that this guide hopes to address. It’s important to think about how you release changes to a running application as early in the products development as possible. This is because it affects how software is developed and tested, and how a product may be supported. Being able to release software on demand is important. 6-monthly or longer release cycles are dangerous. Not only do new features rarely see the light of day but fixing known problems have to fit within a rigid release schedule.  Note that it’s important to make the distinction between releasing regularly and the ability to release all the time. The application should always be in a state where it could be released, meaning quick changes can be made when needed. As an example, changes to the software running GOV.UK are made on average 5 times per day. To do that you have to consider: Although tools, potentially including commercial tools, are required to aid rapid releases the discussions should not start with what tools should be used or procured but with the needs of the service and the product team. In some organisations, people fear releasing new applications or new versions of software. Lots of websites, especially large applications within large traditional organisations, don’t change very often. Many will have fixed release schedules which might mean one release every six months or so. This means bundling up lots of changes into a single release, which is bad in at least two ways: It could be weeks or months before an improvement that only took a few days to finish is actually released for people to use, and the complexity means there are lots of different ways the release can go wrong. The combination of complexity, risk and the infrequent nature of releases makes for a stressful event for all involved. No wonder most people don’t like release day! Releasing software comes with risks, so trying to minimise those risks is prudent. We do that in a number of ways: As well as reducing risk, being able to release early and often also helps products improve quickly, by reducing a potential barrier to quick experiments and rapid iteration. Finally consider the following two measures of a system; mean time between failures and mean time to recovery. A very traditional approach involves focusing completely on reducing the time between any failures happening, by hopefully improving the quality of the overall system. But problems will always happen at some point, so focusing some effort on reducing the time taken to fix problems that do occur can often be much more cost effective as well as improve the overall system uptime.","description":"Constantly improving online services means releasing changes to the underlying software. How often you want to do this will affect how you design and build the applications and presents a number of challenges that this guide hopes to address.","link":"/service-manual/making-software/release-strategies.html"},{"title":"Sandbox and staging servers","indexable_content":"Shared context Example Further reading everyone on the team should be able to understand progress to date by using running software people working on the service should be able to understand the impact of their work by seeing it working in context the team should be confident that the service as a whole works before shipping those changes to the public Regular Releases Reduce Risk Everyone working on design, development or maintenance of a service should have a clear, easily accessible place to review the latest version of the software. Those working hands-on building the software for the service should be able to run their own reasonable replica of the entire service.  There should be a clear staging environment where changes are reviewed and tested in the context of the entire end-to-end service before they are deployed. Everyone working on a service should be able to see progress and understand their work in its full context. That means a shared environment where any team member can see the current state of the service and where stories can be signed off, and individual sandboxes where more experimental or early stage work can take place and be reviewed. In addition to those environments it can be helpful to have a separate staging environment where final quality assurance and testing can take place before changes are deployed to the live/production environment. This should be identical to the production environment so it can be used effectively for performance testing. For those working on GOV.UK we use the Vagrant tool to provide all developers with a development environment configured similarly to the production environment. We then have a preview environment that is updated by our Continuous Integration system whenever tests have passed on a change. There’s then a staging environment for review of specific changes before they go to the production environment. It’s updated and reviewed as part of the release process. We follow this procedure because:","description":"Everyone working on design, development or maintenance of a service should have a clear, easily accessible place to review the latest version of the software. Those working hands-on building the software for the service should be able to run their own reasonable replica of the entire service. ","link":"/service-manual/making-software/sandbox-and-staging-servers.html"},{"title":"Standalone mobile apps","indexable_content":"The government’s position on apps Brief background on apps landscape Rationale for the government’s position Exceptions Further Reading Native apps What is responsive web design? More conditions to consider The process Pros Cons Pros Cons revenue (download and buy) persistent presence on device can access all functions on a device snappier performance in general can be used offline, in some cases expensive to develop and maintain needs several different versions (Android, iPhone, iPad, Blackberry etc) service iteration more complex (at least 3 types of app to deploy) can only be downloaded via gatekeeper app stores (Apple, Google) most apps are rarely downloaded, and even then hardly used it is your website, so costs are minimised and service iteration simplified uses open standards (HTML5) no gatekeepers to constrain access performance still good mobile web outstripping mobile app reach clear winning strategy for ‘utility’ services which do not require complex device features or persistence not persistent on device some device features unavailable (camera, address book) requires internet connection not snappy enough for some complex services (eg Spotify, Facebook, Skype) no ‘download and buy’ revenue stream What’s the user need? Please provide supporting evidence. Which third-party native/hybrid apps have already been developed to meet this user need? If there are none and condition 2 has been met, please provide your thoughts on why this might be the case. If there are third-party alternatives, please state why you believe a government-developed app is required. Is this user need of sufficient importance to (your users to) justify the lifetime cost of your proposed app? If you believe it is, how have you determined this? You might find it useful to review articles within the service manual such as, Know your users and Writing user stories. Is there evidence of demand for this type of app among your target users? If you believe there is, please provide supporting evidence, eg similar apps that have proven popular with your target audience and evidence of their popularity. Is there evidence to justify building an app for the platform you’re proposing to do this for? If so, please provide supporting evidence, eg analytics data that shows proportion of visitors to your content/service that currently access it using relevant devices. Tom Loosemoore’s blog post about standalone apps    “Standalone mobile apps will only be considered once the core web service works well on mobile devices, and if specifically agreed with the Cabinet Office”     NOTE: If these conditions are not in place, it is unlikely that your app proposal will be approved. If you believe there are compelling reasons why these conditions have not been met, please set them out in your proposal.  The government’s position is that native apps are currently rarely justified. At the October 2012 Digital Leaders meeting, the position was clarified: native apps could not be developed without Cabinet Office approval. The November 2012 Digital Strategy says: “Standalone mobile apps will only be considered once the core web service works well on mobile devices, and if specifically agreed with the Cabinet Office” Ensuring your service meets the Digital by Default Service Standard means it will work well on mobile devices. Making your data and application programming interface (API) available for reuse will stimulate the market if there is real demand for native apps. ‘Apps’ come in several different flavours, therefore confusion is understandable. When people are talking about ‘apps’, they can mean device-specific ‘download and install’ apps (aka native apps) or websites that respond to various screen sizes (aka responsive websites, web apps or HTML5) or even various hybrids of the two. Native apps are downloadable software applications that run using the device’s operating system code and APIs. They persist on the device and can access all the hardware features (camera, storage, phone capabilities etc). Because they run using native code, different versions must be created for each operating system. Examples of native apps include Spotify, Angry Birds, Instagram and Skype. Responsive web design is a design approach that optimises users’ viewing experiences across a wide range of devices. When a responsive website is accessed via a mobile phone, it is sometimes referred to as a ‘mobile web app’. Responsive websites are built using open web standards (HTML, CSS, javascript etc) and they run inside a device’s web browser. Examples include GOV.UK, FT webapp, BBC Sport. If there is a market for native apps, why should the government monopolise it? There’s a vibrant market of third party native app developers using government data and APIs. The government’s position is that native and hybrid apps are currently rarely justified. We are backing open standards rather than risking proliferation of parallel versions of services as devices proliferate. While people spend as much time using apps as using mobile web the vast majority of app use is for gaming and social networking. For ‘utility’ needs, such as those met by government services, the mobile web is preferred to native apps We recognise that there’ll be a few exceptions. To help you assess whether your case is likely to be considered an exception, consider whether you’ve met the necessary conditions: Condition 1: Your web service is already designed to be responsive Condition 2: The service or the content you’re looking to build an app for is already open to third parties via APIs or as open data NOTE: If these conditions are not in place, it is unlikely that your app proposal will be approved. If you believe there are compelling reasons why these conditions have not been met, please set them out in your proposal. In addition to the evidence requested above, all digital spend for the development of standalone mobile apps is subject to the GDS spend approval. Guidance (including details about response times) on the process can be found on GOV.UK. If you have any queries, please contact GDS.","description":"The government’s position is that native apps are currently rarely justified. At the October 2012 Digital Leaders meeting, the position was clarified: native apps could not be developed without Cabinet Office approval. The November 2012 Digital Strategy says:","link":"/service-manual/making-software/standalone-apps.html"},{"title":"Testing in an agile environment","indexable_content":"Building quality in Everyone is responsible for quality Fast feedback Tests are an asset of the product Faster delivery into production Clear and consistent view of testing Optimise value Types of testing Code testing Exploratory testing Load and performance testing Penetration testing Accessibility testing Crowdsourced testing Test your ideas build the best quality system you can make sure it does what the customer requires compete it a cost that everyone agrees can be afforded (cost being money, business change, risk etc.) building quality in everyone is responsible for quality fast feedback tests are an asset of the product faster delivery into production clear and constant view of testing optimise value the approach to testing where they fit in what they’re required to do help make the tough decisions direct development based on the risk of each user story help with the order of priority, based on the complexity of the system always planned in advance, in detail, including:     the specific aspects you want to explore       any need for extra data or a different system set-up to perform the testing      the specific aspects you want to explore any need for extra data or a different system set-up to perform the testing used to find and test the less obvious outcomes (automated tests prevent bugs whereas exploratory tests find them) normally timeboxed (runs for a fixed period of time) has a specific purpose – eg ‘I will spend x hours exploring y and z aspects of the system (though this may take me elsewhere and it may take more or less time, so I’ll use my judgement as I go)’ not always without automation – automation can be used to set up the data or to get a set of transactions (just not to run the tests) as developers are deeply involved in writing the code, it’s sometime difficult for them to see ways through the system that they hadn’t previously thought of it’s ideal for developers to be assigned to exploratory testing for a full day to allow the appropriate amount of context switching (the way a central processing unit changes from one task to another without the tasks conflicting) it’s preferable if they explore parts of the system that they haven’t really developed putting out a call for as many volunteers as possible across the organisation to put aside a few hours on a particular day for testing giving some guidance on what needs to be tested providing somewhere to log bugs sometimes motivating testers by creating a ‘leaderboard’ that shows who’s tested the most things the pre-release testing of additional devices/browsers the detailed checking of hundreds of pieces of content on GOV.UK against old content on DirectGov and BusinessLink The basics of any testing approach still apply in the agile world, but the aim of testing can be quite different. It’s important to recognise you are testing in the first place to: Too often the aim of testing is to validate what has been produced and nothing else. Your testing should be more about these 7 concepts: Use the vast majority of your efforts building quality in at the start, and test the quality throughout the project – not at the end when it’s too late. Define acceptance criteria standards for your user stories. You can either do this when you first write your user stories or later on in the form of acceptance tests when you start development work. Testing should confirm what you already know and understand to be true, so there should be no surprises in the latter stages. Service quality isn’t just a testing issue. The quality of a system is defined by the people who create it. Your team should be able to see a problem in the quality of your system. Every person on the project should be taking action to increase quality and fix issues. A successful agile project relies on fast feedback loops. Getting feedback and getting it fast means you can actually be agile and change when you need to change. Your testing should be about giving and getting that fast feedback – at a time when it’s useful. Automated code testing techniques have their place and you can use them, but don’t make them the centre of your approach to testing. When you build a test, build it to a standard that makes it suitable to use for testing multiple times throughout your project. It takes a lot of effort to do testing correctly, so don’t make it a throwaway exercise that has to start from scratch each time there’s a new release, or a new project. Write your automated tests with the same care and accuracy as production code. Testing is necessary and valuable to the programme, but the time it takes for your code to go live once written is time wasted. Use testing to get the fastest possible confirmation that everything is as you expect – or that it isn’t and needs to be fixed. It doesn’t always need to be exhaustive at every level, but it does need to be relevant to the current situation. Your team needs to agree on the necessary testing at each level, based on the preferences of the product owner and the likelihood of risk in the application itself. Everybody in your programme needs to understand and agree on: When done well, testing will inform you of the best way forward. It’ll also give you the best value in terms of effort used in various functional or nonfunctional areas and will: The most noticeable difference with testing in an agile project is that the majority of your test efforts will be centred on automated tests.  These tests run in continuous integration, which means that they form part of your codebase – every time you make a change to your code, your tests are automatically run. You’ll get immediate feedback on the quality of your code and it’ll help to prevent bugs happening at a later stage (when they’re expensive and complicated to resolve.) Read the guidance about testing code. Exploratory  testing is a term commonly used to describe unscripted manual testing. This is where the tester uses his or her knowledge, experience and intuition to go through through the software and identify bugs. The difference between unscripted and scripted tests is that a scripted test can only ever test a predetermined outcome. Exploratory testing (unscripted) doesn’t mean you don’t prepare for the testing. Exploratory testing is: The quality analysts or testers in your team carry out this type of testing. If your team is made up of only developers, you’ll need to put time aside for the developers to do the testing, but bear in mind that: When a manual test finds a defect, it’s important to always add an automated test to stop it from happening again. Read Cem Kaner on exploratory testing Read the guidance about load and performance testing Read the guidance about penetration testing Read the guidance about accessibility testing Crowdsourced testing doesn’t use a set group of people to carry out testing (known as outsource testing). It uses different people from different places in different jobs. It’s a good way of speeding up your manual testing and/or covering more ground. There are external companies who provide this as a service, but GDS do it internally by: Examples of where GDS used this effectively include: Don’t forget, don’t just test the product itself – test your ideas.   For information on how to do this read the guidance about user research.","description":"The basics of any testing approach still apply in the agile world, but the aim of testing can be quite different.","link":"/service-manual/making-software/testing-in-agile.html"},{"title":"Version control","indexable_content":"Commits Version control systems Not just code development stories bug reports or third-party documentation All software development projects must use a version control system. Version control allows you to track changes to code over time. This lets you quickly step back to an earlier version where necessary and you can annotate your changes with explanatory details to help future developers understand the process. Version control will also provide tools to audit who has made changes to the code and what has changed. Those updating the code should make small, discrete ‘commits’ of changes that are grouped according to their intention. They should be committed with a clear message explaining what the intention of the change was and (where appropriate) providing links to any supporting information like: At GDS we prefer to use a distributed version control system. This means that everyone involved in the process has a full copy of the code and of its history. This makes it easier for developers to create ‘branches’ in their code to explore new features or approaches without treading on the toes of those working on different aspects of the service. We use Git, which is one of the highest profile options. It also provides extra resilience: if the network is unavailable the developers can continue to work and make small, incremental commits, merging their changes back with everyone else’s at a later date. It’s a good idea to also use version control for other aspects of your work, not just code. We use the same version control tools to manage this document as we do our code, and the Government Digital Strategy was also produced that way.","description":"All software development projects must use a version control system. Version control allows you to track changes to code over time. This lets you quickly step back to an earlier version where necessary and you can annotate your changes with explanatory details to help future developers understand the process. Version control will also provide tools to audit who has made changes to the code and what has changed.","link":"/service-manual/making-software/version-control.html"},{"title":"Completion rate","indexable_content":"What you will be measuring How to measure completion rate When to measure completion rate Before the transaction starts Start and end points Dropouts Saving or resuming progress during a transaction Multiple endpoints Offline fulfilment Completion rate measurement frequency Testing the completion rate before launch Post-launch measurement Work out the number of completed transactions. Divide it by the number of started transactions, expressed as a percentage. what the outcome of the transaction will be who can use it how long it’s likely to take what they’ll need to complete it (eg a reference number or credit card) begins at the start page reaches the end page during the discovery phase you     make sure all transaction pages have unique URLs       develop a plan to measure completion rate throughout product development       assess the available analytics tools      make sure all transaction pages have unique URLs develop a plan to measure completion rate throughout product development assess the available analytics tools in your alpha phase you     benchmark task completion rate via usability testing and establish reasons for failed transactions       procure digital analytics tools      benchmark task completion rate via usability testing and establish reasons for failed transactions procure digital analytics tools during your beta phase you     conduct another round of usability testing to ensure that task completion rates improve      conduct another round of usability testing to ensure that task completion rates improve when a service is live you     analyse task completion rates so you can continually improve the service       carry out further usability testing to continually identify any usability problems and feed into service design      analyse task completion rates so you can continually improve the service carry out further usability testing to continually identify any usability problems and feed into service design registering applying submitting verifying amending unsubscribing When users are unable to complete a digital transaction, they can be pushed to use other channels. This leads to low levels of digital take-up and customer satisfaction, and a higher cost per transaction. Measuring end-to-end completion rates helps to identify whether users have problems completing a transaction. Analysing the drop-out rate at each step of a transaction will pinpoint the specific processes that users fail to complete. The completion rate measures the proportion of people who start a transaction and are able to complete it. A transaction is a self-contained process that’s defined by the service manager, and is typically only completed when a user has been through the whole thing end to end. A transaction completed only partially online means the user may complete a single process within the overall transaction, like booking an appointment or completing a part of an application. Before the transaction has started, users should clearly be told: Also provide users with the eligibility criteria and the costs to complete the transaction. This will help you to reduce dropouts later on in the transaction. Typically the eligibility criteria will be given on a single page. In some cases, there may be a set of pages that checks a user’s eligibility based on information they provide. Don’t make it possible for users to bypass your service’s start page via links or search engine results. Users who try to directly access another page in the transaction should be sent back to your start page, unless they are resuming a previously saved transaction. Transactions will begin and end on GOV.UK to allow GDS to monitor completion rates.  A transaction is considered to have been started and completed only when the user: Data on the number of started and completed transactions will be shared with service owners.  Service managers will be responsible for measuring and monitoring dropout rates within transactions which aren’t hosted on GOV.UK. You should analyse this data and use it to improve your service. Build your service with unique URLs for each step or page. This will make your service much easier to measure. Some services allow users to save a transaction mid flow and to resume it another time. For the purpose of completion rate, make it possible to match your saved transactions with resumed transactions so they can be treated as one continuous process. Consider applying a nominal time limit to saved transactions after which, if they haven’t been resumed, they are classed as failed. Alternatively, saved transactions could be set to expire after a given length of time.                           Some services have complex journeys and there may be several points where a transaction can be successfully completed. Service managers define these areas, and make sure they point to appropriate end pages on GOV.UK so they count towards the completed transaction total. Some transactional services have both digital and non-digital areas. For example, when granting a lasting power of attorney, users can start and finish the transaction online, but are required to print, sign and post a form in the middle of the process. In these situations treat the second online part of the transaction as a user continuing on from the first part. Where it’s not possible to match the two, treat the single digital parts of the service as separate tasks, with the completion rate measured for each. Offline parts of the process can still be measured, but this is likely to be done through qualitative feedback (eg from surveys, diary studies, focus groups). To successfully measure your service completion rate, GDS recommend that: Completion rate should be measured continuously. Once the service is live this will be done on GOV.UK. Before launch, measure completion rates with usability testing. This should be iteratively tested with at least 5 people. This should identify over 85% of usability problems. Users will be given a pre-determined set of tasks that reflect what needs to be done to use the service. These tasks will include all aspects of using the service that apply, like: If users are having difficulty completing tasks, carry out more user testing after each new development. Digital analytics will be the primary method for measuring task completion rates post launch. Please note that this relies on extra configuration in the analytics tool. It will not be available by default. The aim of your service team’s activities will be to continually improve completion rates by monitoring where users are dropping out of the transaction process, and testing out new designs for those pages. End-to-end completion rates will be piped automatically from GOV.UK’s digital analytics into the Performance Platform and will be publicly available from the point of launch. Further usability testing should also be carried out once a service has gone live to measure use of the service and identify any issues and improvements that can be made. ","description":"When users are unable to complete a digital transaction, they can be pushed to use other channels. This leads to low levels of digital take-up and customer satisfaction, and a higher cost per transaction.","link":"/service-manual/measurement/completion-rate.html"},{"title":"Measuring cost per transaction","indexable_content":"What you will be measuring How to measure cost per transaction Costs to include How often cost per transaction should be measured Cost per digital transaction Work out the total cost of providing the service. Divide the total cost of providing the service by the total number of completed transactions. Work out the total cost of providing the digital service. Divide the total cost by the total number of transactions completed digitally, including assisted digital transactions. accommodation, including capital charges for freehold properties fixtures and fittings maintenance utilities office equipment, including IT systems postage, printing, telecommunications total employment costs of those providing the service, including training overheads, eg (shares of) payroll, audit, top management costs, legal services etc raw materials and stocks research and development depreciation of start up and one-off capital items taxes (VAT, council tax, stamp duty etc) capital charges (if they were not met separately when the service was established) speculative or actual insurance premiums fees to sub-contractors distribution costs, including transport advertising bad debts provisions enforcement costs replacement costs of items notionally insured start up costs (those which can be capitalised in the accounts) Cost per transaction is an important measure of a service’s efficiency. As services become more efficient, the cost per transaction will fall. Cost per transaction is about how much it costs government to provide each completed transaction. You should measure the difference in the cost of the transaction through each available channel. Understanding these costs will help you to accurately forecast savings and build a strong business case for making services digital by default. The total cost includes all fixed and variable costs of the transaction through a given channel, including overheads. It does not include start up costs. Where resources (eg call centres) are shared with other services, costs should be split appropriately. For example, if half of all calls received relate to a specific service, then assign 50% of the call centre costs to that service. Where processes and costs are common to more than one channel (eg processing wet signatures for passports, or printing driving licences), split costs appropriately. For example, if half of all transactions are completed digitally, then assign 50% of the common costs to the digital channel. In the full cost of the transaction, include: Exclude these costs from the transaction: Measure cost per transaction on a quarterly basis. It should cover the last 12 months to eliminate seasonal fluctuations. Measure cost per transaction for your existing service – if there is one – to create a baseline for comparing the future cost per transaction. When the digital service is exposed to real users (whether in alpha or beta) include it in the reported cost per transaction.","description":"Cost per transaction is an important measure of a service’s efficiency. As services become more efficient, the cost per transaction will fall.","link":"/service-manual/measurement/cost-per-transaction.html"},{"title":"Digital take-up","indexable_content":"What you’ll be measuring Assisted digital support How to measure digital take-up The level of digital take-up to aim for Services for users who are more or less than 82% online Achieving your digital take-up targets How to increase digital take-up How should I increase digital take-up? Take the number of completed digital transactions in a calendar month. Divide it by the total number of transactions in the same month, expressed as a percentage. age gender socio-economic status Digital take-up is a long term strategic measure of how well digital by default service is working. You’ll monitor this on a monthly basis to track that it’s on course with the desired performance levels. Digital take-up is the proportion of transactions completed online. A digital transaction is one where the main interaction between the user and the service has been through a digital user interface. Classify transactions as digital only if a significant part of a transaction is completed online. Assisted digital support is provided for people who are not able to use digital services without help. Services should be designed so that the need for assisted digital support is minimal. Assisted digital transactions should be counted separately from digital transactions and should not count towards a service’s digital take-up.  Read our guidance on assisted digital for more information. The total number of transactions will be made up of all non-digital transactions, including call centre transactions and paper-based transactions. Record and collect this data to make sure your digital take-up figure is accurate. The Digital Landscape Research found that 82% of adults in the UK are online, so it’s sensible to aim for something in that region. However, about a third of those online have never accessed government information or services online, so this will involve a significant channel shift for most services. Who’s online, and how often, varies by:  Use the Digital Landscape Research as a guide. It has a useful breakdown of who’s online and offline. Digital take-up is a long-term strategic measure. Look to achieve the target level within 5 years of launch. Read our guidance on increasing digital take-up. Read guidance on ways of encouraging the use of digital by default services.","description":"Digital take-up is a long term strategic measure of how well digital by default service is working. You’ll monitor this on a monthly basis to track that it’s on course with the desired performance levels.","link":"/service-manual/measurement/digital-takeup.html"},{"title":"Key performance indicators","indexable_content":"Understand user and stakeholder needs Decide what to measure Further information Check your progress Check your progress cost per transaction user satisfaction completion rate digital take-up technical requirements, like improving response time or server resilience financial requirements, like reducing costs or increasing revenue customer service requirements to improve customer satisfaction wider strategic aims to reduce fraud and error Have you identified who your users and/or stakeholders are? Have you clearly expressed your users and stakeholders’ needs? Have you prioritised services? the number of downloads a transaction’s drop off rate the payment success rate Are your metrics simple, actionable and collectible? Have you mapped your metrics to the relevant audience? Have you identified where your metrics will come from? Do you know how frequently your users need performance information? the NAO report Digital Britain One: Shared infrastructure and services for government online, which identified that there was a lack of information on the costs and associated benefits of digital services Occam’s Razor, a blog by Avinash Kaushik with useful advice about metrics, KPIs and analytics (for example this article on how to set good performance indicators) the Performance Platform by GDS, which will allow aggregation of data from a range of sources including web analytics, survey and finance data Think about other useful key performance indicators (KPIs), specific to your service, that will help you measure and improve its performance. Measure these in addition to the four core KPIs: Different sets of users within each department will typically have their own requirements for each service. On a daily basis, a service manager will need to see a range of metrics that help them make decisions about the service. Identify the various stakeholders in your organisation and prioritise their requirements. These could be: These needs will typically be reflected in the organisation’s business objectives or in the business case for transforming a service. An approach that worked well for GOV.UK was to run a brainstorming session to generate a long list of metrics. This list was then narrowed down by asking senior managers, ‘if you had to choose one KPI, what would it be?’ Develop simple, actionable and collectible metrics based on your understanding of user and stakeholder needs. Identify where that information will come from and how frequently it will be needed. Think especially about the functions your service is relying on and try to identify some KPIs to ensure you are constantly monitoring and improving these functions. Measure things like: Record every event generated by the system even if it’s not currently of interest. Measure everything (where it’s cost-effective and practical to do so). This will make it much easier to revise your chosen metrics later on, and customise data visualisations to different audiences. You might find these texts useful to identify possible KPIs your service might measure:","description":"Think about other useful key performance indicators (KPIs), specific to your service, that will help you measure and improve its performance. Measure these in addition to the four core KPIs:","link":"/service-manual/measurement/other-kpis.html"},{"title":"Performance Platform","indexable_content":"What the platform will offer Put data in Get useful insights out Further information improve services meet the service standard gain insight by comparing data across multiple services and sources visualisations alerts dashboards feeds service desks infrastructure analytics surveys finance data make it possible for government to make decisions based on data do the hard work to make data collection, retrieval, and presentation simple for users be open to any government service supplying their data and customising their outputs be open to the public be independent of proprietary monitoring software, eg Google Analytics automate routine analysis and visualisation, and use insights to provoke further detailed analysis relevant to all services relevant to similar groups of services (eg applying for something, requesting information, booking an appointment) Key Performance Indicators The Performance Platform Contact the team The Performance Platform is for service managers that need to be able to make data-driven decisions based on both digital and non‑digital sources. It’s a toolkit that allows you to supply service data in return for alerts, visualisations and structured data, which can then be used to: The Performance Platform will provide: This will help with making timely and transparent decisions about your service and the content that government offers. The platform will combine different sources of information: Combining these sources will produce information that encourages a greater understanding of the data, rather than simply communicating facts. It will: You will be able to supply your service performance data via standard Application Programming Interfaces (APIs). These APIs will be defined, documented and supported by the Performance Platform team. Your service will be responsible for collecting and supplying data to the API. Your service can also send additional data. For instance, there might be data that’s important but there isn’t yet an existing module to make use of it. Your service will be able to start sending it to the platform straight away, so that when an appropriate module is available, the historical data is already there. Designed by the Performance Platform team, the platform will be made up of a series of modules that you can choose from. The team will work closely with you and your service in this process.  When a new module is added, it will be offered to all services on the platform. Each service will have the ability to decide whether or not to use it. There will be default modules that are: You’ll also have the ability to develop your own custom modules for your service.","description":"The Performance Platform is for service\nmanagers that need to be able to make data-driven decisions based on both\ndigital and non‑digital sources.","link":"/service-manual/measurement/performance-platform.html"},{"title":"User satisfaction","indexable_content":"What you will be measuring How to measure user satisfaction What happens when a user exits the transaction midway through When to measure user satisfaction Post-launch measurement Further reading Discovery Alpha Beta Production (live) very satisfied satisfied neither satisfied or dissatisfied dissatisfied very dissatisfied benchmark your existing service for future comparison develop a plan to measure user satisfaction throughout product development measure user satisfaction via remote usability testing and/or satisfaction survey identify why people are dissatisfied completing transactions and take steps to improve user satisfaction levels measure user satisfaction continually and monitor results on at least a monthly basis carry out a more comprehensive user demographics, usage and attitudes survey every 6 months A good service makes it possible for users to successfully complete tasks. User satisfaction helps you to measure the overall quality of the service experience. Many government transactions are mandatory for users, so they’re not naturally enjoyable, and are sometimes referred to as grudge transactions. Still, try to make them as pleasant as possible for users, who may be nervous or stressed when interacting with the government. Asking users how satisfied they are with a service can provide you with a measure of all the parts that contribute to the overall user experience, like ease of use, navigation and design. Define user satisfaction as the percentage of people who answered either “very satisfied” or “satisfied” on a 5-point scale in response to this question: Q: Overall, how satisfied were you with this [eg car tax] service today? GOV.UK will provide a user satisfaction survey at the end of your transactional service and make this data available. Measure all user journeys through your transaction to better understand dropout points. If a user drops out you should try and collect some data around the quality of your service to explain why they didn’t complete the transaction. For example,‘please tell us why are you unable to complete this transaction.’ Although this survey will not be contribute to the overall measure of user satisfaction it will help you understand service dropout points and find ways to improve transactions. To successfully measure the user satisfaction of your service, we recommend that you measure through all stages of your service. During the discovery phase you should: Benchmark user satisfaction via remote usability testing and/or a satisfaction survey. During your beta phase: Once your service is live you should: An exit survey will be run continuously on your service, and will report on user satisfaction on a monthly basis. You can use this data to improve your service. Analyse the major factors generating satisfaction with the service. You can do this by asking additional questions (eg on ease of use, accuracy, look and feel). You can then determine which of those factors is most positively contributing to user satisfaction and prioritise where to direct ongoing design efforts. Survey design","description":"A good service makes it possible for users to successfully complete tasks. User satisfaction helps you to measure the overall quality of the service experience.","link":"/service-manual/measurement/user-satisfaction.html"},{"title":"Using data","indexable_content":"Understand user needs/decide what to measure Install and configure platforms Establish a baseline Aggregate data Analyse and visualise data Monitor, iterate, and improve Further reading identify the effect of your communications or design initiatives reveal seasonal variations in performance dashboards reports alerts channel used to access service – through which channel(s) did the user find out about and attempt to use the service? new compared with repeat visitors – are first time users behaving differently to those who have used the service before? geographical region – how popular is the digital service by region and how does that compare with online penetration in general? product type – does the user experience vary depending on the type of product or service? value – is performance dependent on the monetary value of the product or service being sought? dashboards – based on objectives and will help inform decisions, often with the help of real-time data reports – providing regular, scheduled snapshots of data, but tend to need extra context and time to absorb alerts – used to inform the users about a change or an event, often using attention-grabbing tools to do so keeping charts plain – don’t use shading, 3D or other distracting effects removing clutter – don’t use trend lines, grid lines, unnecessary labelling not using pie charts – they require more space and are harder to read than bar charts using text where a chart adds nothing Test a range of actions for improving performance and monitor to see which work well. Pilot these on a portion of your users to minimise risk. Implement the best performing solutions widely and then repeat this process continuously as what you measure will change over the course of your product or project’s lifetime. price – change the price eg to attract people to the digital channel product – improve the user experience (eg from user feedback, user testing, A/B testing, multivariate testing) placement – place the digital service URL on printed materials and in voice recordings promotion – increase your use of email and social media to encourage repeated use of your digital service an article on online customer satisfaction scores for retailers, based on the Customer Satisfaction Index, written for The Guardian The role of the data scientist by Mike Loukides Costing customer time (PDF, 79k), a method for calculating the cost of a transaction for both the service provider and the user (HMRC developed a method for calculating the cost of a users time when interacting with government – this is important because some channels may be quicker to use than others) Designing with Data, an excellent book by Brian Suda which helps you to design beautiful and powerful data visualisations Juice Analytics, a website with lots of useful resources on how to design and develop useful data visualisations and dashboards Edward Tufte’s The Visual Display of Quantitative Information, an original piece of work on data visualisation and introduces the concept of chartjunk the Flowing Data blog by Nathan Yau, a useful source of data visualisation news the D3 Gallery, a stunning collection of data visualisations a nicely presented overview of some of the tools available for data visualisation this article in Wired, which shows how A/B testing was used to good effect in Obama’s election campaign this article in eConsultancy, which shows how multivariate testing was used to improve conversion rates at Lovefilm Simply collecting information about how your service is running isn’t enough to make judgements about how to improve it. A process of continual iteration and close measurement will help you to see what needs improvement and how to investigate ways of improving your service.  Make sure the core key performance indicators (KPIs) established in the service standard, and any other KPIs you choose to measure, accurately reflect the needs of your users and stakeholders. This will allow you to measure the ability of your service to meet user needs. While building your service, make sure that appropriate analytics tools are being used to monitor the service, collecting the data necessary to produce accurate and timely measurements. For more information, read our guidance on choosing analytics tools. Establish a ‘baseline’ based on the current performance trends of each channel. Use this to judge the effectiveness of any changes to your service. This will help you pinpoint the effectiveness of your initiatives, and identify what worked. Look at performance trends over time, rather than taking a snapshot at a particular point. Peaks and dips in performance can then be measured in relation to this base (or trend) line, which helps to: Collect and combine performance information from multiple sources and across multiple channels. Make sure you understand what this will mean in terms of system requirements. Combining data often reveals useful insights, eg into service efficiency (cost per transaction and total cost to serve) or the amount of usage by channel (percentage digital take-up vs post, phone etc). Communicate performance information to your users through the appropriate: Highlight specific segments that you know users are interested in, and make sure that your visualisations are simple, useful and contain minimal amounts of chartjunk. Typical segments include: By making your visualisations easy to understand you significantly increase the likelihood that the information will be used to improve your service. Best practices include: Any service that meets user needs will include an element of user feedback. Monitor this and act on it. Doing so will help to continually improve the service for your users. Many options are available for improving the overall performance of your service. The following examples are based on the 4 Ps of marketing: Taking an iterative approach to service development will increase the speed of improvements and minimises the risk of failure. Don’t wait until the end to do this – do it continuously throughout the process. To find out more about ways to use data, try reading:","description":"Simply collecting information about how your service is running isn’t enough to make judgements about how to improve it. A process of continual iteration and close measurement will help you to see what needs improvement and how to investigate ways of improving your service.","link":"/service-manual/measurement/using-data.html"},{"title":"Devops","indexable_content":"Why Devops Good habits Warning signs Related guides in the Service Manual Further reading Culture Automation Measurement Sharing development quality assurance operations business based in different locations work for different organisations under completely different management structures culture automation measurement sharing release management (releasing software) provisioning configuration management systems integration monitoring orchestration (the arrangement and maintenance of complex computer systems) testing those who build and test software those that run it in production quality release management performance have a shared sense of ownership of the service have a shared sense of the problem develop a culture of making measurable improvements to how things work cross-functional teams – make sure your teams are made up of people from different functions (this helps with the team owning the end-to-end quality of service and makes it easier to break down silos) widely shared metrics – it’s important for everyone to know what ‘good’ looks like so share high and low level metrics as widely as possible as it builds understanding automating repetitive tasks – use software development to automate tasks across the service as it:     encourages a better understanding of the whole service       frees up smart people from doing repetitive manual tasks      encourages a better understanding of the whole service frees up smart people from doing repetitive manual tasks post-mortems – issues will happen so it’s critical that everyone across different teams learns from them; running post-mortems (an analysis session after an event) with people from different groups is a great way of spreading knowledge regular releases – the capacity for releasing software is often limited in siloed organisations, because the responsibilities of the different parts of the release are often spread out across teams – getting to a point where you can release regularly (even many times a day) requires extreme collaboration and clever automation Devops tools (nearly always marketing) a Devops team (in many cases this is just a new silo of skills and knowledge) Devops as a job title (you wouldn’t call someone “an agile”) Configuration Management Monitoring Release Management What Devops Means to Me what is this Devops thing anyway? what is Devops (and the wall of confusion) there’s no such thing as a “Devops Team” Devops is a cultural and professional movement in response to the mistakes commonly made by large organisations. Often organisations will have very separate units for: In extreme cases these units may be: Communication costs between these units, and their individual incentives, leads to slow delivery and a mountain of interconnected processes. This is what Devops aims to correct. It is not a methodology or framework, but a set of principles and a willingness to break down silos. Specifically Devops is all about: Devops needs a change in attitude so shared ownership and collaboration are the common working practices in building and managing a service. This culture change is especially important for established organisations. Many business processes are ready to be automated. Automation removes manual, error-prone tasks – allowing people to concentrate on the quality of the service. Common areas that benefit from automation are: Data can be incredibly powerful for implementing change, especially when it’s used to get people from different groups involved in the quality of the end-to-end service delivery. Collecting information from different teams and being able to compare it across former silos can implement change on its own. People from different backgrounds (ie development and operations) often have different, but overlapping skill sets. Sharing between groups will spread an understanding of the different areas behind a successful service, so encourage it. Resolving issues will then be more about working together and not negotiating contracts. The quality of your service will be compromised if teams can’t work together, specifically: The root cause is often functional silos; when one group owns a specific area (say quality) it’s easy for other areas to assume that it’s no longer their concern. This attitude is toxic, especially in areas such as: High quality digital services need to be able to adapt quickly to user needs, and this can only happen with close collaboration between different groups. Make sure the groups in your team: Devops isn’t a project management methodology, but use these good habits in your organisation. While not unique to Devops, they help with breaking down silos when used with the above principles: Like agile, the term Devops is often used for marketing or promotional purposes. This leads to a few common usages, which aren’t necessarily in keeping with what’s been said here. Watch out for: Those interested in Devops are often also interested in:","description":"Devops is a cultural and professional movement in response to the mistakes commonly made by large organisations. Often organisations will have very separate units for:","link":"/service-manual/operations/devops.html"},{"title":"Helpdesk","indexable_content":"Volume forecast: how many and what type of enquiry Handling time: the time needed to deal with the contacts Handling time: time it takes to deal with an enquiry Service level: how fast you respond to user enquiries Staffing and scheduling Data collection and reporting Going even further handle user requests for information help direct users to the information they want email phone online chat Twitter Facebook surface mail volume handling time service level improve and refine the forecasts you made previously create long-term projections about the performance of your helpdesk the group itself individual members of staff You need to form a helpdesk as part of providing a high quality service. A helpdesk is a dedicated group of specialists – or current staff dedicating some of their time – that: Your planning will benefit from the use of traditional contact centre management and planning skill sets. If you don’t have access to staff with these skills, you may have to work closely with other groups who do in a consultancy capacity. This may include the team at GDS, or outside specialists. You need to have an idea of the volume and type of enquiries you’ll receive. In many cases, you can use historical data for similar services as a baseline. Also take into account the contact methods you intend to support, such as: To help with your planning, make sure each of these channels has a separate volume forecast. These forecasts will either inform, or help to make, decisions on what technology you’ll use to route, handle, and store the historical data of these enquiries. Work out the average handling time (AHT) and variance for each type of enquiry. If you’re building AHT with deep historical data, you may be able to model the AHT separately for each kind of enquiry. Use the best historical data available to calculate the average number of enquiries per day a single agent can handle when dealing with a representative mix of enquiries types. Work out the expected level of service that should be maintained by your helpdesk. Generally, service level (SL or SLA) is worked out as x per cent of enquiries resolved in y time units. For example, you might plan to answer 80% of your incoming email enquiries in 24 hours. You need to know the desired service level so you can work out how many agents you’ll need for your helpdesk. If your staffing is constrained by other factors, use this value as a constant and aim for the best service level that can be achieved at a certain volume level. You can only finalise your overall demand forecast and work out staff numbers when you have finished planning for: The same historical data you used to create your enquiry forecast will likely give you an idea of how the enquiries will be received throughout the planning period (likely a week). This will then help guide you on how to schedule your helpdesk staff, so they can achieve the service level you’ve planned for. Once your helpdesk is operational and actually supporting users, collect data on all the areas you planned to collect information on. This data can then be used to: Plan on how to measure the performance of your helpdesk from the very beginning. Develop at least minimal reporting so you can evaluate the performance of: These are good starting points to consider as and when you plan to develop new digital products and initiatives. There’s quite an extensive amount of online information that expands on the points made here, including some free planning tools. However, as with all areas of specialised knowledge, use the services of an experienced analyst rather than depending entirely on online sources for guidance.","description":"You need to form a helpdesk as part of providing a high quality service. A helpdesk is a dedicated group of specialists – or current staff dedicating some of their time – that:","link":"/service-manual/operations/helpdesk.html"},{"title":"Hosting","indexable_content":"Decide how you’ll host your application Types of suppliers Make your decisions carefully Further information Ownership Co-location Shared or managed hosting Infrastructure as a Service Platform as a Service assess different options shortlist suppliers interview suppliers make a decision a hands-on technical understanding of the service and underlying software – these people are essential knowledge of the available procurement options knowledge of acceptable costs ownership co-location shared or managed hosting infrastructure as a service platform as a service infrastructure as a service platform as a service the technical requirements of your software applications your future capacity requirements user support reliance and redundancy mandated government information security requirements government network connectivity the cost of service flexibility and on-demand billing different supplier operating models Why IaaS? – a blog post about why the Government Digital Service chose an infrastructure as a service for GOV.UK G-Cloud – the procurement framework intended for purchasing infrastructure as a service, platform as a service and software as a service products You need servers to run your software. The information here will help you decide how you host your applications and the things to consider when selecting a vendor. To begin with, involve a small cross-functional group of people who will quickly: Make sure the people in your group have: It’s important that you keep good notes from any interviews or deliberation sessions and assess different suppliers equally. One way to do this is to use a scoring matrix. Know that it’s very common to use multiple suppliers. This can be because they offer different but compatible services, or additional redundancy. It can be technically challenging to do this, but can provide extra resilience for larger projects. Suppliers offer a number of different approaches to hosting services, which can make it difficult to compare them. The following information is intended as only a brief introduction of each type, covering: This advice is complicated because many service providers redefine the meanings of hosting terms for marketing reasons, especially: These terms are marketable at the moment and often used incorrectly, so always look into the details of the services being offered. If you have a large project with very specific requirements, you may decide that purchasing hardware and even running a dedicated data centre is suitable. The costs and timescales involved here are very high and in most cases it’s unlikely to be your best option. Many providers offer co-location services – you purchase your own hardware and rent space in a managed data centre.  This provides a great deal of flexibility, but can introduce lead times and other physical constraints. It also requires a wide range of technical specialist skills. Lots of service providers have a shared or managed hosting option. This tends to mean renting specific virtual or physical machines for fixed periods of time. Different suppliers offer different management services; some just manage the underlying machine while others will support the operating system and even specific applications running on the machines. In the last several years this has become a common approach to managing hosting and infrastructure requirements. It tends to involve a capability to rapidly add or remove capacity, often in minutes, and to be billed only for what is used. This provides a great deal of flexibility and the ability to keep costs down, but also requires a degree of technical skill to manage well. Similar to infrastructure as a service, platform as a service offerings tend to allow for quickly adding or removing capacity and precise ‘pay on demand’ pricing. The difference is that you are considered completely separate from the underlying infrastructure. The unit here is the running application, not a virtual or physical machine. Using a platform as a service places a number of restrictions on the software architecture, but can move the support burden for parts of the stack onto the supplier. Before making a decision about your hosting supplier, weigh up a wide range of different factors, including: Unfortunately this is a technical field with many options. Seemingly similar services can have wildly different structures or different cost models that can result in large differences in your total cost of ownership (cost of the project).  It’s important to involve technical colleagues or trusted third parties in any discussions and/or decisions.","description":"You need servers to run your software. The information here will help you decide how you host your applications and the things to consider when selecting a vendor.","link":"/service-manual/operations/hosting.html"},{"title":"Load and performance testing","indexable_content":"Capacity planning Test things yourself, as well as with vendors Types of testing Further reading Load testing Performance testing capacity planning work at the beginning specific load and performance testing will traffic or database load increase every month as the site grows are some days or months particularly spikey (self-assessment deadline or bank holidays) performance testing load testing the software running the site the networks, proxies and caches involved in serving traffic over the internet Matt Cutts from Google on page speed distributed Denial of Service attacks capacity planning presentation History is littered with countless government projects that collapsed under load or worked so slowly it frustrated users. As a government service, it’s important that your systems and applications are highly efficient and can deal with expected (and unexpected) levels of traffic. This means you need to do: Capacity planning is the process of determining what amount of infrastructure and software is required to run a live system. You also need to look at how this changes over time: This planning will help with both estimating your ongoing costs as well as setting up realistic load and performance tests. Several companies offer products and services around load testing. In many cases it’s useful for you to work with these companies, especially for final testing or the testing of finished components. Still, make sure you also have the capability within your development team to do more specific load and performance testing. Having this makes it possible to have a rapid iterative development style. Without it, your system can have scalability or performance issues that will only be caught later on – when they are much harder to fix. To make sure your system is efficient and able to deal with high levels or traffic, carry out: Although related and tested in similar ways, load testing and performance testing are done for different reasons. It’s useful for you to understand the subtle differences. Make sure you consider both types when testing and analysing results. This is when a site or application is given the most it can handle, while still being able to function properly. GDS test sites and applications under realistic load (traffic) to make sure that sites and applications work for the people using them. Load testing should involve testing load in excess of your expected traffic levels. Do this so you can simulate certain types of Denial of Service (DoS) attack, including a Distributed Denial of Service (DDoS) attack. This is about testing stability and responsiveness. Even if a site or application is able to scale out successfully it doesn’t mean it’s fast. Site performance is dependent on many factors like:","description":"History is littered with countless government projects that collapsed under load or worked so slowly it frustrated users.","link":"/service-manual/operations/load-and-performance-testing.html"},{"title":"Managing user support","indexable_content":"Arrange enquiry data into subgroups Potential subgroup categories Send data to appropriate groups Treating enquiries as defects Improvement projects Enquiry type Requester details Reply type Enquiry status Enquiry category or categories Service category or categories Root cause category or categories how complex your enquiries are the sophistication of the systems you use to handle them channel (phone, email, chat, social media, surface mail etc) the target group that can act on the feedback question problem complaint Freedom of Information (FOI) request non-actionable rant support level priority internal resolving group resolving agent(s) day/date/time received day/date/time resolved page failing to load database down forgotten password user error software bug Now you’ve created a helpdesk, it’s time to figure out how to make the best use of user feedback so you can improve your service and your user experience. Your ability to act quickly and constructively on user feedback depends on how well you can arrange enquiries for analysis. How you choose to put enquiries into relevant subgroups will depend on: These factors will decide if the process of arranging enquiries will be carried out manually (staff create the metadata for the enquiries they handle) or be automated (the software you use for handling enquiries will add most of the metadata automatically). At the very least, arrange enquiries by: In addition to the very basic channel and target arrangements mentioned above, the following categories may be suitable. Is the enquiry a: Given user privacy concerns, spend time thinking about how you’ll collect data on your users. At the very least, gather minimal information on who is requesting assistance. Is a reply necessary/expected or not? Is an enquiry open, pending some other action, solved etc Some internal sectional or functional categories along which you’ll want to be able to stratify–think of separate URLs or sections on a large website as a concrete example. Capture information on the different aspects of the handling processes. You’ll want to have information on: The ultimate reason for contacting helpdesk, eg: As with most of these, tailor these categories based on the specifics of your product and support model. Now that you’ve gathered your data, you need to decide how to share it. Sharing data is easier if all the data can be handled and used directly within your organisation. However, it’s likely that different teams will act on different types of problems, and some may be outside of your department. If possible through your system, use automation for sharing data. Treating each enquiry from a user as being due to a defect in the service itself is a very ambitious improvement model. Here enquiries are either seen as a fault in your support model for your service, or in the communications processes you use to handle enquiries. While this can exaggerate the reason for some kinds of contact (is a user’s email of thanks and appreciation genuinely a defect?) it’s an excellent way to direct critical thinking on how you provide service. Once the enquiry-level improvement data has been sent to the right group, begin doing analysis for improving processes. Join your enquiry level data with cost data to help prioritise which areas to target first. But you can also simply use the number (or percentage) of affected users as a tool for prioritising. In any case, make sure you have access to staff with experience in process analysis and improvement, ideally with relevant contact centre or technology experience. Directly involving the individuals who handle enquiries and those who built the service into improvement projects will produce better and faster results.","description":"Now you’ve created a helpdesk, it’s time to figure out how to make the best use of user feedback so you can improve your service and your user experience.","link":"/service-manual/operations/managing-user-support.html"},{"title":"Monitoring","indexable_content":"Set up monitoring early Include high-level checks Errors are interesting Make data widely available Further reading be alerted to problems affecting the availability of the service so they can be resolved help with capacity planning activities by providing metrics over time identify potential future problems find areas of improvement, eg badly performing systems or inefficient services be able to identify the root cause of an outage after the fact, based on data collected during the problematic event alert you to problems help identify the cause at the same time ground conversations about low-level problems (disk space, slow performance) in relation to service performance a user problem attacks in progress failing systems problems with capacity monitoring system dashboards interactive tools reports the Radiating Information blog post gives examples of some of the monitoring dashboards used while building GOV.UK open source monitoring explains some of the terminology and available open-source tools Any online application should have tools dedicated to alerting any problems to those running the service. These can be low-level issues involving the infrastructure supporting the service, to a sudden high rate of user errors. Know the current state of your service and infrastructure. It will help you to identify problems before they happen, as well as alert you to issues that need your immediate attention. The main goals of monitoring are to: Don’t leave monitoring to the end, tacked on as part of running the final production service.  Talk about monitoring and agree on an approach. This way you’re more likely to build useful checks as you go along. Writing tests at the same time as writing code is common, monitoring checks can be viewed as tests for the running system. Often monitoring is seen through a very technical lens, so teams may only look at web application performance, available disk space or memory usage. Although these are important, it’s also essential to track these alongside more business-related metrics. For example, being able to compare page-loading tests with failed transactions and application errors can:  When you have an error, record it and track it over time. Errors always contain interesting information, which can be about: It’s important to be able to see errors that are part of the overall system and errors specifically related to a particular application or machine. Make the following as widely available to everyone as possible, and also useful to teams outside of operations and systems administration:","description":"Any online application should have tools dedicated to alerting any problems to those running the service. These can be low-level issues involving the infrastructure supporting the service, to a sudden high rate of user errors.","link":"/service-manual/operations/monitoring.html"},{"title":"Operating a service.gov.uk subdomain","indexable_content":"One entry point Creating a domain Subdomains Transport Layer Security Cookies robots.txt and root level redirections Origin servers for CDN-based provider of DDOS protection Emails sent to service users Lifecycle of service subdomains www.servicename.service.gov.uk is for the public facing, dynamic web pages that make up your service. assets.servicename.service.gov.uk is for assets such as static images and shared javascript files needed to run your live service (note: written content about the service, such as guides to eligibility or detailed guidance for applicants, SHOULD be on GOV.UK) admin.servicename.service.gov.uk is for features that enable non-technical staff to run the service (eg contact centre staff might use this subdomain to access and process work items where human judgement is needed)      have a robots.txt file on the www, admin and assets subdomains asking search engines not to index any part of the site. Example content for robots.txt is given below, and more details can be found on The Web Robots Pages:      User-agent: * Disallow: /          have an HTTP 301 redirection from the top-level index page of the www and assets subdomains to the relevant start page on GOV.UK. (Note: this means that the service start page on GOV.UK SHOULD NOT link to the root of the www domain.)    www-production.servicename.service.gov.uk admin-production.servicename.service.gov.uk assets-production.servicename.service.gov.uk the DDOS protection provider’s servers the locations where the service itself is being developed and/or managed continuing to use SSL serving a redirect from your service to the GOV.UK start page    Note: This document is written as a ‘standard’, and as such uses the words MUST, SHOULD, MAY and MUST NOT as defined in RFC 2119.     Note: This does not apply to the set of interactive tools on GOV.UK known as ‘smart answers’ which are developed and maintained by GDS in partnership with other government departments.  Government offers a number of different digital services to citizens. While the start and end of a user’s journey will be on GOV.UK, the service itself will typically be hosted elsewhere, and will need a different domain name as a result. This page describes the use of service.gov.uk subdomains for hosting digital services. Note: This document is written as a ‘standard’, and as such uses the words MUST, SHOULD, MAY and MUST NOT as defined in RFC 2119. Every digital service offered by the UK government MUST have a single, well-known place on the internet where users can go to when they want to use the service. That well-known place will be the relevant start page on GOV.UK – for instance, the DVLA’s tax disc service is at https://www.gov.uk/tax-disc. Service managers MUST NOT advertise any URL other than that of the GOV.UK start page as the starting point for the relevant service. This is what gets printed in literature and used in email signatures, TV adverts etc. The start page URL for a given service will be allocated by GDS based on discussions with the service manager and analysis of user behaviour, search referrals and other relevant data. The transactional part of a service – the dynamically generated pages where users interact with the service – will typically not be hosted on the www.gov.uk domain. That means that each service needs its own domain name for the transactional part of the service. Note: This does not apply to the set of interactive tools on GOV.UK known as ‘smart answers’ which are developed and maintained by GDS in partnership with other government departments. For all new digital government services going live from 1 April 2013 GDS will create a domain name of the form servicename.service.gov.uk (where “servicename” is a plain English description of the service agreed between the relevant dept/agency and the Government Digital Service). This will introduce consistency across central government domains for digital services and remove the dependency on departmental subdomains (which are of course vulnerable to machinery of government changes) and the now-retired DirectGov and BusinessLink online brands. The process of obtaining a service.gov.uk subdomain begins either when the service manager asks a Government Digital Service product manager for a start page on GOV.UK (for services already under development as of 13 March 2013) or when the service manager asks for a subdomain to be created via the GOV.UK service desks’s government contact form (for services where development starts after 13 March 2013).  Subdomains of service.gov.uk SHOULD describe the service (eg lastingpowerofattorney.service.gov.uk) and should not contain the name of the service owning department or agency (eg ministryofmagicwandregistration.service.gov.uk) The service-owning dept/agency will be given delegated authority to manage the domain and its subdomains, although in some cases this work will be carried out by third party suppliers. This section gives some guidance about which subdomains a service manager should create once they have been given control of servicename.service.gov.uk. Maximum number of visible subdomains The user-facing live service SHOULD be operated using at most three user-visible subdomains of servicename.service.gov.uk: You SHOULD NOT create separate domains for application programming interfaces (APIs) unless there’s a really good reason to have a completely separate domain. (Really good reasons are few and far between.) Service managers should notify the Government Digital Service technical architects (via your transformation team contact) if you intend to create user-visible subdomains other than the three listed above. We’re developing some patterns for more unusual system designs as well as for mainstream transactional services, and we’re always up for a discussion about exceptions and edge cases. Usernames and passwords If the service is a private alpha or private beta release then it should be protected by a username and password known only to the development team and the users who are testing the service. If a service, or part of a service, is a public alpha or beta releases then it should be clearly marked as such with a text label on every page (ie don’t use an image containing the word alpha or beta) and in every API response. Multiple environments It is good practice to have multiple ‘environments’ for the development, testing and live (aka production) versions of any service. The development and testing environments allow the team to assess the correctness and quality of the service before it goes live. Typically, the subdomains used to access a development or testing instance of the service are structured in the same way as the subdomains used in the live version of the service. Therefore, you MAY create other subdomains of servicename.service.gov.uk for use in testing and development, such as www-preview. and www-dev, or www.preview. and www.dev.. If there’s a compelling reason to use a non .gov.uk domain for testing and/or development subdomains, that’s also acceptable. Regardless of the domain name used, web-based services on testing and development domains (including APIs) should be protected by a username and password along the same lines as private alpha and beta releases. Many services will collect personal information from users. It’s very important that this information can’t be intercepted by malicious third parties as it travels over the internet. Therefore, all services accessed through service.gov.uk domains (including APIs) MUST only be accessible through secure connections. For web-based services this means HTTPS only (often referred to by the acronyms TLS or SSL, which both refer to the protocol underpinning these secure connections). Services must not accept HTTP connections under any circumstances. Once a service manager has verified that their HTTPS setup is working fine they SHOULD enable HSTS on the production domains (www., admin. and assets.), by setting an HTTP response header such as representing a commitment to HTTPS-only traffic for 14 days. Once the service manager is confident that HSTS is configured correctly, they SHOULD increase the commitment to months or years: Cookies used on www.servicename.service.gov.uk and admin.servicename.service.gov.uk MUST be scoped to the originating domain only. Cookies MUST NOT be scoped to the domain servicename.service.gov.uk. Cookies SHOULD NOT be used on assets.servicename.service.gov.uk (they introduce a browser overhead that slows down the response time for users without providing any benefit for the service manager). Cookies MUST be sent with the Secure attribute and SHOULD, where appropriate, be sent with the HttpOnly attribute. These flags provide additional assurances about how cookies will be handled by browsers. GOV.UK is the place for users to find all government services, so it’s important to ensure that users always start on the relevant GOV.UK page, rather than a different or duplicate start page on www.servicename.service.gov.uk. As a result, services need to ask search engines not to index pages on their domains, so that the relevant GOV.UK page and the service domain don’t compete with each other in search engine results. This can be achieved by redirecting users to the relevant GOV.UK start page if they go directly to the service’s domain name, and by asking search engines not to index pages on the service’s domain name. Therefore, every service hosted on a service.gov.uk domain MUST: have a robots.txt file on the www, admin and assets subdomains asking search engines not to index any part of the site. Example content for robots.txt is given below, and more details can be found on The Web Robots Pages: have an HTTP 301 redirection from the top-level index page of the www and assets subdomains to the relevant start page on GOV.UK. (Note: this means that the service start page on GOV.UK SHOULD NOT link to the root of the www domain.) If you have contracted with CDN-based DDOS-protection suppliers then you should register these additional subdomains for use by your suppliers: Your suppliers will use these subdomains to address your www, admin and assets services. Detailed configuration advice for origin servers is outside of the scope of this document, but it’s important to ensure that these ‘origin domains’ only listen for traffic from trusted sources like: At present we advise against allowing DDOS protection suppliers to terminate SSL connections for transactional services carrying personal information, but this behaviour isn’t prohibited at present. Although SSL termination on the third party network would allow the supplier(s) to carry out additional analysis and potentially extra mitigations against certain types of attack, it would also give the supplier access to all the personal information being submitted to your service.  There are obvious downsides to allowing this level of access, especially if the supplier’s network and processes have not been accredited to the same level as the rest of the service. It’s a risk-based decision, but if in doubt we suggest a presumption against SSL termination on third party networks. Many suppliers offer IP forwarding DDOS protection, which does not have the same security issues as SSL termination, and is recommended in preference to SSL termination.  If your service requires transaction monitoring (which is not at all the same thing as DDOS protection) you should contact your CESG account manager for advice.  This is interim guidance and will be updated – check back in early May 2013 for an update. Emails to users of your service SHOULD be sent from a human-monitored email address that originates from the domain servicename.service.gov.uk (and not the dept/agency or any other domain name). You SHOULD enable SPF on the sending domain. You MAY also want to use DKIM on the sending domain; it can provide additional guarantees about message delivery and help recipients to more easily distinguish genuine mail from forgery. If your service should need to wind down for any reason, you MUST ensure continued useful service and information for users by: For services that have been live for less than 6 months, you MUST continue to do the above for the remainder of a year total. For services that have been live longer than that you MUST continue to do the above for a further 12 months or until the expiry of the current SSL certificate, whichever comes first. The GOV.UK start page will be amended to explain that the service is no longer running, and cease to provide a start button pointing at the defunct service.","description":"Government offers a number of different digital services to citizens. While the start and end of a user’s journey will be on GOV.UK, the service itself will typically be hosted elsewhere, and will need a different domain name as a result. This page describes the use of service.gov.uk subdomains for hosting digital services.","link":"/service-manual/operations/operating-servicegovuk-subdomains.html"},{"title":"Vulnerability and penetration testing","indexable_content":"Involve the right people Not just software Automate where possible Why GDS do this Further reading Talk to CESG Use in-house expertise as well as external services physical security (where is the equipment housed and how secure is it?) the interaction between an online system and a call centre static analysis input fuzz testing OWASP Top 10 Making sure your web-based systems and applications are secure requires more than just good design and development. Sometimes referred to as pen testing, vulnerability and penetration testing is the act of analysing and testing a service for security problems. This is often a specialist activity done via a third party. View security testing as an ongoing activity in your project, and not as a final check. Security is important to a product and its audience. It’s essential that non-technical audiences understand vulnerability testing reports and the risks they identify, as well developers. Security isn’t just about whether a product secure or not – it’s also about reducing the risks of a wide range of issues. Get an independent body to help find potential security problems in your system/application before it’s released to the public, as this helps to identify its vulnerabilities. CESG is the National Technical Authority for Information Assurance. Based at Cheltenham, they provide both standards and advice for information security. If your project is fairly large, contact CESG as early as possible. They can provide guidance and expertise on potential problems and help you to make sure the right things are tested. Test for security issues throughout the development of your service, as well as regular testing when your service is live.  Having third parties do this testing is a good way of introducing genuine experts and getting a different view on something. However it’s also important to make sure that your team building the software know that security is their responsibility, and not something that is outsourced. When testing for vulnerability, look at the whole system and not just the software involved, such as: An obvious example would be physical security (where is the equipment housed and how secure is it?) but a more interesting example is often the interplay between an online system and a call centre. From using information available from a call centre it’s possible to exploit the software system in some way. For instance, getting a call-centre team to change an email address on record for someone else, and then using a forgotten password facility, which relies on the email address being trusted, to access the account. It can and does happen. Some exploratory manual testing is always a good idea when looking for vulnerabilities, but as this is time-consuming and expensive, also have some level of automation. This may take the form of tests written or tools used specifically to test the security of a feature, eg: The web is a hostile environment, and the nature of government services means they can be targets for a wide range of different threats – from financially motivated criminals and online activists up to nation states. Even where personal or sensitive information is not at risk, the smallest issues can damage the reputation of government. Exploits of web application tend to follow a relatively small number of common patterns. Automated and manual testing, as well as awareness of these common problems, can have a drastic effect on the security of your system. The Government Accreditation processes mandate some form of vulnerability testing, often working with CHECK approved suppliers. View this as the minimum amount of effort required.","description":"Making sure your web-based systems and applications are secure requires more than just good design and development.","link":"/service-manual/operations/penetration-testing.html"},{"title":"Uptime and availability","indexable_content":"Introduction Not just on or off People Things to watch out for Weekends and evenings Trade-offs Underlying infrastructure availability Scheduled maintenance Penalties Contracts The expectation when using an online service is that it’s available 24 hours a day 365 days a year. This is one of the advantages of shifting services online, people can transact with you when it suits them, not just when your office or call centre is open. But achieving good uptime for a service takes planning and good design. It is important to build services that wherever possible can be turned off (or fail) gracefully. The possible states for a service should not be limited to on or off. Individual components could be designed to fall back to minimal functions. A read-only mode could be introduced where information can be looked at but not changed. If your service is reliant on a third-party service, you could queue information for later processing if that service were to become unavailable. Remember to build in redundancy and avoid single points of failure. You can avoid the problem of a web server failing by having more than one and load-balancing between them, or minimise disruption if a database crashes by using database systems which spread data and queries across a cluster. A critical part of maintaining a service is the people running it day-to-day. People may be involved in responding to customer issues or in more technical activities like resolving minor outages before they cause a real problem. Don’t forget that people will be using your service when you’re not at work. Failures during out-of-hours periods can go unnoticed for long periods of time if no-one is responsible for them. Make sure you consider evenings and weekends and also whether an on-call system is appropriate, or whether you need a dedicated 24/7 support capability. Ultimately you may make trade-offs, eg sacrificing certainty around uptime for lower cost. Try to do this openly and communicate decisions made. Many problems regarding uptime come from mismanaged expectations. Remember the availability of the service is dependent on the availability of lots of systems, potentially across multiple suppliers not all of whom you have a direct relationship with. Maybe your application (maintained by your development team) relies on an application server or database (provided by another team in your department) which runs on an Infrastructure as a Service platform (provided via contract with a commercial supplier) which relies on network connectivity and power from utilities (which you have no direct contract with). This can become quite complex in a service-oriented architecture where you rely on other software applications and interfaces. Understand your dependencies and understand their dependencies and all the intended uptime expectations. Sometimes uptime can be quoted to ignore pre-arranged maintenance periods. So for example a service may claim 100% even though it shuts down every Monday evening for maintenance. Hiding uptime problems behind lots of maintenance periods should be avoided. However, it may be fine to quote both planned and unplanned downtime where services genuinely do need scheduled maintenance. Some suppliers will offer recompense for missing uptime guarantees or service level agreement response times. Although this can obviously be useful consider whether the money or service credits offered really offset the difficulties faced by people using the service. If you’re regularly getting credits for uptime problems consider whether you are really getting the offered uptime or SLA from your supplier. The impact that contracts with suppliers (of products or services) can have on availability should not be underestimated. Comprehensive guidance on contracts is out of the scope of this guide but ensure contract terms, service level agreements and uptime guarantees and understood by the team responsible for the service. And that at the very least the service manager is involved with contracts for underlying systems.","description":"The expectation when using an online service is that it’s available 24\nhours a day 365 days a year. This is one of the advantages of shifting\nservices online, people can transact with you when it suits them, not\njust when your office or call centre is open. But achieving good uptime\nfor a service takes planning and good design.","link":"/service-manual/operations/uptime-and-availability.html"},{"title":"Alpha phase","indexable_content":"The objective of an alpha What should be in your alpha Alpha phase duration Team requirements Outputs An ideal alpha gain greater understanding of a service test design approach test some technologies begin to form the team gain shared understanding of the service at a coding and integrations level understand what or who you will need to deliver a beta is the solution appropriate? is your approach viable do you have enough understanding of your users’ needs to meet them? high level story cards plan for beta and running of the live service (decreasingly detailed) working basic system that provides limited functionality that can be shown to a number of users understanding around legacy systems to replace or wrap or integrate with cross-functional requirements decision to progress to beta phase final analysis on the research you have commissioned on user needs options for the assisted digital support for your service Next phase: beta Previous phase: discovery When designing a service it’s impossible to predict everything upfront. Each project features many challenges, and in your alpha you will start exploring solutions for these. You may need to bring more developers and designers into the team, who will help you to build and test prototypes and possible solutions for your users needs. By the end of the alpha you should have a clear idea of what’s required to build a beta. The whole phase should not last longer than about 6 to 8 weeks. The objective is to build a working prototype. This will be used by stakeholders or a closed group of end users to: Continue to build upon and analyse the research you have commissioned on user needs and use this to set up an open, engagement process with your stakeholders. Involve a wide range of stakeholders from the private, voluntary and other parts of the public sector. Run a series of workshops with these stakeholders to develop your options. Following demonstration of your alpha, you may choose to discard the code and start fresh in the beta. If, however, your code is effective you may continue to iterate against your prototype. Develop options for the assisted digital support for your service. To help develop the options, continue to build upon and analyse the research you have commissioned on user needs. Bring in the expertise of organisations working with people who are offline and users themselves. Run workshops to develop your options. The alpha doesn’t need to be a complete, end-to-end transaction. You’re looking to demonstrate just enough so users gain some understanding of the service. Think of it as a proof of concept: If not, find out more and make a new prototype. The alpha phase is another relatively short phase. At GDS, we try to limit these to about 2 months, running in week-long sprints over a 6 to 8 week period. This phase involves a relatively small core team, who will be capable of rapidly iterating solutions. It will probably expand and contract in size as different specialisms are required. This core team will be a mix of stakeholders, and makers (designers and developers) particularly those familiar with user research. It will be led by the service manager. The outputs for the alpha phase are: For a worked example, we have written up some information on an ideal alpha.","description":"When designing a service it’s impossible to predict everything upfront. Each project features many challenges, and in your alpha you will start exploring solutions for these.","link":"/service-manual/phases/alpha.html"},{"title":"Beta phase","indexable_content":"The objective of a beta Duration of the beta phase Team requirements Outputs delivered a public, end-to-end prototype of the service a collection of prioritised work to be done (your backlog) a user testing plan accurate metrics and measurements to monitor your KPIs tested the assisted digital support for your service a working system that can be used, for real, by end users Next phase: live Previous phase: alpha You’ve tested your solutions to user needs and built up a clear picture of what it will take to build and operate your service. Now you will build an end-to-end prototype, test it in public and prepare to run it. The objective of this phase is to build a fully working prototype which you test publicly with users. You’ll continuously improve on the prototype until it’s ready to go live, replacing or integrating with any existing services. This is achieved by providing the user stories in the backlog created in the alpha phase. This is the time to resolve any outstanding technical or process-related challenges, get the service accredited and plan to go live. You’ll also be resolving technical and process challenges, meeting for the first time many of the technical criteria outlined in the service standard. You should be rapidly releasing updates and improvements into the development environment, and measuring the impact of your changes to the key performance indicators (KPIs) established in your discovery and alpha phases. You’ll also test the assisted digital support for the digital service. You might test one or more of the options you developed in the alpha phase. The exact duration of your beta will depend on the scope of your project, but an appropriately sized team shouldn’t take more than a few months to create a public beta. Following the release of your beta you’ll spend some time iterating on the service until it is ready to go live. You’ll now know what size team you need to create the service, scoping it in response to the findings of your alpha prototype(s). It will be run by a single, suitably skilled service manager, and will include designers, developers, web operations specialists and performance analysts as appropriate. At the end of the beta phase, you’ll have:","description":"You’ve tested your solutions to user needs and built up a clear picture of what it will take to build and operate your service. Now you will build an end-to-end prototype, test it in public and prepare to run it.","link":"/service-manual/phases/beta.html"},{"title":"Discovery phase","indexable_content":"Timescales Objectives The team Outputs workshops simple mock ups paper prototypes plenty of whiteboard diagrams a prioritised list of user needs a prioritised list of story cards to feed into project teams understanding of team and capability required to complete the project ability to scope and plan an alpha a decision to progress to next phase maybe some rough prototypes maybe some user personas a list of stakeholders and input from them about existing services understanding of existing services, including those run by non-government sources understanding of how many of your users will need assisted digital support, and what their user needs are Next phase: alpha What are the needs of your users? What services currently meet those? How are they performing? What technological or policy related constraints might there be? Before you start building a service you need to build up a picture of what the context for that service is. That means lots of user research, close analysis of policies, laws and business needs, and workshops and interviews which establish the criteria for success of your service. The discovery phase will give you a high-level understanding of user needs, what the existing service landscape looks like and a sense of what your initial prototypes will explore. You’ll need to think about user needs for digital and assisted digital users. As a start, you’ll need to understand what proportion of your users you think will need assisted digital support. The high-level business context will become clear, and you’ll begin setting targets for your KPIs. You’ll also get a better understanding of the legacy interfaces and infrastructure you must deal with, and what existing process are in place for replacing or decommissioning these. This information is found through: A small team will be required, consisting of your stakeholders and any core team members that have been identified, including the service manager. The phase should not take longer than 4 to 8 weeks. During the final week you should be setting up the broad scope of a project and an initial set of user stories (also known as a backlog) to work to. This is known as an ‘inception’. At the end of the phase a decision should be made whether to proceed to the alpha phase. You will leave the discovery phase with:","description":"What are the needs of your users? What services currently meet those? How are they performing? What technological or policy related constraints might there be?","link":"/service-manual/phases/discovery.html"},{"title":"Ideal alphas","indexable_content":"Preparing Executing Identifying goals Identifying risks Identifying the team Identifying process Inception Iterations Ending the alpha explore the major risks to the project discover whether the project is workable get some ideas about the cost of the project identifying risks prototyping solutions attempting to prove that you can replace an existing digital service prove that you can replace a paper service with a digital service create a new digital service design business process technical risks how easy is it to use will the user get through it correctly the first time what do errors look like how will I analyse the user research how do I build prototypes quickly how the department will integrate with the new process do they have sufficient fraud checks are they set up to handle support calls how to operate the service what sort of ongoing connections will be necessary are there special hosting arrangements, eg high security hosting design (service, UX, content as needed) user research (user testing) prototype development service integration development delivery management business analysis build a wireframe or prototype user journey in 1 week host it on the internet conduct some user research analyse the findings an inception a series of iterations of design, development and test either:     alpha termination       alpha to Beta transition      alpha termination alpha to Beta transition a shared understanding of the service User Personas (who will use the system) a clarified current business process (where applicable) a clear set of goals and tradeoffs some vision exercises a clear understanding of the existing technical estate a prioritised backlog of work for first iteration the entire team get to know one another beginning to form a team identity beginning to form a team understanding of the business itself Day 1     Retrospective (1 hour)       Iteration Planning (2 hours)       Start iteration      Retrospective (1 hour) Iteration Planning (2 hours) Start iteration Day 2     User research on previous iterations prototypes       Analysis of test results      User research on previous iterations prototypes Analysis of test results Day 5     Demo      Demo user research in iteration 1 stories completed in iteration 2 further research on updated prototypes in iteration 3 the first and last day of the iteration are the most critical for the entire team Mondays/Fridays have the most absence Next phase: beta Previous phase: discovery An alpha is a process that happens at the beginning of a project. It allows us to: It is primarily defined by two activities: This document covers how to go about executing an alpha project. Following this guidance will help you create a successful alpha project. You should bear in mind that these guidelines are flexible and are intended to direct you towards achieving your project goals simply and quickly. Before planning an alpha, the team must work out what the goals of the alpha are. For example, some of your goals might be: The department should communicate their goals in the request for proposals. The team will probably develop their own additional goals as well. The reason for running an alpha is to identify risks to the wider project early. This means it should identify a project’s biggest risks and explore those risks to understand them better. In most government projects, the risks fall into 3 main categories: The design risks cover the issues that come with running a user centered design process within an iterative model. We find that it often takes several attempts to get the service proposition correct for the citizen. You’ll need to think about things like: The business process risks include: The technical risks tend to be about integrating into the existing systems. For example: In our experience, there are always technical risks to all projects. Most government departments do have a history of solving technical risks and have entire teams dedicated to them. Business process and design risks are far less well understood, and so we encourage alphas to focus on exploring those risks with prototyping over full service integrations. The well-rounded team will need to have various skills, including: The team will probably be bigger than this, and the service may supply its own team members who will normally be learning during this process, and thus probably won’t be contributing significantly. The team will work together in an agile manner, doing iterations of either 1 or 2 weeks. For a short project we’d normally recommend 1 week iterations where possible. The team must have the core skills to: The Business Analyst and Delivery Manager roles are there to ensure that the findings can be fed back into the team and the wider organisation. Each alpha will be different, because the risks and goals are different. But we find that there are common aspects in how good alphas are run. We want to maximise the amount of learning that is happening in the alpha. So we tend to expect alphas to be short, about 8 weeks in length. We expect alphas to normally consist of: We believe that alphas should start with an inception process. An inception is designed to bring the team together and share the knowledge about the risks and goals identified. We tend to recommend a short inception – between 4 to 7 days seems to work well. The inception should be run by the supplier team, probably by the Delivery Manager. It will look at a variety of business, technical and user aspects of the project. Each supplier has a different format for these, but we see fairly common outputs including: The facilitator will of course give up time to various specialists to run individual sessions. The aims include: The inception may well cover introductions to agile itself, if the project team need it, and should also have a demonstration and retrospective at the end of it. Alphas are often about identifying competing hypotheses about the user journey or business process. We expect to produce many prototypes to test the hypothesis, and run tests in a short amount of time so we can learn, change and retest. These feedback loops are crucial. At this early a stage of a project, it is rare to be able to choose a single flow or design. Because of this, we encourage the building and testing of many prototypes. This allows you to quickly get feedback on the best experience for the users and to test competing ideas. We expect that a first draft of a user flow will have issues. You should seek to revisit the design, research and test process on each flow during many iterations. The iterative process we would recommend should be a straight up agile process combined with UX and user research. We would encourage the following format: This structure might move around a bit, but you will want to do the user research early in the iteration to give plenty of time to feed into the next iteration. This enables fast turnaround on the stories. Working at this pace will result in: This assumes fairly high definition prototypes. These may have backend systems that mock out service integrations, but which contain actual working logic. If you need to iterate faster (as is likely at first), you can start with much lower fidelity prototypes (like paper). This can let you test various flows and allow much faster iteration of research and prototype development. We would expect that you will do a demonstration of the service journey so far. This happens at the end of each iteration, and highlights what the team has learnt and what the team is planning on doing next. We’ve often found that aligning the iterations to Monday to Friday is not a best practice. It can cause logistical issues: Starting the iteration on a Wednesday, with User testing on Thursday and demo the following Tuesday has worked well in the past. The value of the alpha process is that the team can use it to identify early the biggest risks to the Beta part of the program. It is important to remember that the most valuable goals from the alpha process are the increased domain understanding and the team forming behaviours. If the code produced during the alpha is not high-enough quality, then it’s fine to throw it away and rebuild. For example, the team may elect not to use test-driven development during this stage. Those activities represent investment in quality. Remember that a primary goal during the alpha is learning. We might not be ready to make the investment in creating production-ready systems. In particular, that investment might be premature if the team learns they are not solving the right problem. Another output of the alpha can be the early decision not to continue into Beta. This represents a successful alpha since it reduces wasted time and money. It may well be the case that at the end of the alpha you go back to the discovery outputs and start a new alpha with a different focus or go back and perform another discovery. However, assuming a successful alpha project, then early in the alpha process, the Delivery Manager and the Service Manager will start to work on the business proposal for the Beta program. The reason for starting so early on the Beta program proposal is that often the team would like to continue without a break into the beta program to maintain the momentum and so need to allow for the time to gain approval for further spending The last demo of the alpha should have the attendance of the project board or senior management team from the service. At that demo, you are able to show what the alpha has achieved and show the feasibility of the beta program. Assuming that the status reports to project board and the show and tell have got approval, then as the project draws to an end, the team will want to have a final retrospective, and produce a backlog of epic stories for the Beta program.","description":"An alpha is a process that happens at the beginning of a project. It allows us to:","link":"/service-manual/phases/ideal-alphas.html"},{"title":"Live phase","indexable_content":"Going live Post-launch stages Team requirements monitor system performance optimise the code ensure the service remains secure Final phase: retirement Previous phase: beta You’ve been building a service to meet users needs, and after your public beta you have a tested solution that is ready to release. To provide a fully resilient service to all end users the service should now meet all security and performance standards. You have configured your analytics to accurately monitor the key performance indicators identified in the building of your service, and you have planned the transition or integration of any existing services. You have liaised with the team governing the Digital by Default Service Standard to make sure that you have met the requirements of new and redesigned services. And, most importantly, you have met the user needs identified in the discovery, alpha and beta phases. This is not the end of the process. The service should now be improved continuously, based on user feedback, analytics and further research. Operational support – both technical and customer-focused – is in place, and you have implemented pro-active monitoring methods. These will help you to: You’ll repeat the whole process (discovery, alpha, beta and live) for smaller pieces of work as the service continues running. Find something that needs improvement, research solutions, iterate, release. That should be a constant rhythm for the operating team, and done rapidly. You’ll have identified the roles required to run your service, including the service manager and user support teams, while building the service. As different areas of your service are iterated and improved the team size will expand and contract, accommodating specialists as appropriate.","description":"You’ve been building a service to meet users needs, and after your public beta you have a tested solution that is ready to release.","link":"/service-manual/phases/live.html"},{"title":"Retirement phase","indexable_content":"User needs Next stages Data Let your users know Plan to redirect traffic Make sure your subdomain continues to work Previous phase: live    When we retired the DirectGov, BusinessLink and departmental websites, we invested a lot of effort in mapping content on those sites to GOV.UK content. We redirected as many of the old sites’ URLs as we could to corresponding GOV.UK URLs, and provided ‘Gone’ pages with links to the National Archive where the content was not being transferred. We wrote about that in “No Link Left Behind”.  Even the best services may eventually reach retirement. Changes in policy may mean that the service is no longer offered or new understanding may mean that those user needs are better provided through a different service. Whatever the reason, the retirement of digital services should be handled with the same degree of care as their creation, concentrating on user needs. Your service will have been built to serve user needs. It’s vital to understand how those needs are to be addressed once your service has been retired, whether they are deemed to no longer exist, are no longer served by government, or will in future be served as part of another service. If the needs will no longer be met by government but will now be met by the private or voluntary sectors then it’s important to for these organisations to be adequately prepared. This means that the appropriate online user journey can be developed. Similarly, if the needs are to be served as part of another government service or services then you should identify which services they are and work with those services’ teams. This means the new teams can learn from your experience and you can understand how you’ll support your users to make the transition. The vast majority of users of your service will begin and end their journeys via GOV.UK. As soon as you know that your service is likely to be retired you should contact the GOV.UK team to make sure that those journeys are amended and appropriate information is supplied. The GOV.UK team will need to know why the service is being retired, and how those user needs are to be served in future so that they can provide the appropriate information, advice and links to users. For that majority of users who begin and end their journeys on GOV.UK the most important thing is to ensure that GOV.UK is updated. There will be some users, however, who access your service directly whether via links in emails, bookmarks on their computer or remembering your URLs. It’s important to prepare them for the change and lay out clearly what it will mean to them. Your planning should aim to produce the minimum possible disruption for users, but it will still be a significant change for them. Details of what the change is, why it’s being made, what they will need to do, and what will happen to their data should be made easily available. Users who access your service via an application programming interface (API) will need time to update their software to use the replacement service’s APIs or to make other relevant adjustments. You should reach out to your API users as early as possible and remember that they may have significant lead times for making and distributing changes. Changes to the service online will also need to be seamlessly tied in with messaging to offline users who are receiving the service through assisted digital channels. Once GOV.UK is updated the vast majority of users will begin to be directed to the new service. Some users will still try to access the service at its current (now retired) home. You should have a plan for redirecting those requests to the appropriate new service, or to provide clear information about the service that has been retired in perpetuity. When we retired the DirectGov, BusinessLink and departmental websites, we invested a lot of effort in mapping content on those sites to GOV.UK content. We redirected as many of the old sites’ URLs as we could to corresponding GOV.UK URLs, and provided ‘Gone’ pages with links to the National Archive where the content was not being transferred. We wrote about that in “No Link Left Behind”. If you operate a service.gov.uk subdomain, please read your technical responsibilities for ending your service. In running your service you will have accrued a large amount of data about the service and its users. You should already have policies in place to manage that data responsibly, including details of how long it will be retained. Those policies will continue to apply, and you will need to ensure that there is support in place to maintain them. Where data is being transferred to a new service owner, that should be done in accordance with your existing data protection policies, and communicated clearly to your users.","description":"Even the best services may eventually reach retirement. Changes in policy may mean that the service is no longer offered or new understanding may mean that those user needs are better provided through a different service.","link":"/service-manual/phases/retirement.html"},{"title":"Technology architecture","indexable_content":"Our reference model Products and services Platforms The legacy estate the high-level business vision and requirements that guide the design of processes and systems around the needs of users (recognise that business requirements will change over time, so design solutions that minimise the impact of such changes) an information model covering both public and private data, as well as wider information management requirements like business intelligence technology, the modelling of core capabilities, their structure and relationships implementation, including the development and application of guidelines, patterns, blueprints, models and communities that ensure consistent implementation and compliance across multiple programmes and projects throughout the project lifecycle you will need to make sure stakeholders stay aligned on what is needed (you’ll need to resolve issues when stakeholder requirements conflict with business growth (like reduced cost), system design (like network latency and operational issues (like security, privacy and auditing)) avoid duplication of data ownership – to prevent synchronisation, data quality, security, and privacy problems limit duplication of development effort – to prevent the continual reinvention of the wheel across multiple projects dealing with common data, users and processes wherever possible (and tested against the “rule of three”), we will use and reuse standard, interoperable components, interfaces, data formats and processes using open standards new user services must take the opportunity to reduce dependency on legacy systems, and new programmes should where possible avoid nugatory expenditure on integrating with such systems the volume of such systems, their cost of support and business criticality should continually decrease, ie they exist to fulfil a current specific need, but should not be the focus of further investment departments should baseline and plan to migrate/mitigate existing systems (triage the ‘old and broke’, ‘old and works’ and ‘defunct’), ensuring value is released:     badly engineered systems should be contained       the working legacy should be modernised to reduce costs and risks and wrapped where appropriate to provide open interfaces and interoperability with platforms       legacy applications should be replaced / rearchitected periodically to prevent situations where the application can no longer be supported because it’s grown too complex for anyone to understand or too expensive relative to the benefits delivered       open standards must be used for interoperability between systems and departments, with adaptors used to integrate existing host systems, programs, messages and data (from mainframes to line of business services, where those systems do not natively support open standards) – you should avoid point-to-point bespoke integration between systems to reduce costs and inter-dependencies      badly engineered systems should be contained the working legacy should be modernised to reduce costs and risks and wrapped where appropriate to provide open interfaces and interoperability with platforms legacy applications should be replaced / rearchitected periodically to prevent situations where the application can no longer be supported because it’s grown too complex for anyone to understand or too expensive relative to the benefits delivered open standards must be used for interoperability between systems and departments, with adaptors used to integrate existing host systems, programs, messages and data (from mainframes to line of business services, where those systems do not natively support open standards) – you should avoid point-to-point bespoke integration between systems to reduce costs and inter-dependencies Architecture is not just about the design and deployment of technology – to be successful you need to understand how several elements work together. These elements include: The term “architecture” includes both the logical design as well as its physical implementation. Logical architecture based on user-centric service design and clear user needs should always precede physical architecture. At a minimum you need to understand what capabilities are required before starting to narrow down on product choices. The technology code of practice provides guidance on the consistent use of technology across government. Our reference model draws on modern platform-based architectures. We should use the highly scalable open platform model of internet-scale organisations and draw upon the practical experience of other governments like Estonia.  Note: examples are for illustration purposes only. To support this our architecture will need to move away from tightly coupled and proprietary models towards an open, interoperable architecture using open standards and open interfaces, eg XML- and JSON-based data services, to maximise government’s flexibility and improve the deliver speed of services. We should strive to: Duplication is OK when used to optimise the user experience (but there should be clear master-slave data relationships if information itself is replicated, eg for reasons of performance). Products and services are the things that users engage with to find information or complete transactions. As such, they should be designed around the needs of the end user rather than the service provider. They can be either Mission IT (internal facing) or Digital Public Services (external facing). Platforms provide a set of open interfaces, protocols, data formats and tools that enable software developers to rapidly provide products and services. Examples of platforms are the GOV.UK publishing platform, the Performance Platform, and the identity assurance platform – they are not things like ‘Windows Server’ or ‘Linux Server’. Platforms should be kept as simple as possible, but designed to have no barriers to scaling up and scaling out. New features should be added only when required by new product needs – platforms emerge from the demands of products rather than being driven by top-down architectural process. Learn more about government platforms. “Legacy” is used as shorthand to reference existing mission IT systems and components that have been designed to meet historic requirements. Often they are built using technologies that have become difficult and expensive to maintain or change. Examples of legacy applications are likely to span benefits systems, case management systems, and customer relationship management systems. Over time the dependence on legacy platforms should be reduced as successful new platforms are developed. To reduce dependency on legacy platforms:","description":"Architecture is not just about the design and deployment of technology – to be successful you need to understand how several elements work together.","link":"/service-manual/technology/architecture.html"},{"title":"Technology code of practice","indexable_content":"The technology code of practice £5 million – full lifetime cost of the project £1 million – current or new back office / administrative systems, including Enterprise Resource Planning (ERP) systems, HR systems, Finance / Accounting systems, procurement systems. The control includes implementation, licensing and support costs, as well as any extension to those services £100 thousand – Common Infrastructure Solutions for the Public Sector Network (PSN) £0 (no lower limit) – spend on digital projects including identity assurance, digital services, and the commissioning of all government digital activity Ensure systems, information and processes are designed around the needs of the service user, providing as simple and as integrated an experience as possible. Be very clear who the users are and how to engage with them and ensure their needs are met. Demonstrate value for money in your business case and articulate the options considered in a full and objective appraisal. All new or redesigned digital services meet the Digital by Default Service Standard. Ensure a level-playing field for open source software. Demonstrate an active and fair consideration of using open source software – taking account of the total lifetime cost of ownership of the solution, including exit and transition costs. Use open standards, and common government platforms (eg GOV.UK, identity assurance, shared services) where available. Make data open by default, while minimising and securing personal data, or data restricted for national security reasons. Public data should be made available by default in both human and open machine readable formats. Users should have access to, and control over, their own personal data. Establish the sensitivity of information held in accordance with the Security Classification Policy, establish legal responsibilities, develop user friendly, proportionate and justifiable security controls according to the Security Policy Framework. Separate commodity from niche needs. Use cost-effective commodity services for infrastructure and utility business activities like office productivity (word processing, spreadsheets and presentation software, email, scheduling and collaboration). Identify and acquire capabilities rather than infrastructure where services required are bespoke/innovative. Ensure that any procurement is designed to encourage competition and follows published Government Procurement Policy. [Competition will be encouraged with no like-for-like extensions to existing contracts. The renewal of incumbent products, services and suppliers, or the stipulation of their brands, is not permitted: there must always be an open and competitive specification and reprocurement process. All requirements must be expressed in terms of business outcomes and user needs – not solutions or product feature sets. Programmes will be ‘disaggregated’ for commercial purposes – broken down into components supported by the market to enable many suppliers to bid. There is a presumption that no procurement will have a lifetime cost over £100 million. Contract lengths for services should be kept to the minimum level necessary to ensure commercial flexibility.] Purchase networking and telephony services through the Public Services Network frameworks. Objectively evaluate potential public cloud solutions first – before you consider any other option. In order to do this you will need to identify the capabilities and services that make up your technology design, and demonstrate that the solution chosen represents best value for money. Follow the guidelines laid out in the EUD strategy, design and implementation guidance to ensure your solution will work for any end user device. Ensure best environmental practices, whether in-house or via external suppliers, including compliance with Greening Government ICT. Any software licence agreements must evidence actual user needs – there should be no default continuation of enterprise licence deals or specification of products or brands. Share resources: services, information, data and software components must be shared in order to encourage reuse, avoid duplication and prevent redundant investments. Reuse includes the use of existing services and capabilities that already exist outside of government where they provide best value for money, eg identity verification, fraud and debt management, cloud-based commodity services. Align to the shared services strategy for HR, procurement, finance and payroll. Plan on using an agile process, starting with the user need. Waterfall should only be used by exception and where it can be shown to better meet user need. Projects may need the best of both formal and agile methods, playing to their respective strengths: producing successful IT services is about knowing when to use the right tool at the right time. Demonstrate that adequate capability is available in your organisation – you shouldn’t outsource strategic decision-making or service accountability. If the necessary capability does not exist in-house, then you need to evidence a plan for developing or recruiting people with the right skills and experience. GDS will examine and challenge all technology-related spending over a certain threshold – no public commitment or expenditure should be made above these thresholds without prior approval. The thresholds are: For your project to proceed, you must demonstrate that you have met all applicable elements of this code – this will be verified through the controls process:","description":"GDS will examine and challenge all technology-related spending over a certain threshold – no public commitment or expenditure should be made above these thresholds without prior approval. The thresholds are:","link":"/service-manual/technology/code-of-practice.html"},{"title":"Creating a culture that supports change","indexable_content":"What good looks like What you might need to change Breaking down barriers to technology change Understanding fear, uncertainty and doubt Why you need an environment that supports change Example: hosting changes for GOV.UK Further reading a team that is knowledgeable about existing and emerging technology options a governance and management structure that is an enabler, not a bottleneck or barrier to change flexible commercial models that meet your needs, eg commodity or utility provision a diverse, competitive market from which to choose your suppliers or products transparency and trust in your supplier relationships pricing competition and the ability to switch between alternative suppliers Your digital services and the technology on which they’re built should enable change, giving you the freedom to evolve your services according to changing user needs, expectations and technology innovation. Improving the way that you produce digital services is more about cultural and behavioural change than it is about IT. Technology alone will not be able to improve public services. Your digital services are based on user needs, not on organisation structures and processes. You have: To create better public services you’ll need to change existing behaviours and cultures, and successfully transition to new ways of working. You may need to develop skills within your team and keep up to date on new methods and technologies. It may also be necessary to change your physical working environment to get the best out of your team. You may need to radically change governance and management structures and will need to run projects in a more agile way. You’ll need to build relationships with new suppliers that you may have found through new procurement routes, such as the G-Cloud framework. It may be necessary and more cost-effective to write-off previous financial investments, so you’ll need courage and conviction to stop spending on old, legacy systems. The Cabinet Office, through spending controls, has a strong preference against extending contracts to support legacy systems. At the very least you’ll need an exit management strategy that sets out a plan for how you will significantly reduce any dependence on outdated technology. The cost of maintaining out-of-date technology that isn’t fit for purpose could far outweigh the cost of changing to a more suitable technology that better meets user needs. If you’re in this position, it’s also likely that the architecture and hosting of your existing estate may need to be rethought. You may come across inertia or resistance when you try to make necessary changes to your technology or how your service is provided. Your existing supplier base will tend to resist change – and the more successful a supplier has been working under the old model, and the more entrenched it is, the greater this resistance is likely to be. The changes needed will often be more successfully initiated and produced by those not encumbered by past success – new market entrants and those traditionally closed out from government work, like small or medium-sized businesses. You may find that you have become locked-in to a particular contract or technology. As part of your consideration of the total cost of ownership of a particular solution, you should have estimated the cost of exit at the start of implementation. If you try to move and find that cost is a barrier, any unlocking costs that you have identified must be associated with the incumbent supplier/system and not be considered as part of the cost of a new solution. Building on open standards or selecting products which use them will help you to avoid this problem in the future. If you have incumbent suppliers, it’s important that you understand and anticipate their likely reaction as they struggle to maintain the status quo and resist change. Incumbent suppliers will have data that reflects the past success of their business models and the way their business worked. They will be less certain about the future and may be concerned about the weakening of their market share and their exposure to genuine, open competition. The rewards and culture of these companies are likely to be built on their current business model. That will reinforce their internal resistance to change. It’s difficult for them to persuade shareholders and financial investors to replace a well-understood, if obsolete, business model with an approach that favours the treatment of some IT products as commodities. The IT as utility market is not yet fully established, so they will be uncertain about it even though this model might bring them higher profits on lower revenues. You’ll also see similar resistance in-house, where many staff may have long grown accustomed to old ways of working. It’s important that as a leader you provide the assistance and mentoring they need to help update their skills and experience. Some may have become dependent on external suppliers’ advice. Some may even have built their career on accreditation in a single supplier’s technologies. Flexible technology and services give service managers the ability evolve services and make them responsive to users’ needs. It also avoids costly and more risky big bang changes. Reducing the risk of lock-in to suppliers, software, service and support, or too old and inefficient IT, means that you have the ability to choose what best meets the outcomes you are looking for at a sustainable and competitive cost and at a timescale that suits you. GOV.UK is an example of how this works in practice. On the technology side, we set up hosting to build the beta of GOV.UK and then transferred it to a new supplier on the G-Cloud framework before launch. In early 2014, the infrastructure team at GDS migrated GOV.UK to a new hosting environment. Sam Sharpe wrote a blog post on the technical aspects of the migration and Carl Massa explained the project in a video. G-Cloud is the procurement framework intended for purchasing Infrastructure as a Service, Platform as a Service and Software as a Service products. Guidance about the team you’ll need to have in place to build a successful service. A blog post from GDS about making better choices for the technology we use.","description":"Your digital services and the technology on which they’re built should enable change, giving you the freedom to evolve your services according to changing user needs, expectations and technology innovation.","link":"/service-manual/technology/culture-that-supports-change.html"},{"title":"End user devices","indexable_content":"The mandate for change Start with user needs overly expensive end user devices and associated services an inability to change user IT to meet user needs a growing divergence from the modern user-friendly consumer IT being exploited in other sectors an entrenchment of incumbent products and suppliers through technical and commercial dependencies or “lock in”. The government agreed in the ICT Strategy to developing and implementing an end user device strategy. This strategy has aimed to address historic issues, including user tools that do not meet user needs, degrade civil service productivity, and provide poor user experiences.  These issues have been compounded by: The Civil Service Reform Plan requires departments to address “frustrating” IT tools for civil servants. A workplace transformation programme will streamline working practises, supported by effective IT. The Government Protective Marking Scheme Review and resulting Security Classification Policy, together with the Open Standards policy, require a reform of the principles by which IT tools for civil servants are designed, procured and managed. Civil servants are expected to take responsibility and apply reasonable judgement when dealing with information, removing the need for overbearing technical controls. The government and departmental digital strategies establish user needs as central to digital services and tools. This requires a step change in agility to meet changing user needs, together with an excellent user experience. We need to get 4 basic things right: The end user device strategy will help departments and suppliers understand what kinds of design decisions they should now be making. There’s specific guidance on the standards that need to be met to ensure that digital services don’t become heavily dependent on particular products or suppliers, and new security guidance has been developed by CESG (the government’s specialist technology security advisors) for those working with government information across all types of mobile devices.","description":"The government agreed in the ICT Strategy to developing and implementing an end user device strategy. This strategy has aimed to address historic issues, including user tools that do not meet user needs, degrade civil service productivity, and provide poor user experiences. ","link":"/service-manual/technology/end-user-devices.html"},{"title":"Government as a platform","indexable_content":"Government as a platform Moving to the platform model Working with organisations outside of government Tim O’Reilly’s “Government as a Platform” Agile Architecture The government is implementing a platform-based operating model. Google, Amazon, Twitter and Facebook, among many others, have all built their success on the back of platforms. They have developed a core technology infrastructure that others have then built upon, driving the success of the platform and meeting far more users’ needs than the original provider could have done on their own. One simple analogy for a platform is the Lego brick, which provides the basis for constructing many different products from the same components. The GOV.UK publishing platform provides the basis for many different products and services to be provided to citizens and businesses. Businesses like Amazon iterate and improve their services based on data, not intuition or guesswork. Their services are built on common technology platforms (search, recommendations, purchase etc). They deliver a scalable, consistently high-quality user experience using open source and open standards. Their world looks something like this:  While government can learn from this model, it cannot simplistically copy it. Government has many diverse and complex legacy systems and processes, almost all of them operating in silos. As a consequence, government currently looks something like this:  This move to a platform-based model is a significant transition. A platform provides essential technology infrastructure, including core applications that demonstrate the potential of the platform. Other organisations and developers can use the platform to innovate and build upon. The core platform provider enforces “rules of the road” (such as the open technical standards and processes to be used) to ensure consistency, and that applications based on the platform will work well together. Moving to a platform model does not imply a “top-down” conceptual identification of a list of platforms that government will buy or build. Platforms will emerge out of user needs and good architectural practices, but only as a consequence of the practical experience of building the features required. Platforms are defined by connectors and functionality – characterised by open interfaces, open data standards, and commonality. The figures below illustrate the journey required:   A move to platforms does not mean that government has to develop everything in-house: many of government’s needs can be met by existing cost-efficient utility services. However, government can help to establish best practice in areas such as personal data privacy. Wherever appropriate, the government should use existing external platforms, such as payments services (ranging from third party merchant acquirer services to the UK’s national payments infrastructure). Deciding to develop platforms in-house will happen only where that is the best way to meet users’ needs in the most flexible and cost-effective way. Government will draw upon the experience of other organisations that have already made this journey. Many have created platforms that initially sit across the top of existing silos to expedite agile and effective digital service delivery. This co-existence enables the benefits of the platform model to be realised quickly, even in a brownfield environment such as government, while the silos below the waterline are gradually reduced in importance and eventually made obsolete. This is the approach that government will follow, ensuring that it develops a well-defined schedule for switching off legacy environments as the platform model is progressively implemented. Further reading:","description":"The government is implementing a platform-based operating model. Google, Amazon, Twitter and Facebook, among many others, have all built their success on the back of platforms. They have developed a core technology infrastructure that others have then built upon, driving the success of the platform and meeting far more users’ needs than the original provider could have done on their own.","link":"/service-manual/technology/government-as-a-platform.html"},{"title":"Open data","indexable_content":"The data that you should make open Making your data open Building on open data Building trust Licensing your open data for reuse Why we do this Examples of open data in action Where to get help Further reading accessible (ideally via the internet) at no more than the cost of reproduction, without limitations based on user identity or intent in a digital, machine-readable format for interoperation with other data free of restriction on use or redistribution in its licensing conditions on which public services are run and assessed on which policy decisions are based is collected or generated in the course of your service delivery Public data policy and practice will be clearly driven by the public and businesses that want and use the data, including what data is released when and in what form. Public data will be published in reusable, machine-readable form. Public data will be released under the same open licence which enables free reuse, including commercial reuse. Public data will be available and easy to find through a single, easy-to use, online access point (data.gov.uk). Public data will be published using open standards, and following relevant recommendations of the World Wide Web Consortium (W3C). Public data from different departments about the same subject will be published in the same, standard formats and with the same definitions. Public data underlying the Government’s own websites will be published in reusable form. Public data will be timely and fine-grained. Release data quickly, and then work to make sure that it is available in open standard formats, including linked data forms. Public data will be freely available to use in any lawful way. Public data will be available without application or registration, and without requiring details of the user. Public bodies should actively encourage the reuse of their public data. Public bodies should maintain and publish inventories of their data holdings. Public bodies should publish relevant metadata about their datasets and this should be available through a single online access point; and they should publish supporting descriptions of the format provenance and meaning of the data. your users information and informed choice about the services they use your service managers the information they can rely on to provide what your users need businesses and the community or voluntary sector the opportunity to take the data released and produce goods and services from it Analytics tools Open formats Open standards Open Data White Paper: Unleashing the Potential Open Government Partnership: UK National Action Plan 2013 National Information Infrastructure Data that is truly open is: Overall, government produces a lot of data that describes the services that we offer and how well those services are performing, for example data from analytics tools or key performance indicators. There’s also data on how people use these services and who those people are. Data about service performance allows service managers to see how well a service is running. It also means that users can hold us to account. Data about service performance should therefore be public data. You should publish all public data, unless it is private data collected from people or restricted for national security reasons. Public data is anonymised data: If for some reason you have made a procurement choice that means your performance data is monitored or stored by a third party, you should make sure that you have the right to access, export, share and reuse that data openly and in an open format. Your department has its own Open Data Strategy (for 2012-2014) and will already be publishing data sets on data.gov.uk. Your commitment to open, public data doesn’t stop there. Your open data should be user-friendly and findable. You should support people who want to reuse it and provide guarantees about how it will be made available. Publishing on data.gov.uk makes your data findable and will tell you how it ranks on a 5-star rating scheme to indicate whether the data and the format that it is published in is open. You should aim to reach at least the 3-star standard. The Open Data Certificates from the Open Data Institute measure how usable the data is for people who want to reuse it, and help identify ways in which you can improve how you publish the data. You should get a certificate for your data. The Open Data Principles that you must follow are: Users are already using your services, giving you lots of data about their behaviour. This means you can learn from real world behaviour when you’re designing a new digital service. You can watch and learn from your users, shaping the system to fit what people naturally choose to do. Recognise that you can’t do it all. In making your data open, you are encouraging greater use of the data and helping users to innovate. Developers need to be able to use the data, to share it, and combine it with other data to use in their own applications, for example through application programming interfaces (APIs). As with all government digital services you’ll need to understand the user need for the data you publish. You’ll need to keep developers aware of what datasets you are releasing and to maintain relationships with those primary data users at the cutting edge of technology who can help you to do things differently and in more agile ways. You can use the engagement areas of data.gov.uk to reach out and keep in touch with data users. You need to be scrupulous in protecting individual privacy by taking appropriate steps to ensure personal data is secure. You should consider privacy issues at the beginning of all discussions concerning the release of a new dataset or the building of or change to a digital service dealing with personal data. If you are using new technology to handle personal data or reusing the data in a different way, you may need to carry out a Privacy Impact Assessment. This is an important part of the process for identifying and managing risk. The Cabinet Office is preparing a new Code of Practice (Datasets) (the consultation on this draft version is now closed). This code includes the licensing framework you must use when making your datasets available for reuse. There are a number of licenses in the framework. For UK government open data, this will be the Open Government Licence. Your open data can give: Building your services on open formats and open standards means that you can more easily share, reuse or exchange data. It will also mean that you will have a choice in which technology to implement, rather than being limited to a particular product or supplier. You can browse the apps section of data.gov.uk to see hundreds of applications that have been built using open government data. You can also find case studies of how people are sharing, using and building open data on the data.gov.uk site. The team at data.gov.uk is a good first point of contact for help in understanding how to make your data open. You can get in touch with them through data.gov.uk. The Open Data Institute and the Open Knowledge Foundation can give you advice and training on how to open up your data. The Government Statistical Service has developed computer-based training to help you to make the right decisions about how to make data available in open formats.","description":"Data that is truly open is:","link":"/service-manual/technology/open-data.html"},{"title":"Security as enabler","indexable_content":"Security must be proportionate and justified User experience and security aren’t exclusive Commercial threats, commercial solutions Trust responsible users, audit and verify New classification policy Procurement Top 8 security myths 1. “Security says No!” 2. Accreditation of government systems is costly, time consuming, and doesn’t help secure them. 3. Open source software is more / less secure than proprietary code. 4. “Product X can’t be used because it isn’t accredited by CESG” 5. Restricted systems need to have bespoke security controls. 6. “I’m not a target for cyber attack!” 7. Impact Levels define security requirements. 8. “I’ve got lots of IL3 records, which aggregate to IL4, so I need to build a confidential back end system” Protecting information from valid threats to its confidentiality, integrity and availability is an enabler of digital services. Without such protection, digital services would be impossible or unsafe. Please note: this guide sets out the governing principles for developing efficiently secured digital services. It also busts some common myths around security. There is another guide about the practical process of security accreditation. Security must be applied intelligently. This means analysing the probable interest in official information from threat sources, establishing their capabilities and methods, and matching proportionate mitigations against these in a traceable manner. Other attempts at security run the risk of over-engineering security controls, or providing an illusion of security by not mitigating the actual risks. Technology alone is never capable of addressing security and privacy risks: there needs to be a risk model that spans technology, people (their behaviours and culture), and processes. Bad user experience arising from over-prescriptive use of technology can lead users to circumvent security controls by employing less secure unofficial IT solutions. Users who then retain the official IT systems suffer degraded productivity. The outcome is that security is not maintained and user experience is unpleasant. However, security and a good user experience do not need to be mutually exclusive. Modern, intelligently designed security can often be made largely transparent to the user, while also providing the enterprise with the confidence it needs that its information is suitably managed. Where some degradation of user experience is unavoidable, a risk management analysis must consider the negative impact of users avoiding unpleasant official IT and degraded productivity. Furthermore, a transparent understanding of the mapping between official information, threat sources and capabilities, through to the required mitigations, enables informed risk management when there is a change in environment or appetite for risk. The vast majority of government’s information will fall into the lowest ‘official tier’. This ‘official tier’ has been created on the basis of a ‘commercial threat model’. This means protection from the type of threats faced by a large company or bank, eg cyber criminals or hacktivists intent on stealing personal or financial information, or disrupting services. To defend against threats such as these, government will use only the very best security technology sourced from the commercial market, and there will be no need for any bespoke or government-only controls at this level. A small set of software will perform security enforcing functions, such as firewalling or encrypting data. Such products require assurance that they function as advertised, achieved through the CESG Commercial Product Assurance (CPA) scheme. This scheme is lightweight compared to previous schemes, reflecting the commercial grade risks for OFFICIAL information. It’s the intention of the Civil Service Reform Plan and the new Security Classification Policy that there’s greater emphasis on user responsibility, reducing expensive and overbearing technical controls. This requires proper training to assist users in handling sensitive information, and auditing to verify users are acting responsibly. Users should be trusted to carry out their roles and given the responsibility to do so securely. Audit and verification of user behaviour should be used to ensure policy compliance instead of preventative measures which add cost and degrade productivity. Such audit and verification should be implemented by services or network infrastructure, away from the end user device. Government should not invest in security controls to protect users from risks they can protect themselves from. Departments should, however, invest in security controls that help defend individual users against threats that they themselves cannot reasonably defend. For example, government should not invest in special technology to prevent civil servants working on sensitive information in an open busy public place; users should be able to judge, assess and use appropriate risk mitigation approaches by themselves. Likewise, civil servants should be able to exercise reasonable judgement about what information is sent to external recipients by email over the public internet. This will lead to reduced technical controls and their associated costs, while also optimising the usability and flexibility of the IT tools for the majority of responsible users. The new security policy provides for a simpler and more meaningful approach to denoting the value of information assets and threat model. Together with a set of common controls for OFFICIAL information, it enables a more consistent, standardised, reusable and interoperable approach to securing information assets in government. There should no longer be situations where information cannot be shared between bodies solely because their interpretation of the same security guidance has led to incompatible controls. The Cabinet Office Security Classification Policy provides guidance as to which information falls into the ‘official’ category. It’s important to start thinking about security from the very start of an IT procurement project, as bolting it on later invariably introduces additional cost and delays. In fact, getting appropriate security into your system needs to start with specifying security requirements correctly in the contract. When we don’t adequately articulate security requirements for design and ongoing maintenance of a system, our suppliers price for the level of contractual risk the ambiguity introduces. Specifying security requirements of the form “must be accreditable for IL3” must become a thing of the past. The risk appetite of departments varies and suppliers not used to working with government will struggle to find out what that means for the design and maintenance of their system – adding cost and risk to both parties. Instead, when writing IT contracts, departments should think about what they care about from an information assurance perspective and specify requirements that actually manage their concerns. As well as getting a solution that address the real business needs, this approach also allows industry to innovate to solve the security problem. It is often said that security is the reason something can’t be done. This is very rarely the case in practice – rather, security is being (mis)used as a handy excuse to not do something. Good information risk management practices allow organisations to understand the risks they are taking with their information assets, and work out the most effective ways to manage and control those risks – while not hindering business. If someone tells you that “Security says No”, or that “CESG say No”, you should ask for more information to learn what the risks actually are, and what techniques and tools are available to help manage those risks. Accreditation effort should always be proportional to the complexity, threat and impact of a system. It is vital that the effort spent should scale to match the challenge at hand. Can the business accept the risks of undertaking a given activity? Accreditation documentation only needs to contain the information needed to enable this decision. Clearly, any accreditation activity which just generates documentation that’s never read is not adding value, nor helping to secure information. When performed well, accreditation helps risk owners have confidence that all aspects of a system’s security have been appropriately considered, and that proper through-life security processes are in place to maintain information security. Experience has shown that the licensing model for software is not an accurate gauge for the security of the finished item. There are very securely developed open source projects and proprietary products. The opposite is also true. The experience and competence of those developing code is of primary importance. The same considerations should be given to the security of open source products as are applied to proprietary code. The Open Source toolkit has more information. There are 3 problems with this statement. Firstly, CESG don’t accredit products – they provide assurance in the security properties of products and certify them when they meet various standards. Secondly, only those products which provide a security enforcing function need to be evaluated and certified by CESG; products like switches or email servers don’t need to be CESG-certified. Finally, CESG certification of a product is a component of risk management – it doesn’t absolve an organisation of their responsibilities to information security. While using certified products should certainly speed up the risk management process, it isn’t mandatory. Restricted systems can be built today using assured commodity security products – many of which are found natively in the operating systems of modern platforms. In the past, some of the security controls which were recommended for protecting Restricted IT systems were bespoke variants of commodity products, or were “government specials”. The requirements which drove these choices are no longer relevant given advances in commodity IT, and better approaches are available to provide an equivalent level of security, but with enhanced usability and overall cost of ownership. Foundation grade assured products are appropriate for the protection of these systems, and commodity security products (including open source) can achieve this grade of certification. Sometimes it’s obvious that attackers are going to be interested in getting hold of your sensitive information – it has clear value to someone. However, immediate financial reward isn’t the only reason government IT systems get attacked. Attackers may be disgruntled insiders, seeking to disrupt or embarrass an organisation. Attacks may come from protest groups, seeking a platform for their views. Or attacks may seek to use one system as a point to launch attacks on another – so you might not be the primary target, but might unwittingly be a conduit to them. Are you specifically being targeted? Maybe not – but your systems and internet presence probably are being attacked on a daily basis for a wide range of reasons. Asking a someone to build an “IL3 system” or to “protect at IL5” is a misuse of the Business Impact Level framework, and will generally cause confusion and an ineffective approach to securing information. Business Impact Levels are intended to help organisations to think about the consequences of certain events, and thus steer where effort should be used when managing information risks. There is no standard set of technical, procedural or personnel controls for any particular Impact Level, and so specifying requirements in terms of Impact Levels is unclear and inaccurate. Instead, security requirements should be defined in terms of the level of protection that each information asset should be given, and expectations around its through-life protection. It’s true that if you are building a system which handles many records, the impact of loss of many of them is likely to be higher than loss of one or two. Although Business Impact Levels are often misused as shorthand for protective markings, there is no direct mapping from Impact Level to protective marking. In the IL3 aggregated to IL4 example, rather than building a system capable of storing confidential records, it’s likely to be acceptable to simply add some simple additional controls around the data store, to reduce the risk of a bulk loss of data.","description":"Protecting information from valid threats to its confidentiality, integrity and availability is an enabler of digital services. Without such protection, digital services would be impossible or unsafe.","link":"/service-manual/technology/security-as-enabler.html"},{"title":"Service integration and management","indexable_content":"Service integration today What service integration looks like Involve everyone in a clear process being able to define different service requirements for critical and non-critical services (for example, some commodity public cloud services may require online service support or service desk only, whereas mission critical IT systems will require a more integrated service model) a performance regime that ensures organisations don’t pay for services they can’t or don’t use explicit service integration arrangements that focus on service performance, usability and availability from a user perspective, not just from a supplier’s commercial perspective skills and capabilities that support transitioning to, and managing services in a new commodity-based environment a focus on open standards and interoperability to support workflow, performance management and service management, billing and payment Service integration and management lets an organisation manage the service providers in a consistent and efficient way, making sure that performance across a portfolio of multi-sourced goods and services meets user needs. Service integration models have been around for some time, but are now evolving from the challenges of managing a small number of large suppliers (typically systems integrators) to a model of managing a greater number of smaller suppliers, often providing commodity services. For the model to be effective the component services need to be well defined and understood. It’s important to avoid any ambiguity about the boundaries of both responsibilities and accountabilities. Key features of an effective model are: The level of service integration will differ depending on the complexity of the business services and/or customers that are being supported, and the complexity of the services that are being delivered to those businesses. As the services and businesses become more critical or complex, the level of service integration becomes deeper.  The design of the service integration function will differ by department. It may be completely operated in-house. Or it might consist of a thin in-house capability ultimately responsible for the integrated end to end operation and management of quality IT services, underpinned by outsourced integration services for specific elements – for example performance monitoring, service desk, or service level reporting. The G-Cloud framework offers a number of services to support service integration. Particularly for smaller departments and simple services, care needs to be taken not to over-engineer the service integration approach – effective use of commodity standards-based IT should mean that integration and support requirements are much less onerous than managing a locked-down bespoke system. As part of service integration, you should maintain an accurate service catalogue including a service dependency map, so that you can effectively manage changes that are high risk, high impact, or that can affect multiple suppliers. When buying in services you should retain the contractual authority to ensure suppliers follow your service integration processes – so that you can ensure the integrity and availability of a department’s user services. This could include an end-to-end service performance incentive model so all suppliers collectively share in the benefits and penalties of a joint performance management regime.","description":"Service integration and management lets an organisation manage the service providers in a consistent and efficient way, making sure that performance across a portfolio of multi-sourced goods and services meets user needs.","link":"/service-manual/technology/service-integration.html"},{"title":"Spending controls","indexable_content":"The process What will be examined Starting the process advertising, marketing and communications strategic supplier management, including disputes commercial models ICT Digital Service Delivery (including identity assurance) external recruitment consultancy redundancy and compensation learning and development property and facilities management greater public and market transparency and two-way engagement through publishing business needs and subsequent decisions towards a solution greater emphasis on problem exploration, solution discovery and market engagement activity before any formal procurement is initiated greater use of agile iterative development over linear waterfall delivery contracts (including contract amendments, extensions or renewals) licences expenditure through existing frameworks feasibility and/or proof of concept studies pilots projects (or an element of a project) and programmes ‘business as usual’ changes or enhancements or maintenance or refreshes common infrastructure solutions including voice and data communications, Public Services Networks (PSN) whether fixed or mobile. Organisations should follow the rules set out in Managing public money which explains the overriding principles for dealing with resources used by public sector organisations in the UK. The government’s Spend Controls supplement those rules with specific controls for spend in: A request for spend approval is likely to require a number of controls – most digital and IT spend requests will also require commercial approval to make sure they are aligned to government procurement policy. Each department has a GDS account manager who will ensure that other areas of Cabinet Office are engaged as needed when considering a case. An online controls tool has been introduced to support this process and give full visibility on case progress. The purpose of the IT and digital controls is to ensure that spending on IT is proportionate and directed at programmes and projects that meet the commonly agreed digital and ICT strategies. It’s an independent confirmation that the department has undertaken thorough consideration of the user need and value for money solution options. The controls process has changed over time to ensure: GDS will examine and challenge all technology-related spending over a certain threshold – no public commitment or expenditure should be made above these thresholds without prior approval. The thresholds are set out in our technology code of practice: These thresholds cover all technology expenditure, including: They also cover programmes that are broken into smaller chunks or projects, which individually are below the thresholds, but which in aggregate are above them. Intentionally breaking down a programme into smaller elements to avoid the controls process is itself a serious breach of the controls. Digital is defined as any external-facing service provided through the internet to citizens, businesses, civil society or non-government organisations.  This includes, but is not limited to information services, websites, transactional services, web applications, mobile apps, and extranets. The control covers the total cost of running that service, including any assisted digital element such as support provided through face to face, phone, third party intermediary or other channels to access the digital service. The definition of digital does not include content creation on existing sites, intranets or those areas listed under the advertising, marketing and communications controls. Departments should engage early with GDS to agree the appropriate points for approval, before the first approval is needed (usually after the discovery phase, at strategic outline case or before a prior information notice is issued).  This early engagement will avoid departments proceeding too far without approval and reduces the risk of irregular expenditure. Full details of current requirements are detailed in the Cabinet Office ICT Spend Control Form (currently V.2.18).  You can start the process by e-mailing gdsapprovals@digital.cabinet-office.gov.uk. If your request contains ‘Restricted’ information it should be sent to ictspendapproval@cabinet-office.gsi.gov.uk. Note that your organisation may have established a single point of contact for engaging with GDS, so check first within your organisation. If your project cost exceeds your department’s main delegated authority set by Treasury it will be classed as a major project. GDS and HMT have released a guidance clarification explaining how agile approval can be streamlined in these cases. You will still need to engage with the Major Projects Authority (MPA). HMT’s ‘Major Project approval and assurance guidance’(PDF) describes the Treasury’s and the MPA’s process. The Cabinet Office’s ‘Integrated Assurance and Approval Plan’(PDF), explains the approvals needed through the lifecycle of the project. Digital projects will also be assessed for compliance to the service standard – as described in this flowchart: ","description":"Organisations should follow the rules set out in Managing public money which explains the overriding principles for dealing with resources used by public sector organisations in the UK.","link":"/service-manual/technology/spending-controls.html"},{"title":"Accessibility skills","indexable_content":"Accessibility leads Related links strong analytical skills a methodical approach to testing empathy for users with different, often contradictory needs an in-depth knowledge of modern accessible development best-practices the ability to communicate at all levels Accessibility is everyone’s responsibility. Provide training and resources to help your team build accessibility into everything they do. That includes planning, designing, building and managing. Include at least one person with strong accessibility knowledge in your team. Give them overall responsibility for educating and supporting your team’s accessibility goals. The key skills required by an accessibility-focused developer are: Accessible, usable products are the heart of everything we do at the GDS. Your service needs to be just as thoughtful to the needs of all possible users so that no one is excluded on the basis of disability. Read the guidance on accessible design","description":"Accessibility is everyone’s responsibility. Provide training and resources to help your team build accessibility into everything they do. That includes planning, designing, building and managing.","link":"/service-manual/the-team/accessibility.html"},{"title":"Content designer","indexable_content":"The importance of content designers Skills and attributes Guidance Job description identify user needs – based on:     legacy content       source material provided by policy colleagues       feedback from users and stakeholders       analytics data both from the site and from search engines      legacy content source material provided by policy colleagues feedback from users and stakeholders analytics data both from the site and from search engines gain an in-depth knowledge of a wide range of subjects – so they can make informed decisions about the best way to present information to users develop content plans and strategies – high-level plans showing how the identified user needs will be met write great content – in plain English, optimised for the web and according to house style edit content – making sure the site remains accurate, relevant, current and optimised both for users and search engines make tough decisions and work hard for the user – grappling with complicated legislation and turning it into clear, clean, crisp web content (that still has enough depth to be useful) work with developers and designers to create better solutions – for example, writing logic and content for smart answers understand and incorporate the results of user testing review the work of other editors – to make sure consistency and excellence is maintained across the site publish content – using various systems communicate the principles of good content design to others in the organisation advocate for the user and act as a guardian for the site – pushing back on change requests that don’t contribute to meeting user needs and incorporating change requests that do build positive relationships with others inside the team and in the wider organisation innovate and anticipate – excellent content designers are excited about the possibilities of web content and contributing to the digital sector’s future Content designers make sure that the writing on the site or service meets the needs of the user as clearly, simply and quickly as possible. You can add up, but it doesn’t mean you’re an accountant. You can write, but it doesn’t mean you’re a content designer. For many reasons, in the past the government has often published content that’s difficult to understand and difficult to act on. What gets published can be more about what the government wants to say than what the user needs to know. At best this results in a frustrated user. At worst, citizens and businesses get into trouble because they can’t understand (or can’t face wading through) difficult content. The content designer’s job is to make sure that doesn’t happen. Content designers must be able to: Read guidance in the manual of particular interest to content designers. Click either of the options below to download a template Content designer job description. Download as Open Office doc / Download as MS Word doc Cabinet Office will help departments to recruit suitably skilled individuals through the Recruitment Hub.","description":"Content designers make sure that the writing on the site or service meets the needs of the user as clearly, simply and quickly as possible.","link":"/service-manual/the-team/content-designer.html"},{"title":"Delivery manager","indexable_content":"The importance of delivery managers Skills Job description templates Further reading strong estimation and budget scoping skills experience in Agile Project Management methodologies familiarity with structured programme and project management environments experience delivering digital services experience in open source and cloud technologies and their sourcing good communication skills strong organisational and communication skills collaborative approach to working good at prioritising time-critical work an understanding of the wider digital landscape Good health check from the Scrum Alliance for delivery managers. A day in the life of a delivery manager - blog The delivery manager sets the team up for successful delivery. Remove obstacles, or blockers to progress, constantly helping the team become more self organising. They enable the work a team does rather than impose how it’s done. Skilled delivery managers remove obstacles, or blockers to progress, constantly helping the team become more self organising. They enable the work a team does rather than impose how it’s done. It’s not about micro managing! Equally important in an agile team – and particularly important to the delivery manager – is ongoing effort to improve products, services or processes. Their role in this is to facilitate project meetings- including daily stand-ups, sprint planning meetings, and retrospectives. They also track progress and produce artefacts for showing this, like burn down/up charts. They must be able to enable the team to produce estimates of how much effort is required to produce features that the Product Manager wants.  Delivery managers need to have: A delivery manager will also need the following skills: Click either of the options below to download a template Delivery Manager job description.  Download as Open Office doc / Download as MS Word doc  Cabinet Office will help departments to recruit suitably skilled individuals through the Recruitment Hub.","description":"The delivery manager sets the team up for successful delivery. Remove obstacles, or blockers to progress, constantly helping the team become more self organising. They enable the work a team does rather than impose how it’s done.","link":"/service-manual/the-team/delivery-manager.html"},{"title":"Design skills","indexable_content":"How designers work How to hire designers Job description templates Further reading Specific guidance for designers working on digital by default services. Those with the title of designer may have a particular focus on one or more specific design disciplines – interaction, graphic, UX – but a good digital service needs talented, flexible designers to help build user-centred products. Designers, user researchers and front-end developers should work together in one team, designing in-browser. This is a better way of working, avoiding silos and ensuring that decisions are made with complete awareness of the implications. As a result, the people you hire should already have worked like this, or at least understand it. Depending on the types of project you are tackling you may require a team of designers with a range of different skills. A good first hire for a team tends to be a strong interaction designer, however adding designers with graphic design skills and designers who also have a background in undertaking user research can also give your team additional flexibility and capabilities. We strongly believe that design and user experience is the responsibility of the entire team and must be considered from the outset of the project through to and beyond going live. UX includes how fast the servers are, to how the copy is written, to how the layout is implemented in code, and what the structure of the URLs is. It’s worth looking at Frances Berriman’s talk on this. When building a team ask to see examples of work and ask the designers to explain their practical contribution. You should look for the designer to share stories and documentation that demonstrates how they have worked closely with other members of the team including developers, content designers, user researchers and stakeholders in an agile and iterative way. When evaluating their design work, it is important that the designer can explain a strong rationale for their design decision making that is based in supporting user needs. This ability to explain their rationale convincingly is more important than their ability to show polished wireframes or designs in their portfolio. Often a lack of polished designs in a portfolio can indicate that a designer is highly collaborative or has strong development skills, so take care not to focus only on polished comps or layouts. You should also ask the designer to talk about any significant differences between the design they present in their portfolio and the actual live design. It is important that designers create appropriate work and are able to persuade stakeholders and their team to make good design decisions. Click either of the options below to download template Designer job descriptions.  Designer – Download as Open Office doc / Download as MS Word doc  Junior Visual Designer – Download as Open Office doc / Download as MS Word doc  Cabinet Office will help departments to recruit suitably skilled individuals through the Recruitment Hub.","description":"Those with the title of designer may have a particular focus on one or more specific design disciplines – interaction, graphic, UX – but a good digital service needs talented, flexible designers to help build user-centred products.","link":"/service-manual/the-team/designer.html"},{"title":"Developer skills","indexable_content":"The importance of developers Skills and attributes Developers in the team Guidance Job description builds software with a relentless focus on how it will be used seeks collaboration and early feedback designs software they expect to operate and maintain leaves code simpler and better tested than when they started looks for opportunities to share progress and knowledge is always hoping to learn from colleagues and the wider community distinguishes the important from the urgent uses data to make decisions, building tools to gather that data have deep skills in at least one programming language be aware of the differences between a few languages and frameworks, and be pragmatic at picking the right one understand the core concepts of the internet and web – they should be able to give a good answer to the question ‘what happens when I click a link in a web browser?’ be deeply committed to testing their work with automated tests and exploratory testing be able to explain their work to people without particular technical skills Developers build software with a relentless focus on how it will be used. They continually improve the service by identifying new tools and techniques and removing technical bottlenecks Good digital services will require code to be written, adapted, maintained and supported. A team of skilled developers will be able to make sure that happens in an efficient and transparent way, and to help continually improve the service by identifying new tools and techniques and removing technical bottlenecks. No digital service can be effectively built, delivered, owned and operated without the technical skills to understand and improve the software enabling it. In order to provide the best service to your users it is vital that you are in a position to rapidly tailor that software to their needs and to the efficient running of the underlying systems. Developers will be able to work directly on those services, but are also an important part of service innovation as they bring ideas, generate prototypes and contribute to a rounded team.  Once a service is live the need for developers continues. There will always be technical optimisations that can be made – faster and more responsive systems, acting to mitigate the risks of a constantly changing security environment – but as you respond to new or more clearly understood user needs the software will need to adapt. As the policy and other context around a service changes the software may need to integrate with new systems or provide new features and a development team can help ensure that work is pro-active. A great developer: You would expect any developer to: It’s really important that your team is clear as to who makes technical decisions. Everyone on the team will have useful knowledge, skills and experience to bring to bear and you will need an agreed way to understand that input and make a call. Some teams accomplish that by appointing a Tech Lead or delegating certain decisions to a Technical Architect, others will put that responsibility in other roles. The important thing is that clear technical decisions be made by people equipped to understand the trade-offs involved. As your team grows it is likely that you will find a range of skills and experience. You should be ensuring a balance within the team, making sure that more junior team members are well supported and coached by more experienced team members, but that everyone’s ideas are considered and engaged with by the team and its decision maker. Read guidance in the manual of particular interest to developers. Click either of the options below to download a template Developer job description.  Download as Open Office doc / Download as MS Word doc  Cabinet Office will help departments to recruit suitably skilled individuals through the Recruitment Hub.","description":"Developers build software with a relentless focus on how it will be used. They continually improve the service by identifying new tools and techniques and removing technical bottlenecks","link":"/service-manual/the-team/developer.html"},{"title":"Digital leaders","indexable_content":"Digital by default GOV.UK Digital leaders act as the single point of contact for the department’s strategic interactions with GDS, including co–ordinating digital activity for Departmental Agencies, ALBs and NDPBs. They are senior civil servants (generally executive or director general level) drawn from every department of government and the devolved administrations, and they have experience of leading large scale transformation programs inside and/or outside government. Digital leaders promote and encourage take up of ‘Digital by Default’ within their department, with stakeholders and customers. This means actively participating in the delivery of the Government Digital Strategy and Departmental Digital Strategy actions, ensuring these are fully embedded in the Department’s Business Plan. Digital leaders also act as the strategic governance mechanism for the GOV.UK domain, informing policy and strategic direction of digital.","description":"Digital leaders act as the single point of contact for the department’s strategic interactions with GDS, including co–ordinating digital activity for Departmental Agencies, ALBs and NDPBs.","link":"/service-manual/the-team/digital-leader.html"},{"title":"Induction and development","indexable_content":"Induction programme Open programme Request a place Day 1 Day 2 Day 3 Why are we here? Putting users first Digital leadership Thinking agile Meeting the service standard Building the team Technology for digital services Make, test and learn KPIs: setting the bar Increasing take-up Assisted digital Always be testing Design principles in action Identity assured Procure to improve Being agile Technology talks why we need to change the way government does digital what has happened so far, including the launch of GOV.UK and adoption of the 25 exemplar transformations what this means for agencies and departments in general and for the role of service manager in particular meet some users of government digital services explore different techniques for finding out what users need write some user stories decide what requirements count as user needs find out how high achieving digital leaders get things done so you can model and demonstrate that behaviour in your department hear war stories from someone who is a leader in the digital field take away tips and practical advice on digital leadership agile as a mindset, much wider than a set of tools and techniques the principles to be found in the Agile Manifesto and how they apply to software projects and beyond the agile disciplines that keep teams focused on outcomes and maximise the chances of success learn about the standard, how it was created and what tools are available to help departments meet it consider how services will be assessed against the standard assess which parts of the standard are most challenging for your own services and department identify development needs to help you and your department to meet the standard understand the roles needed in an agile, user-centred service re-design team get to the heart of what job titles and descriptions really mean, so that your department can appoint the right people, and at the right levels consider the team’s tools and working environments identify specific development needs for yourself and your team members choosing technology, making it possible to change your mind and avoiding lock-in risk and risk management, and getting the whole team thinking about security questions to ask your development team to help them make the best technology decisions possible self-organise to respond to a real-life brief discover user needs, write user stories and prototype solutions to meet them test your solutions with target users and iterate based on what you learn      share your service KPIs as they currently stand         consider what additional KPIs may be required for your services to measure satisfaction, success rates, cost per transaction and take-up         find out about the Performance Platform and how you can use it         design your own performance dashboard to show KPIs for your services in a simple, visual way         hear from GDS specialists in digital take-up         consider barriers to adoption and how your services can overcome them         assess the impact of channel shift on non-digital processes         who are the users who need assisted digital support         what good assisted digital support will look like         how departments and GDS are working together to develop assisted digital support approaches as part of overall service transformation         keep your team concentrating on real user needs         help teams design products which are prioritised by user needs          help teams iterate products in response to user feedback         discover the 10 design principles and how they were created         hear from a product manager who has used them on a project         consider how they will be applied to your own services         creating a market so that users can choose from a number of identity providers         setting standards for consistently meeting the needs of users, services and departments         building and running the hub that connects services to identity providers         working with departments and agencies to identify their services’ identity assurance requirements and plan their transition to using the identity assurance service         consider how to procure tools and systems fit for the purpose of agile, user-centred development         discuss the range of options available, and how to determine make or buy decisions         hear the latest on the government procurement processes and frameworks available on digital transformations         agile artefacts, themes, epics and user stories         sprint planning and the use of walls to make shared priorities visible         retrospectives to continually inspect and improve the work teams do together    GDS is running this highly practical induction and development programme so you can strengthen the skills, knowledge and network you will need to succeed as a service manager. For 2014, modules are divided between two formats: Induction programme Four days to equip newly appointed service managers with the basic knowledge, network and confidence to take the lead in transforming digital services. Open programme Three days of more specialist modules to help you succeed and improve in particular areas of the Digital by Default Service Standard. Who should attend All newly appointed service managers should book to attend both the induction programme and all 3 days of the open programme. More experienced service managers or others in specialist roles in government may choose to book onto any or all of the open programme modules. All sessions will run at a central London location. The programme is funded centrally, but you or your department will need to pay for any accommodation, meals and subsistence expenses, in line with your departmental policies. For newly appointed service managers, this aims to equip them with the basic knowledge, network and confidence to take the lead in transforming digital services. The induction programme includes the following modules: Together we’re leading the digital transformation of government, by making services so good people prefer to use them. This introductory session explores: User-centred processes start with user needs, and so does this induction programme. In this session the group will: Which principles of leadership can be brought to bear in the new era of agile digital services? Your group will: Agile marks a fundamental shift from traditional methods of delivery in government. This session will introduce agile thinking and show how it leads to better outcomes. You’ll cover: The Digital by Default Service Standard is the benchmark your services must reach in order to be taken live on GOV.UK. You will: As a service manager you need the support of a multidisciplinary team working intensively together through rapid iterations. Your group will: You may not be the most technical person on your team, but as service manager you cannot leave all the technology decisions to others. This session will cover: It’s time to try out the methods and tools picked up on the programme so far. In this practical exercise your group will: These 3 days contain more specialist modules to help you succeed and improve in particular areas of the Digital by Default Service Standard. The open programme includes the following modules: The group will discuss how to set and measure key performance indicators so you can lead your services in the right direction. Based on information you will prepare beforehand, you will: share your service KPIs as they currently stand consider what additional KPIs may be required for your services to measure satisfaction, success rates, cost per transaction and take-up find out about the Performance Platform and how you can use it design your own performance dashboard to show KPIs for your services in a simple, visual way Digital take-up is one of the four KPIs that services must benchmark, measure and monitor to meet the Digital by Default Service Standard. The group will: hear from GDS specialists in digital take-up consider barriers to adoption and how your services can overcome them assess the impact of channel shift on non-digital processes Most users will be able to use new and redesigned government digital services unaided, but there are some who will need help. This support is referred to as assisted digital. The group will discover: who are the users who need assisted digital support what good assisted digital support will look like how departments and GDS are working together to develop assisted digital support approaches as part of overall service transformation Carry out user research in every stage of your project. Do it continuously through each stage – don’t leave it as something that happens at the beginning and end of phases. You will find out how doing user research continuously will: keep your team concentrating on real user needs help teams design products which are prioritised by user needs  help teams iterate products in response to user feedback A set of simple but powerful design principles underpin all the work done by GDS to develop the award-winning GOV.UK website. In this session your group will: discover the 10 design principles and how they were created hear from a product manager who has used them on a project consider how they will be applied to your own services We need to know that users of digital services are who they say they are. In this session, you will learn how the Identity Assurance Programme is: creating a market so that users can choose from a number of identity providers setting standards for consistently meeting the needs of users, services and departments building and running the hub that connects services to identity providers working with departments and agencies to identify their services’ identity assurance requirements and plan their transition to using the identity assurance service Delivering high quality digital services that improve continuously often depends on effective procurement. As a group you will: consider how to procure tools and systems fit for the purpose of agile, user-centred development discuss the range of options available, and how to determine make or buy decisions hear the latest on the government procurement processes and frameworks available on digital transformations GDS delivery managers will lead discussions and exercises around key features of agile, such as: agile artefacts, themes, epics and user stories sprint planning and the use of walls to make shared priorities visible retrospectives to continually inspect and improve the work teams do together GDS technical architects will give a series of short talks and Q&A sessions about important aspects of choosing and managing technology for digital services. To request a place on the induction programme or the open programme, please fill in this SurveyMonkey form Any questions? Please contact the GDS Capabilities Team: service-manager-programme@digital.cabinet-office.gov.uk","description":"GDS is running this highly practical induction and development programme so you can strengthen the skills, knowledge and network you will need to succeed as a service manager.","link":"/service-manual/the-team/induction-and-development.html"},{"title":"Recruitment Hub","indexable_content":"Senior Civil Servant (SCS) recruitment Non-SCS specialist recruitment feedback, advice and approval on tailored organisational design and SCS job descriptions as needed, to ensure that the lines of reporting and share of responsibilities between other digital and technology (IT) leaders within organisations are clearly set out. coordinated headhunting support for departments looking to identify suitable candidates for senior appointments GDS expertise to work with departments in sifting, shortlisting and interviewing of applicants feedback and advice on the design and organisation of digital delivery teams within departments coordinated headhunting support for departments looking to identify specialist digital skills GDS expertise to work with departments in sifting, shortlisting and interviewing of some key Band A / Grade 6 / Grade 7 specialist posts GDS is supporting departments on digital and technology recruitment through a Recruitment Hub. This service is designed to help ensure the best possible candidates are in post to deliver excellent digital public services supported by the right technology. The hub will maintain the generic advice and job descriptions published in the service manual, support departments in tailoring them to meet their needs, and coordinate sifting, interviewing and headhunting support for senior and specialist roles. The Recruitment Hub is not intended to replace existing HR and recruitment processes, and exists purely to help organisations acquire these new and hard-to-reach skills. All queries for the hub should be made to digitaltalent@digital.cabinet-office.gov.uk. GDS must approve the job descriptions and appointments for all SCS-level technology positions. All senior grades (Deputy Director / Grade 5 level and above) are part of the Senior Civil Service, which is overseen by the Cabinet Office on behalf of the civil service as a whole. Senior civil servants may be called to account by Parliament, and are barred from holding any political office.  The template job descriptions, organisation design advice and salary guidance should be a useful starting point for departments and agencies to tailor to their specific roles. For SCS level hires, the Recruitment Hub will also provide: After a job description has been agreed and advertised, departments should contact GDS to confirm a date for sifting the applicants and producing a shortlist. Once a shortlist has been agreed, the department should confirm interview dates with GDS so an interviewer can be made available to join the panel. Following interviews taking place, the department must agree with GDS that the candidate selected at interview is suitable for the role before offering the appointment. Departments should continue to liaise with advertisers and Civil Service Commissioners directly. GDS does not need to approve the appointment of non-SCS specialist posts, but will provide a similar level of support for departments seeking to bring in specialist digital skills. The template job descriptions, organisation design advice and salary guidance should again be a useful starting point for departments and agencies, and is likely to require less tailoring than SCS roles. In addition, the Recruitment Hub will provide: In specific cases where a position requires a particular specialist set of technical skills (eg technical architects, designers), departments should request sifting or shortlisting support from GDS via the Recruitment Hub. We cannot promise to provide this in all cases, but will endeavour to make expert advice available from the shortlisting stage for crucial posts. This service should be requested before the post is advertised.","description":"GDS is supporting departments on digital and technology recruitment through a Recruitment Hub. This service is designed to help ensure the best possible candidates are in post to deliver excellent digital public services supported by the right technology. The hub will maintain the generic advice and job descriptions published in the service manual, support departments in tailoring them to meet their needs, and coordinate sifting, interviewing and headhunting support for senior and specialist roles.","link":"/service-manual/the-team/recruitment/hub.html"},{"title":"Job descriptions","indexable_content":"Templates Senior Civil Servant (SCS) roles Non-SCS specialist roles information about the department and its objectives information about the role and current priorities / activities in digital and technology location recruitment process details and contacts information on reporting lines GIS declaration and Immigration Status Forms Chief Digital Officer (CDO) Download as Open Office doc / Download as MS Word doc Chief Technology Officer (CTO) Download as Open Office doc / Download as MS Word doc Head of Business IT Download as Open Office doc / Download as MS Word doc Head of Policy and Performance Download as Open Office doc / Download as MS Word doc Service Manager Download as Open Office doc / Download as MS Word doc Technology Lead Download as Open Office doc / Download as MS Word doc Business analyst Download as Open Office doc / Download as MS Word doc Content designer Download as Open Office doc / Download as MS Word doc Delivery manager Download as Open Office doc / Download as MS Word doc Designer Download as Open Office doc / Download as MS Word doc Developer Download as Open Office doc / Download as MS Word doc Digital Communications Lead Download as Open Office doc / Download as MS Word doc Junior visual designer Download as Open Office doc / Download as MS Word doc Performance analyst Download as Open Office doc / Download as MS Word doc Product manager Download as Open Office doc / Download as MS Word doc Portfolio manager Download as Open Office doc / Download as MS Word doc Technical Architect Download as Open Office doc / Download as MS Word doc User researcher Download as Open Office doc / Download as MS Word doc Web ops Download as Open Office doc / Download as MS Word doc Many of the skills now needed in government – both from the leaders of digital and technology teams and the teams themselves – are ones that have rarely been looked for in the past. To help support recruitment teams across government in bringing in those skills GDS has written a set of template job descriptions for SCS and specialist posts. These job descriptions assume that organisational design guidance set out in the manual has broadly been followed. For advice on tailoring these designs to your organisation, contact GDS via the Recruitment Hub. The templates have left gaps for departments to complete. Recruiters will need to add: Before advertising any SCS technology position, the Recruitment Hub must have reviewed and agreed the job description after it has been tailored to a specific organisation and post. GDS does not need to approve non-SCS job descriptions prior to advertisement, though will provide advice if asked to by departments.","description":"Many of the skills now needed in government – both from the leaders of digital and technology teams and the teams themselves – are ones that have rarely been looked for in the past. To help support recruitment teams across government in bringing in those skills GDS has written a set of template job descriptions for SCS and specialist posts.","link":"/service-manual/the-team/recruitment/job-descriptions.html"},{"title":"Digital performance analyst","indexable_content":"Job description Personal specification – competencies and skills required support the service manager to make sure their service meets the performance requirements set out in the Digital by Default Service Standard communicate service performance against key indicators to internal and external stakeholders ensure high-quality analysis of departmental transaction data support the procurement of the necessary digital platforms to support automated and real-time collection and presentation of data share examples of best practice in digital performance management across government identify delivery obstacles to improving transactional performance in \tdepartments and working with teams to overcome those obstacles be alert to emerging issues and trends which might impact or benefit own and team’s work understand how the services, activities and strategies in the area work together to create value for the user make sure own area/team activities are aligned to departmental priorities identifing a range of relevant and credible information sources and recognise the need to collect new data when necessary from internal and external sources recognising patterns and trends in a wide range of evidence and draw important conclusions exploring different options outlining costs, benefits, risks and potential responses to each inviting challenge and where appropriate involve others in decision making to help build engagement and present robust recommendations communicate using appropriate styles, methods and timing, including digital channels, to maximise understanding and impact communicate in a succinct, engaging manner and stand ground when needed convey enthusiasm and energy about their work and encourage others to do the same promote the work of the department and play an active part in supporting the civil service values and culture proactively manage own career and identify own learning needs with line manager, plan and carry out workplace learning opportunities continually seek and act on feedback to evaluate and improve their own and team’s performance make effective use of project management skills and techniques to deliver outcomes, including identifying risks and mitigating actions develop, implement, maintain and review systems and service standards to provide quality, efficiency and value for money establish mechanisms to seek out and respond to feedback from customers about service provided make sure there is efficient and effective use of resources to deliver programmes and projects on time, within budgets and to agreed quality standards regularly monitor own and team’s work against targets act promptly to keep work on track and maintain performance plan ahead but reassess workloads and priorities if situations change or people are facing conflicting demands familiarity with data analysis, web analytics and visualisation tools essential eg Google Analytics, Google Refine, Tableau etc experience of providing performance analysis and recommendations on digital public services Digital Performance Analysts sit at the heart of a team, working to specify, collect and present the key performance data and analysis for their service.  The post holder will be part of a revolution in the way in which government continuously measures, assesses, and improves performance in transacting with the public.  They support service managers by generating new and useful information and translating it into actions that will allow them to iteratively improve their service for users. You will have excellent analytical and problem solving skills will enable you to quickly develop recommendations based on the quantitative and qualitative evidence gathered via web analytics, financial data and user feedback.  You will need to be confident in explaining technical concepts to senior civil servants with limited technological background. You will be comfortable working with data, from gathering and analysis through to design and presentation. Commercial experience of performance management is an advantage.  The main responsibilities of the post are to: You need to be able to see the big picture. You must have these skills: Essential  \t\t\t\t\t You must be able to make effective decisions by:  Essential\t\t\t\t\t\t You need to be a good leader with strong communication skills. You will be able to: Essential You must be able to build capability for all: Essential You must be able to manage a quality service: Essential You will be able to deliver at pace:  Essential Some specialist skills you will have: Essential","description":"Digital Performance Analysts sit at the heart of a team, working to specify, collect and present the key performance data and analysis for their service.  The post holder will be part of a revolution in the way in which government continuously measures, assesses, and improves performance in transacting with the public. ","link":"/service-manual/the-team/recruitment/performance-analyst-jd.html"},{"title":"Rewarding technology leaders","indexable_content":"Senior Civil Service (SCS) roles Non-SCS specialist roles Delivering the business change required to create excellent digital public services and supporting technology systems in departments will be a large and exciting challenge for those who lead it. For that transformation to be successful, we have to make sure that we have ‘best in class’ candidates for technology leadership and specialist positions in post. These roles will demand a combination of highly sought after strategic, delivery and leadership skills, as well as specialist technical knowledge. Appropriately rewarding such staff is essential to their recruitment and retention. Government is not looking for average candidates. We need people with a track record of delivery and an appetite for driving important change in a highly visible job. As such, GDS recommends that departments offer candidates between 75th and 90th percentile of the typical London-weighted salary band for a similar role in the private sector. GDS has conducted independent benchmarking research that samples hundreds of similar roles in order to produce these range figures, and will review the market rate on an ongoing basis to ensure that the offer government makes to technology is highly competitive. For further advice on salary offers, please contact the Recruitment Hub. Template job descriptions for many of these roles are available to download from the manual. The scope, responsibilities and interactions with other senior leaders that the successful candidate takes on will vary across departments, depending on their size, the number of services they are responsible for, and the extent of their existing IT estate. These figures are intended as a guide, and departments should recognise that they may need to offer more than the top of these bands for highly experienced candidates.","description":"Delivering the business change required to create excellent digital public services and supporting technology systems in departments will be a large and exciting challenge for those who lead it. For that transformation to be successful, we have to make sure that we have ‘best in class’ candidates for technology leadership and specialist positions in post. These roles will demand a combination of highly sought after strategic, delivery and leadership skills, as well as specialist technical knowledge. Appropriately rewarding such staff is essential to their recruitment and retention.","link":"/service-manual/the-team/recruitment/salary-advice.html"},{"title":"Technology leadership","indexable_content":"Leadership People Future organisation design: maintaining agility and relevance digital public services mission IT systems infrastructure back office services Government has traditionally treated IT as a single entity with the same approach being used for very different types of service. Government IT is more appropriately mapped into distinct areas with different user needs — some are public facing, and some are for internal use. These areas include: These 4 areas are in very different stages of development. Digital public services, for example, are being built by developers using very new technologies. In contrast, technology infrastructure such as data storage has evolved over the course of decades. Products which 5 or 10 years ago would have required detailed and expensive customisation are now commoditised (packaged as ‘pay-as-you-go’) – to the extent where they have virtually become utility services like gas or electricity (you can move between suppliers easily). By using technology based on commodity products and services government can benefit from economies of scale and much greater efficiency. Improvements to digital and technology leadership will create a new way of working in government, supporting teams across the civil service in producing digital public services and better department-specific IT (or mission IT), and ensuring these teams have the right technology to support them. For this to happen, government digital and technology leadership should be strongly represented at board level and be specifically responsible for enabling business change. In many cases digital services and department-specific IT should – for now – be managed as separate but strongly linked workstreams, with a common aim of ensuring transformation across the department. In all cases where departments run digital services, digital should be represented at the same or higher seniority level as technology. The new roles of the Chief Digital Officer (CDO) and Chief Technology Officer (CTO) are for strategic, transformative leadership. CDOs will concentrate on producing digital public services and the wider digital by default strategy. CTOs will concentrate on providing the department-specific IT that supports digital public services and internal users. CTOs will also be in charge of moving their departments to common technology services. Both CDOs and CTOs will need to work out which areas can be commoditised and which need customised services. Both roles need to be getting the most out of new technologies, and will need to structure their teams accordingly. Models, structures and strategies are irrelevant without the right people in the right roles. The best way to restructure senior technology roles will vary across departments according to how much involvement they have across the 4 areas of government technology. In some cases both the CDO and CTO will sit on the board; in other cases they might report into a board level Chief Operating Officer (COO), or only the CDO might be on the board.  Where digital public services rely on department-specific IT systems to support them, a service manager role should be in place, reporting into the CDO with accountability and decision-making authority for the end-to-end service. In all organisations, roles that manage existing contracts, retiring legacy systems, and providing infrastructure services should be subordinate to leadership roles. GDS will provide support by establishing common infrastructure services so senior leaders in departments can concentrate on providing department specific technology for their users. GDS has also established a Recruitment Hub – an advisory service provided to departments to help them recruit talented technology leaders and digital specialists. As part of the Recruitment Hub service, GDS has already begun to work with departments and agencies to help with organisation design. Contact digitaltalent@digital.cabinet-office.gov.uk for more information. The CTO/CDO organisational model is not designed to provide a fixed structure for decades to come. It’s a model that reflects the current state of government technology with both large legacy estates and new digital services in development, and is designed to support the changes in technology use over the next 3 to 5 years. Over time the distinction between digital public services and department-specific IT will weaken. As legacy systems are retired and more technology is delivered through the cloud, it will make less sense to draw a separation between the two areas. It’s quite possible that in 5 years’ time both citizens and internal users will use the same digital services and technology. Organisation and governance structures – both within and across departments – will need to develop too. It will be part of the responsibility of senior digital and technology leaders to keep the effectiveness of these structures under review to ensure that they remain fit for purpose.","description":"Government has traditionally treated IT as a single entity with the same approach being used for very different types of service. Government IT is more appropriately mapped into distinct areas with different user needs — some are public facing, and some are for internal use. These areas include:","link":"/service-manual/the-team/recruitment/scs-orgdesign.html"},{"title":"Service managers","indexable_content":"Responsibilities Sample job description Induction and development Guidance Further reading What’s the difference between a Service Manager and a Product Manager? be experienced leaders, with an in-depth understanding of their service (built on continuity of involvement over a period of years) and equipped to represent their service and its users’ needs at all levels within the organisation. For high-profile services these will be at Senior Civil Servant level be accountable for the quality and usage of their service, and able to iterate the service based on user feedback at least every month be able to lead effectively on the change management and process re-engineering required to implement successful services have the digital literacy to engage with technical staff and suppliers to define the best system and platform configurations to achieve business/user objectives encourage the maximum possible take-up of their digital service by effective marketing, and specify/manage the requirements for assisted digital activity to supplement this oversee service redesign and subsequent operational delivery; supporting and ensuring the necessary project and approval processes are followed, monitoring and reporting on progress in line with the digital by default service standard, identifying and mitigating risks, and be empowered to deliver on all aspects actively participate in networking with other Service Managers inside and outside government, and share good practice and learning Service managers are individuals who work full-time to develop and deliver all the changes necessary to provide effective user focused digital services. Outside government, organisations in the public and private sector are learning that empowered, experienced and highly skilled managers (often called Product Managers in the commercial world) are necessary to deliver high-quality digital services. We are adopting that model, requiring each transactional digital service handling over 100,000 transactions each year to be developed, operated and continually improved by an experienced, skilled and empowered Service Manager. These are not technical IT posts, nor are they confined to running a website. Instead, they are individuals who work full-time to develop and deliver all the changes necessary to provide effective digital services. With a handful of exceptions, this is a new role within government. These Service Managers will: This will depend on the scale of the service you are working on.  In some cases the Service Manager will also be able to fulfil the role of Product Manager – working closely with the delivery team (the ‘makers’), prioritising stories for each sprint, attending daily stand ups, being on hand to comment on solutions as they emerge, and accepting stories once they are delivered. However, in many cases it is likely that the Service Manager won’t have the capacity to be this hands-on, so they are likely to need a dedicated Product Manager. Click either of the options below to download a template Service Manager job description.  Download as Open Office doc / Download as MS Word doc  Cabinet Office will help departments to recruit suitably skilled individuals through the Recruitment Hub. Newly appointed Service Managers are supported by GDS through a specialist induction and development programme. Read guidance within the manual of particular interest to service managers. Read more about how Service Managers should interact with other technology leaders in our organisation design guidance. Building a team","description":"Service managers are individuals who work full-time to develop and deliver all the changes necessary to provide effective user focused digital services.","link":"/service-manual/the-team/service-manager.html"},{"title":"User research skills","indexable_content":"How user researchers work How to hire researchers Job description templates Further reading Really understanding who your users are, understanding their needs and maintaining a close focus on designing digital services that best meet their needs is an ongoing struggle again our internal processes and domain knowledge. Regular user research helps to make sure that we start and stay aligned with real user needs. User researchers are involved at all phases of the project and work closely and collaboratively with designers and product owners. In the early stages of discovery they assist in developing a clear understanding and empathy for end users, making sense of existing research and commissioning or conducting additional research. Through the Alpha and Beta phases they work closely with the design team to provide guidance based on their understanding of end user needs and behaviour, and working with the team to design methods to answer outstanding questions about the users and the design of the service being created. As a member of the delivery team, you should hire researchers who are comfortable working in an agile team, delivering continuous research in fortnightly iterations. Especially in alpha and beta phases, you will get better results by bringing a researcher into your team rather than outsourcing research to 3rd party. In particular you should look for a researcher who has experience contributing to other disciplines in the team. Capabilities in design or content are particularly helpful. In product development a mixture of experience in both qualitative and quantitative research is useful, although the most essential will be the ability to design and conduct qualitative research. Teams should aim to observe real users interacting with the product they are designing at least every two weeks, most often in the form of one-on-one interviews. When hiring a user researcher there are two main things to consider. Firstly consider whether they have sufficient experience in designing, facilitating and analysing user research, in particular one-on-one interviews and usability testing. A researcher with hours of experience observing end users interacting with digital products can bring value to the design team immediately. Most experienced researchers should be able to provide you with an estimate of their total hours of experience observing users and, on request, provide video clips showing examples of their facilitation techniques for you to review. You are looking for a researcher who has 100+ hours of facilitation experience and who demonstrates an empathetic yet methodical approach to facilitation. Secondly, consider their ability to work well in an agile team. This means that they need to be able to work collaboratively with designers and product owners, to be flexible and responsive in designing research and comfortable with a fast moving work environment where change is constant, and most importantly, they need to have experience and ability in quickly and effectively communicating research insights in an actionable way so that these insights actively shape the design and development of the product. This will usually mean that the researcher has exercised some creativity and experimentation in their method of communicating findings and they should be able to share examples that were more and less successful. See an example of a user researcher job description provided by GDS. Specific guidance for user researchers working on digital by default services. An introduction to user research techniques for each stage of the project.","description":"Really understanding who your users are, understanding their needs and maintaining a close focus on designing digital services that best meet their needs is an ongoing struggle again our internal processes and domain knowledge. Regular user research helps to make sure that we start and stay aligned with real user needs.","link":"/service-manual/the-team/user-researcher.html"},{"title":"Web operations skills","indexable_content":"The importance of web operations Skills Guidance Job Description Further reading work with developers to optimise existing application and to design new ones participate in stand-ups, planning sessions and retrospectives design, build and run systems for application deployment, systems orchestration and configuration management encourage everyone (developers, delivery managers, product owners) to think about how new applications will be run and maintained contribute to designing internal processes needed in the running of a high performance development and operations organisation help everyone understand constrains around security, performance, cost and resulting tradeoffs deep understanding of the target operating system; Windows, Linux, Unix etc. experience of multiple programming languages common deployment patterns continuous integration capacity planning load and performance testing techniques highly available systems design administration and tuning of production database systems. installation and usage of monitoring tools; for instance Nagios, Ganglia, Riemann, Graphite etc. knowledge of configuration, deployment and management of web application stacks configuration management tools like Puppet, Chef, CFEngine EC2 or similar dynamic provisioning compliance, auditing and security open source development experience in a product centric environment Presentation given to GDS about web operations Article explaining the web operations role Book all about web operations topics Web operations (sometimes called systems administrators, operations engineers or site reliability engineers) run the production systems and help the development team build software that is easy to operate, scale and secure. This involves expertise in infrastructure, configuration management, monitoring, deployment and operating systems. Web operations people help run the eventual production systems, but also to help the development team build software that is easy to operate. Thinking about how the eventual system will be run at the very start of the project is important if you want to smoothly move from prototypes to production systems. At a high level they will: With specific skills: And ideally have an interest in or some experience with: Read guidance in the manual of particular interest to web ops. Click either of the options below to download a template Web Ops job description.  Download as Open Office doc / Download as MS Word doc  Cabinet Office will help departments to recruit suitably skilled individuals through the Recruitment Hub.","description":"Web operations (sometimes called systems administrators, operations engineers or site reliability engineers) run the production systems and help the development team build software that is easy to operate, scale and secure.","link":"/service-manual/the-team/web-operations.html"},{"title":"Your working environment","indexable_content":"What you might encounter Hacking the environment Sit together Communication tools Further reading ‘Has anyone else encountered this bug in our search component?’ ‘Is anyone else working on the main dashboard template?’ ‘Does anyone know how to configure this server to support more connections?’    ‘Has anyone else encountered this bug in our search component?’     ‘Is anyone else working on the main dashboard template?’     ‘Does anyone know how to configure this server to support more connections?’    Working spaces for digital projects will vary, but there are some things you can do to ensure that the space you have available can be used in the way your team needs. Those working in creative and technical fields often need plenty of space for focused, detailed work. It’s not uncommon to see people spending a significant chunk of their day with headphones on to help them focus, or locked in conversation with just one person with whom they’re pairing. Equally, you might expect to see lots of short meetings throughout a day, often around walls covered in notes or reference material. These regular exchanges are important to ensure the quality of the work, but can seem strange in office cultures that are much more used to formal meetings or conference calls. The room a team works in is a tool. It is just as important as the choice of project management tools or choice of programming language. Teams should dedicate time at the beginning of a project to making sure they have everything they need and addressing any problems. This might include setting up a project wall, configuring collaboration software like email groups or project trackers, building and installing dashboards, putting up whiteboards, or simply moving desks about so the team can sit together. Removing those dividers between desks makes a big difference and allows conversation between the team to flow more freely. If large monitors are getting in the way remove them. You might also want to think about getting desk tidies like these. The difference a tidy environment makes to ability of a team to think and work is striking. This process is known as ‘hacking the environment’. This might seem like an obvious one, but teams working together to deliver a product should sit close together. Short, informal conversations are an important way to test assumptions, and this gets much harder when a team is distributed across an office, or worse, in different buildings. When working on the design, development and operation of a service it’s essential that your team be able to be in constant contact with one another to make rapid decisions, provide support and information and to ensure everyone’s aware of the project as a whole. Some of that will be achieved through regular short meetings such as a daily standup or weekly ‘show and tell’ session, but there’s also a need for an asynchronous mechanism that’s more immediate and conversational than email but that allows people to dip in and out. Internet Relay Chat (IRC) – and similar tools such as group messengers, Campfire, and so on – operate as software running on the user’s computer as a dedicated application or in a web browser, supporting a constant stream of live conversation. Typically, a user would leave the service running in the background while working on other things, switching focus when a break is needed or when you need to ask a specific question. Questions might be along the lines of: Anyone on the team would be able to respond with information, suggestions of other people to talk to, and so on. Because it is network based it will also work regardless of geography so distributed teams can continue to communicate as if they were in one room. Paul Graham on Makers’ Schedules vs. Managers’ Schedules","description":"Working spaces for digital projects will vary, but there are some things you can do to ensure that the space you have available can be used in the way your team needs.","link":"/service-manual/the-team/working-environment.html"},{"title":"Working with specialist suppliers","indexable_content":"Identifying the skills you need Individuals Companies Choosing your supplier People, not process Guidance for a smooth induction Working together Knowledge transfer Further reading delivery managers designers developers web ops content designer a proven track record of Agile delivery how they will use continuous delivery methods how they will share knowledge and provide coaching and mentoring to civil servants who are developing and/or will be responsible for the service how they will deliver in an Agile way; possibly while working with departments who work in waterfall or PRINCE delivery method their commitment to work on-site with civil servants to share knowledge how they will ‘bake in’ quality to the process make sure you have enough budget speak to your HR team and confirm you can negotiate salary etc, particularly for technical roles shield your people from the form-filling culture that HR sometimes brings contractors shouldn’t work from home all the time (be sensible here, the odd day won’t hurt) monitoring the work through project tracking tools, but most importantly talk to your people and understand the work they’re doing You may need specialist help to design, develop, build or improve your service. When buying in these services, there’s support GDS may be able to provide. When you have an idea of the service you are going to create, talk to your team about the skills that are lacking. This is your chance to develop skills in-house, so before you go outside the civil service, first see if you have team members who can be developed into roles. If not, you can hire individuals or companies to fill in the gaps of specialist skills. GDS can help you hire some roles. We have job descriptions for: Other descriptions will be made available. Please contact the team if you have particular descriptions in mind. GDS can give you some advice on certain candidates or interviews and setting contracts. If you need part of a team or a complete team, remember: you are buying skills to help your team or to build your team around. You are not locking yourselves into a long contract, with minimum spend criteria etc. When you are considering an individual or company to work with, they should give you evidence of: They should also be aware of the departmental digital strategies and how they will need to work with them. GDS can give you advice on buying this sort of capability through existing processes like G-Cloud, Spot-buy etc. Later in 2013, you will be able to use the Digital Procurement Framework. An underlying principle behind Agile development is that people are more important than process. It has been proven that great teams make great products whether they’re in government or business. At GDS, we don’t call people ‘resources’, whether permanent or contract. You look after people, resources are things you use (and discard). Some guiding principles for service managers or delivery managers: And don’t forget, contractors can’t sign things. Suppliers and permanent staff should sit together, work together and work through problems together. The team should create an environment where difficult questions can be asked and all ideas are listened to. All team members should work to do the right thing for the user, not just what they have been asked to do. Both suppliers and permanent staff should suggest ideas and work together to identify further skills gaps and solutions to create a better product. The individual’s or company’s intention should always be to disengage from the department, leaving the team to function on its own. Your team structure and processes should centre around transferring knowledge to permanent staff, wherever possible. Don’t let a contractor become a single point of failure and treat all team members (permanent and contractors) the same. A blog post by Meri Williams of GDS on people management in an agile setting","description":"You may need specialist help to design, develop, build or improve your service. When buying in these services, there’s support GDS may be able to provide.","link":"/service-manual/the-team/working-with-specialists.html"},{"title":"Accessibility","indexable_content":"Accessibility standard Accessibility testing Accessibility statements and policies Assistive technologies Accessible formats Accessible content Further reading JAWS, NVDA VoiceOver for OS X Window Eyes and Supernova ZoomText MAGic is there disabled parking?  how far is it from the entrance?  what’s the terrain like? Uphill? Downhill?  will I have to cross any roads?  The services we provide are for the benefit of all citizens of the United Kingdom. No user should be excluded on the basis of disability. To do so would breach the Equality Act 2010. Your services must also comply with any other legal requirements, including providing services in accordance with your Welsh Language Scheme, if you have one. As a starting point, your service should aim to meet Level AA of the Web Content Accessibility Guidelines (WCAG) 2.0. Your service should be tested by disabled people, older people, and people who use assistive technologies. You should aim to do this at least twice as your service is developed. Find out more about how to conduct accessibility testing We work hard to make our sites and services as accessible and usable as we can for everyone who needs to use them. The GOV.UK website doesn’t include a separate accessibility statement, however, as we aren’t comfortable with a statement that draws a distinction between accessibility and any other aspect of best practice development. This blog post by GDS accessibility expert Léonie Watson explains in more detail why we took this decision. Your service should be usable by recent versions of these screen readers: Your service should also be usable by basic operating system screen magnifiers like: Your service should be usable by speech recognition software including Dragon Naturally Speaking, and native operating system speech packages. HTML is quicker, easier and more widely usable/accessible than PDF, but where no other option is possible this PDF guidance should be followed. There’s more information on choosing appropriate formats. Accessibility is more than checking the boxes of standards compliance. When writing content, consider what information would be useful to people with access needs. For instance, in a ‘find my nearest’ service, consider user needs like:  You can read more about the accessibility testing we’ve carried out while building GOV.UK on this blog post by GDS Accessibility Lead Joshua Marshall.","description":"The services we provide are for the benefit of all citizens of the United Kingdom. No user should be excluded on the basis of disability. To do so would breach the Equality Act 2010. Your services must also comply with any other legal requirements, including providing services in accordance with your Welsh Language Scheme, if you have one.","link":"/service-manual/user-centred-design/accessibility.html"},{"title":"Browsers and devices","indexable_content":"Verified browsers Developing universally accessible services Continuous compatibility Further reading Desktop Small screen devices Audience operating system browser browser version screen size/resolution mobile device Services should be universally accessible, regardless of how the user is choosing to access them. Due to the large range of browsers, devices and screen sizes available, users’ experience of your service will vary according to the technical capabilities of their browsers and devices. You must verify that your service works across a representative range of these browsers and devices. Creating your service with web standards in mind will give it the best possible chance of working across all devices. These are the browsers we recommend testing on when developing your service.  This list strongly recommends testing on a range of browsers created within the last 3 years that cover the largest representation of the user base. The list is based upon usage statistics for GOV.UK and represents approximately 95% of the most popular browsers (the remaining browsers are individually insignificant). Browsers not listed may still work well, and it should be noted that this is not a list that intends to suggest that these are the only browsers the service will work on – this is simply a benchmark for testing against to ensure that the service will likely work for as many users as possible alongside appropriate cost-effectiveness and development overhead. Services should ensure there’s an obvious way for users to report problems they may find, and additional testing and adjustments should be made upon receiving such a report. Note: An exception is made for IE6, as this is still in large-scale use in government departments. Two distinct levels of support are given and denoted next to each browser. Where ‘latest version’ is listed, it means the latest stable version plus one version back, as these browsers regularly update without intervention from the user. Digital by default services must take into consideration the limitations of the browsers people use to access them. One important idea for achieving this is progressive enhancement. This recognises that different bits of technology have different capabilities. While everybody gets access to core functionality, those using more sophisticated technology get an enhanced experience. Progressive enhancement is also important in providing a consistent experience to people using mobile devices or those who may have limited bandwidth. Because mobile traffic now accounts for 17% of all internet use in the UK over the last year and 20% of traffic to GOV.UK, this mode of access is not an optional extra to consider. Where we might previously have developed separate mobile and desktop versions of a service, or bought bespoke apps, design should now be done with one website in mind. This should be done using a responsive design approach. This means websites adapt to suit the dimension of the screen being used to view it. Don’t try to build services for every possible combination of operating system and browser. Avoid the temptation of designing for the obvious without first researching your users. Every service has an audience and you should investigate yours to see whether it has particular characteristics that you need to be aware of. Do you have existing data for the browsers and devices that your audience has been using already? If so, analyse it to see if you can identify any patterns in usage, or any combinations of: This data may sometimes support the case for deprioritising certain development work. Although most of GOV.UK is designed to work across all screen sizes, the Trade Tariff team chose not to tailor their tool to the smaller screen as it is largely used by office workers working during office hours. Equally, if your audience is likely to include those working in the public sector, there may be higher use of older, more limited browsers. Channel shift means you must also consider your potential future audience. Device data for GOV.UK is displayed on the Performance Platform and this will be expanded to include operating system and browser data. Before the launch of GOV.UK we noted a marked difference between the existing non-government and government audiences so you should also investigate the data provided by NetMarketShare and GlobalStats who can provide UK and global trends. It’s important to distinguish between those browsers and operating systems whose popularity is either increasing or holding steady and those for which the opposite is true. Although Internet Explorer versions 6 and 7 are only used by a minority (2.5% of visits to GOV.UK in the last year) this still accounts for a significant number of individuals that government services must take into account. However, over time this will change. So it’s important to set thresholds for abandoning support and for adopting new and emergent platforms. The iPad Mini, Kindle Fire, Windows 8 and Internet Explorer 10 highlight this dilemma – recently launched products might not appear in any data but it is likely that they’ll eventually enjoy widespread use. Decisions about compatibility can’t be something you specify at the start of the project and then forget about. Transformed digital services need to reflect and adapt to the broader internet context of their users on an ongoing basis. James Weiner writes about the decisions made about browser support for the Beta of GOV.UK (January 2012). Ben Welby discusses the operating systems, browsers and devices supported for the launch of GOV.UK (October 2012). Tom Byers explores the practical ways in which GOV.UK has been designed for different devices (November 2012). Dafydd Vaughan with an update on browser usage on GOV.UK post-launch. The Guardian introduce their use of responsive design (October 2012).","description":"Services should be universally accessible, regardless of how the user is choosing to access them.","link":"/service-manual/user-centred-design/browsers-and-devices.html"},{"title":"Card sorting","indexable_content":"How to carry out a card sort Where/how you might use it Weaknesses/when not to use Online tools Closed sorting Card sorting is a research method used to understand the way that the intended users of a website naturally organise or think about different types of information or content. It’s also a method service teams can use to sort and arrange user needs. The method is very simple to use: it involves simply writing the needs or topics on pieces of card (maximum of around 50) and asking representative users to organise these into groups, which are then given a meaningful label. 15 to 30 users are recommended: a minimum of 15 users will give reasonable confidence in the results, but any more than 30 gives diminishing returns. It is also possible to conduct a closed card sort, whereby the users are given pre-determined categories and are asked to allocate each card to one of these categories. Open card sorts are most suitable for identifying how users think about information, in order to build an information architecture. Closed card sorts are most suitable for validating an information architecture. The analysis of card sorting can be difficult if no clear categories emerge. It can also be difficult to conduct a card sort on a very large website with a broad scope of content (eg GOV.UK) since the maximum number of cards may not be representative of all of the content. Card sorts can also be conducted using an online tool such as OptimalSort. This is a cheap and effective way of reaching a large sample of users without needing to bring them into a dedicated lab facility for the study.","description":"Card sorting is a research method used to understand the way that the intended users of a website naturally organise or think about different types of information or content. It’s also a method service teams can use to sort and arrange user needs.","link":"/service-manual/user-centred-design/card-sorting.html"},{"title":"Choosing appropriate formats","indexable_content":"Choose the format to fit the content Don’t assume your users can read proprietary formats      For written reports, the native format of the web (HTML) should be your default option. PDF can be an excellent display format, but without additional effort it can be inappropriate for users of screenreading software. It’s faster and easier to make accessible HTML that’s suitable for every platform and device. If you must publish PDFs, you should provide accessible alternate formats for the document, and invest effort in accessibility tagging your PDFs.         For data, use CSV or a similar ‘structured data’ format (see also JSON and XML). Don’t publish structured data in unstructured formats such as PDF.         If you’re regularly publishing data (financial reports, statistical data etc.) then your users may well wish to process this data programmatically, and it becomes especially important that your data is ‘machine-readable’. PDFs, Word documents and the like are not suitable formats for data publication. In addition, you should consider making your data available through an API (application programming interface) if this will simplify your users’ interactions with your publications. For more information on APIs, and for more detailed technical guides on publishing data, please see our guidance on APIs and formats.         If you’re publishing a written report that contains statistical tables, provide the tables alongside or in addition to your report in suitable data formats.         For textual reports, provide HTML, plain text (.txt), or PDF rather than formats that require proprietary software to view, such as Word documents (.doc/.docx).         For tabular data, provide CSV or TSV rather than Excel spreadsheets (.xls/.xlsx).         For other structured data, see our guidance on representations for the consumer. Wherever possible, choose an open format over a proprietary one.    Almost all content relating to the policies or publications of government departments should live on GOV.UK. Where exceptions to this rule are required, content and data should be provided in formats that appropriately reflect their purpose and intended audience. You should publish documents in file formats that reflect the nature of the information they contain, and the uses to which they will likely be put. For written reports, the native format of the web (HTML) should be your default option. PDF can be an excellent display format, but without additional effort it can be inappropriate for users of screenreading software. It’s faster and easier to make accessible HTML that’s suitable for every platform and device. If you must publish PDFs, you should provide accessible alternate formats for the document, and invest effort in accessibility tagging your PDFs. For data, use CSV or a similar ‘structured data’ format (see also JSON and XML). Don’t publish structured data in unstructured formats such as PDF. If you’re regularly publishing data (financial reports, statistical data etc.) then your users may well wish to process this data programmatically, and it becomes especially important that your data is ‘machine-readable’. PDFs, Word documents and the like are not suitable formats for data publication. In addition, you should consider making your data available through an API (application programming interface) if this will simplify your users’ interactions with your publications. For more information on APIs, and for more detailed technical guides on publishing data, please see our guidance on APIs and formats. If you’re publishing a written report that contains statistical tables, provide the tables alongside or in addition to your report in suitable data formats. In summary, consider your users, and the uses to which they’ll put your published data and content. If in doubt, treat the native format of the web, HTML, as a good fallback option. Web browsers are available on all platforms and devices, and web pages tend to be both passably accessible and machine-readable. Wherever possible, publish in accessible, patent-free, open formats, for which software is widely available on a variety of platforms. If publishing in proprietary formats, you should always make a non-proprietary alternative available. For textual reports, provide HTML, plain text (.txt), or PDF rather than formats that require proprietary software to view, such as Word documents (.doc/.docx). For tabular data, provide CSV or TSV rather than Excel spreadsheets (.xls/.xlsx). For other structured data, see our guidance on representations for the consumer. Wherever possible, choose an open format over a proprietary one. Again, if in doubt, you should treat the native format of the web, HTML, as your best default option.","description":"Almost all content relating to the policies or publications of government departments should live on GOV.UK. Where exceptions to this rule are required, content and data should be provided in formats that appropriately reflect their purpose and intended audience.","link":"/service-manual/user-centred-design/choosing-appropriate-formats.html"},{"title":"Data visualisation","indexable_content":"Introduction Telling the story Choosing your visualisation and templates Creating your visualisation Be open and honest Further reading Best practice GDS example Checklist Column chart Bar chart Line chart Pie chart Scatter chart Checklist Worked example Checklist Worked example: GDS performance dashboard Checklist Worked example Before After Before After Who are your audience? How much detail do they need? What story does the data tell? Do you need a visualisation? Negative values below the x-axis. If needed, the target should be a single line that is visible but not too thick. Limit stacked columns to 3 segments. Arrange bars in size order, from biggest to smallest (unless there’s good reason, ie data needs to be represented alphabetically). Negative values to the left of the y axis. Limit number of data sets to 4. Keep axis labels horizontal. Use line points to differentiate between data types (use line dots for projections and estimates only). The largest segment should be at 12 o’clock going clockwise. Label the chart directly and avoid text inside segments. Limit items to two to avoid confusion. Include trend line if required. This should be a single solid line. What visualisations are available? Have you chosen the right visualisation for the data? What about infographics? Too many segments make them hard to compare. Too many colours made the chart confusing. The chart includes cost recovery as a negative which cannot be represented properly in a pie. This stacked chart is much clearer. Comparisons can easily be made and sorting the data provides quick insight. The stacked chart provides additional information which could not be visualised in the pie. start axes at zero unless there’s good reason not to (ie data is clustered at high values) don’t say too much, limit the number of data sets if needed, put legend at the top of the chart in the same order as the data in the chart maximise the space available to the chart and remove chartjunk include units of measurement in the chart title or directly on the axis, avoid doing both keep colours simple, do not repeat/alternate or use opposites - use the GDS Colour palette use the same colour when reporting a single data set 3D effects borders unnecessary axes lines random colours or backgrounds unnecessary text Have you removed the chartjunk? Have you got the right amount of supporting information? Have you got the right amount of data? Have you used the GDS colour palette? Will the colours work for people with colour blindness or greyscale? Are your colours appropriate for the data? Have you referenced data with a URL? Have you provided contact details (eg a mailto link)? Is it clear whether data is internal or public? Have you been open and transparent with data? Chart only shows a select few data points The small range on y-axis has exaggerated the fluctuations in data Y-axis starts at 60 but chart suggests data has reached lowest possible value Y-axis has been formatted to show the full range of data The data fluctuations can be seen in the context of the wider data series.    “We want transparency to become an absolutely core part of every bit of government business.” - Francis Maude  As we surface more data about government services, we need to make sure that the visualisations of it are easy to understand, visually compelling and prompt action. To do that, we need to have a consistent visual grammar, for use both within GDS and across government. This guide sets out 4 principles of good data presentation, with easy to follow checklists to help you achieve this. For context, we’ve added examples of how the principles have been employed at GDS. The principles and examples found in this guide are likely to evolve as we find new challenges and applications for them. There are many examples of best practice style guides already in place. For example, The Economist has a clearly defined house style that allows its readers to readily identify and understand their visualisations. They publish a new visualisation every day in their Graphic Detail. This guide attempts to build on the best practice from a range of organisations. The GOV.UK Performance Platform helps the government make decisions based on data, often presented through innovative visualisations (built using D3.js). The example below compares weekly visitors to GOV.UK with the two main websites it replaced.  To effectively tell the story behind the numbers, you need to understand both your audience and the data. Only use visualisations if they make the story clearer. In many cases, a good table or words may communicate better than a visualisation. If there are very few data points (eg top rate income tax down 5%, all other rates unchanged), it’s clearer to write a sentence than draw a picture. If every item must be read precisely (to several decimal places) then a table is best. A good table will be clear and uncluttered. The data should be easy to read with the same decimal places or rounded and sorted into a logical order. Don’t use too many different types of font, and make sure your data is referenced. But visualisations often are the right answer and the data is the most important feature. It should tell its own story and it’s best to not try to say too much in one go. Keep charts simple, cutting down on unnecessary items and jargon.  Explanatory text will be needed in some cases, but it should not simply repeat the story being told in the visualisation. A well written chart title can reinforce the story of the data and reduce the amount of additional text needed. Choosing the right visualisation will help the data tell its own story and give powerful insight. There are many ways of displaying information visually.  Most computer programmes come with a range of visualisations. There also visualisation tools available online: for example, this blog showcases some free ones and GDS has produced a guide to infographics Each chart has its own strength. Below are the core 5 with templates (a Google spreadsheet of these is available):  Strengths - comparing items, or a small number of time periods.  Strengths - comparing items, especially if they have long names or many items.  Strengths - comparing over time or between variables for a single item (eg site traffic vs site performance)  Strengths - simple share of total. Use with caution as column or bar charts are often better. Limit to 2 segments to avoid confusion.  Strengths - relationships between variables where there are many items (eg volume vs cost for numerous transactions) There is more help on choosing the right chart here. It is important to not confuse your audience. Choosing the correct visualisation is important and at GDS we reviewed what was being used in the performance dashboard. As the example below shows, Pie charts with many items are not clear. We used a stacked bar chart to better represent the data.   Keep in mind these useful tips when creating your charts: Keeping your visualisation simple will help the data tell its own story. Chartjunk is anything in your visualisation using ink that actively reduces clarity. Avoid: Know your audience so you give the right amount of supporting information. External or non-technical audiences will need more explanation but internal or expert audiences may find this tedious. Don’t use the text to simply repeat what’s being said by the data. Visualisations should avoid too much data. Only include what’s relevant. If the trend is obvious, don’t include a trendline. Sometimes it may be more effective to focus on high-value items only (if you’re being selective, be open and clear about this). Poor colour choice can change how the data is perceived in a visualisation. For example, red is strongly associated with negative performance so is best avoided for positive/normal figures. Colour blindness makes it difficult for a user to differentiate between data sets. Labelling charts directly and different line styles can help. If your visualisations are likely to be printed it’s important the colours work in greyscale as not all users will have high quality printers. The example below from the GDS senior management dashboard shows how avoiding chart junk and limiting the number of datasets can enhance your visualisation.  The legend accounts for a quarter of display space. The Y axis quotes £ and not £m. The segments are profiles and proportionate for each time period, so the bright colouring adds no extra detail. The mix of bar and line is confusing with so much information in the chart.  The stacked column gave a level of detail which wasn’t needed. This has been rationalised to best suit the audience. Axes have been standardised. The legend has been relocated giving the chart more space. Heavy grid lines and axes have been removed to give a clearer display. “We want transparency to become an absolutely core part of every bit of government business.” - Francis Maude Being open and transparent supports the Open Data White Paper. Similarly, our Open Public Services agenda is built on transparency. Sourcing data builds trust and credibility. Providing contact information shows ownership but also helps collaboration and information sharing. When presenting data be aware of axes and scales. Data can be misrepresented by only showing a selection if it isn’t clear why an extract has been chosen. Consider where the visualisation might be published. For example, if published alongside other visualisations, the reader is likely to assume the scales are consistent. This might change how your data is perceived.   More information about the Performance Platform. This chart chooser from Andrew Abela builds on the work of Gene Zelazny’s classic book Saying it with Charts. This interactive tool from Juice Analytics helps guide your chart choice through filters. Brain Suda’s A Practical Guide to Designing with Data provides a comprehensive understanding of how to best engage the audience with your data. Here is a video of Brian Suda presenting on a section of his book at the 2012 DIBI conference. Dona M. Wong’s The Wall Street Journal, Guide to Information Graphics details the do’s and don’ts of presenting data. Edward Tufte’s The Visual Display of Quantitative Information is a seminal work on data visualisations and introduces the concept of chartjunk. Here is a video of Edward Tufte discussing his theories on visual thinking and analytical design. This article from the Peltier Tech blog covers the ten chart design principles. The Flowing Data blog is a source of data visualisation news.","description":"As we surface more data about government services, we need to make sure that the visualisations of it are easy to understand, visually compelling and prompt action. To do that, we need to have a consistent visual grammar, for use both within GDS and across government.","link":"/service-manual/user-centred-design/data-visualisation.html"},{"title":"Designing transactions","indexable_content":"Transactions Choosing the right interface How to structure the transaction Example structures Saving progress Indicating mandatory fields Helping users A few principles Resources Option 1: Single page Option2 : Wizard Option 3: Accordion form Option 4: Hybrid 1. Be honest about the proposition 2. Respect the natural flow of the underlying process 3. Consider all aspects of the user experience 4. Assume users have no prior domain knowledge 5. Don’t ask for information you don’t need The good The bad The good The bad The good The bad will your users want to move through the transaction in a fixed order, or one of their choosing? will they be able to complete the transaction in a single go? will their answers affect other parts of the transaction? will they want to go back and review or change answers to previous questions? will they need to add or remove items from a list, or change the order of things? how many parties are involved in the transaction? do any parts of the transaction take place offline? at what point is the transaction regarded as complete? There’s only one submit button to press A single URL gives access to all form fields It doesn’t force a fixed order of completion You benefit from context of neighbouring sections Progress is self-evident Long forms can be overwhelming and off-putting It’s less well suited to branching or non-linear flow How do you save partial progress? Can be harder to track analytics like drop-off rates It’s easier to handle branching and dependencies between sections It’s easier to let the user save progress A long transaction can feel more manageable Easier to guide a user through an unfamiliar process Easier to capture analytics like drop-off rates for each section Can be harder for users to see where they are within the transaction It can slow users down as they have to click and load each section You lose the contextual information from neighbouring sections Harder for users to review and edit previous sections There’s no single place for users to go back and edit their data Not a natural fit for non-linear processes like looping, adding and removing Can handle branching and dependencies between sections Can easily review and edit previous questions Can help guide a user through an unfamiliar process User still benefits from some surrounding context Progress is clear Implementation and interface is more complex Will future questions be shown in any way or will you only see the questions you’ve answered? What happens if you go back and edit a previous question?     Does the current question stay open or closed?       How do you get back to the current question once you’ve edited a previous one?       Do you lose all your answers to questions that follow the one you go back to edit?      Does the current question stay open or closed? How do you get back to the current question once you’ve edited a previous one? Do you lose all your answers to questions that follow the one you go back to edit? costs involved waiting periods or delays uncommon or hard-to-find information they’ll need to provide constraints on who can complete the transaction (age, nationality etc) equipment that will be required (eg a printer) non-digital parts of the transaction requires additional physical and cognitive effort on their part creates another opportunity for them to get something ‘wrong’ increases the time taken to complete the transaction increases the perception that the service is invasive Transactions Explorer http://www.uxforthemasses.com/forms-usability/ A transaction is an exchange between 2 or more parties. Government transactions typically involve an exchange of information, money, rights, goods or some combination of these. Usually 1 party will be a citizen or business and the other will be the government. For example, when someone applies for a passport they exchange information and money for the right to travel.  A service is made up of a collection of related or connected transactions. Most digital transactions take the form of a dialogue between the user and a system, which acts on behalf of the other party. It’s the system’s interface which determines in large part the quality of the overall user experience. The dialogue between a user and a system can be realised in different ways. It might resemble a verbal conversation, with the system asking questions and the user answering them. It might be more visual, with the user querying the system by interacting with a map or diagram.  Either way, you’ll need to decide what kind of interface best fits your transaction and how to structure it. It can help people understand how to use your service if you adopt an appropriate design metaphor for your interface. Interface metaphors can come from the physical or digital world and include things like address book, library, trash can, email inbox, timeline, bank statement, pinboard etc. Avoid the temptation to go over the top with any physical metaphors (we don’t want to see any leatherbound address books with bookmarks sticking out of them). You should choose a structure for your transaction that most naturally fits the ways your users are going to want to use it. Ask yourself: How you answer these questions will help you decide how to structure the transaction. It can help to think in terms of levels: sections, subsections, groups etc. Try not to worry about how those levels should be represented in the interface until you have a broader understanding of the overall structure. For example, on a website, you might choose to have 1 section per page, or multiple sections per page. It depends on what’s going to work best for your users. All sections are positioned on a single page.  Each section goes on its own page.  All sections on a single page, but each new section only appears once the previous section has been completed.  Done well, option 3 is a hybrid of the other two that has benefits of both the other options. Within this hybrid option there are still some important design decisions to make. For example: For more complicated transactions, some combination of the other options might be your best bet.  Again, done well this can give you the benefits of both the single page and wizard approaches. It also allows you to create a sense of rhythm to the overall flow, which can help people to understand when they have moved into a different part of the transaction, and break up the monotony of filling in forms. As always, these design decisions must have a strong, user-centred rationale behind them. If the average time to complete a transaction is more than you can reasonably expect your users to spend in a single session, then you’ll need to provide a way for them to save their progress. The same goes if the session is likely to be interrupted for some reason. For example, if the user is suddenly asked for information which they might not have immediately to hand (a way to mitigate this is to warn users if they’re going to be asked for that kind of information). Saving progress does not necessarily mean you require user accounts, logins, email validation etc. For simpler transactions that don’t store personal information you might be able to store the data in the URL itself. The user then simply has to bookmark that URL. For more complex transactions that don’t store personal data, you might be able to offer users a unique and hard-to-guess URL that they can use to get back to their session. If you follow the principle of not asking for information you don’t need, then most of your fields will be mandatory and you shouldn’t need to mark them as such with ‘*’ symbols or other conventions. Instead, mark the optional fields, by adding ‘(optional)’ to their label. You should be aiming for a service that’s so intuitive people don’t need any help in using it. If you find yourself explaining the interface within the interface it’s a sign that something has gone wrong. Time to try out some different ideas. These principles may help: Sometimes though, people are going to need extra help. Many government transactions involve concepts and terminology that people will be unfamiliar with. Some of this stuff takes a lot of explaining, so we need a way of providing contextual help of varying degrees of detail throughout a transaction. Here are 3 approaches: Here are a few principles to keep in mind when you’re designing transactions. The further in to a transaction someone gets, the more time they have invested in it and the greater their annoyance if they have to abandon it for some reason. Be honest about what you’re offering users, and what you’re not offering them. In particular, people need to know up front about any: Use common sense though. Don’t try to make everyone read a page of terms and conditions before they start (they won’t). The best approach is to meet (or exceed) people’s expectations. For example, if your delivery times are typical and you accept all the usual payment methods then you won’t need to warn everyone about them up front. For example, if the underlying process is inherently non-linear then choose an interface that works well with non-linear processes. If you try to fit a process into a model it’s unsuited for you’ll confuse your users and confound their expectations. Remembering the broader context in which a transaction exists, including the emotional states of the people involved and the  broader situations or activities that the transaction is embedded in. Don’t assume that your users already understand all the concepts and terminology used in your transaction. A few people will want to read up on all that stuff before they start, but many will choose to dive straight in. You need to support both types of people. Every request for information from the user: Asking for information because ‘it might be useful’ or ‘it helps with our record keeping’ is not acceptable.","description":"A transaction is an exchange between 2 or more parties.","link":"/service-manual/user-centred-design/designing-transactions.html"},{"title":"How users read","indexable_content":"Reading Reading age Lower case Plain English Context Learning disabilities Why we do this Further reading The style guide is set in best practice and relates to how users read. This is an explanation of some of our guidance and the reasons behind the rules. Users only really read 20 to 28% of a web page. Where users just want to complete a service as quickly as possible, there’s added user impatience so you may find users skim words even more. The content style guide and transactions style guide give guidance on how to write. This page details why we do it. All of this guidance is based on the learning skills of an average person in the UK, who has English as a first language. You don’t read one word at a time. You bounce around. You anticipate words and fill them in. By the time you’re 9 years old, your brain can drop up to 30% of the text and still understand. Your vocabulary will grow but this reading skill stays with you as an adult. You should also be confident in sounding out words and blending sounds. You may not know the word, but you have the skills to be able to learn it. This is why we talk about the reading age being around 9 years old. When you learn to read, you start with a mix of upper and lower case but you don’t start understanding upper case until you’re around 6 years old. At first, you may sound out letters, merge sounds, merge letters, learn the word. Then you stop reading it. At that point, you recognise the shape of the word. This speeds up comprehension and speed of reading. So we don’t want people to read. We want people to recognise the ‘shape’ of the word and understand. It’s a lot faster. Capital letters are reputed to be 13 to 18% harder for users to read. So we try to avoid them. Also, in modern usage it sounds like we’re shouting. We are government. We should not be shouting. By the time you are 9, you’re building up your ‘common words’. Your primary set is around 5,000 words in your vocabulary; your secondary set is around 10,000 words. These are words you use every day. They include a lot of plain English words, which is why we should be obsessed with them. These are words so easy to comprehend, you learn to read them quickly and then you stop reading and start recognising. We explain all unusual terms on GOV.UK. This is because you can understand 6-letter words as easily as 2-letter words – if they’re in context. Sometimes, you can read a short word faster than a single letter – if the context is correct. Not only are we giving users full information, we’re speeding up their reading time. By giving full context and using common words, we’re allowing them to understand in the fastest possible way. In tools and transactions you need to give people context. By giving them information they’re expecting, you help them get through it faster. We should remember that people with some learning disabilities read letter for letter. They don’t bounce around like other users. They also can’t fully understand a sentence if it’s too long. People with moderate learning disabilities can understand sentences of 5 to 8 words without difficulty. By concentrating on common words we can help all users understand sentences of around 25 words. Our audience is potentially anyone living in the UK. We need to be able to communicate in a way that most users understand. Government can’t afford to be elitist and use language only specialists can understand. We need to open up our information and services to everyone. That means using common words and working with natural reading behaviour. Nielsen: For more detail on why 20 to 28% of text is read.","description":"The style guide is set in best practice and relates to how users read. This is an explanation of some of our guidance and the reasons behind the rules.","link":"/service-manual/user-centred-design/how-users-read.html"},{"title":"An introduction to user research techniques","indexable_content":"Product research Strategic research Research in the product’s lifecycle Assisted digital research Research methods Background Qualitative user research techniques Quantitative can’t ever use the digital service independently and will always need assisted digital support could use the digital service independently but will require initial assisted digital support to build their confidence in using the service should use the digital service (ie have the digital skills but currently use other channels) and don’t need assisted digital support sampling methodologies user research briefs survey design user research tools ethnographic research expert review guerrilla testing same day user testing lab-based user testing focus groups and one-to-one interviews community user groups user research surveys remote usability/summative testing online research panels online omnibus surveying This guidance provides a broad overview of the methods and techniques available to conduct user research. More detailed guidance on each of these techniques can be found in the links below. User research can be categorised into 2 broad themes – product research and strategic research. Product research can incorporate both qualitative and quantitative techniques. Qualitative techniques are intensive and often small scale. These include focus groups and one-to-one interviews, and are typically used to explore and analyse unstructured data. Quantitative techniques involve higher-volume research, and include online surveys, face-to-face interviews, and involve a structured approach to data collection and analysis. Product-based research can be conducted in-house or via a specialist agency. In-house approaches can be quicker as they involve less lead time (eg procurement), but require skilled in-house researchers, and can involve the procurement of some specialist software.  Typically agencies are only used when specialist skills/experience is required that is not available in-house, and/or the scale of the project would mean it is difficult to provide dedicated internal resource. Strategic research also uses quantitative and qualitative methods, and is used to help understand the appetite for a product or service – typically looking at the size of the market, trends, type of users etc. Secondary data, also known as desk research, can be used, and involves the analysis of existing research or information sources. Ideally, secondary research should be conducted before embarking on any research project to understand what is already known, and what research is required to fill the knowledge gaps. This can range from published research, news article, or internal research that has been conducted previously. The table below illustrates the typical product lifecycle, showing the stages at which user research should be conducted. This chart can be used as a reference to ensure that user needs are being factored into design at every stage. Typically this will involve some initial fact finding in the early stages to understand the user needs, including who they are, how they currently do things, how they’d like to do them, and what information is currently available. Each service/product will have a different proportion of users who are not online and this will need to be considered when formulating an appropriate research approach for assisted digital users. The services/product will need to consider the digital skills of their users to understand which users who currently don’t use digital channels: Some services, eg where users are large corporations, will not require assisted digital. The Digital Landscape Research contains a demographic breakdown of who is offline and online and service teams and these techniques may be useful for doing user research.","description":"This guidance provides a broad overview of the methods and techniques available to conduct user research. More detailed guidance on each of these techniques can be found in the links below.","link":"/service-manual/user-centred-design/introduction-to-user-research.html"},{"title":"Know your users","indexable_content":"User needs An introduction to user research User research techniques User research briefs Card sorting Discussion guides Multivariate testing User research surveys Sentiment analysis Sampling methodologies Survey design User research tools Ethnographic research Expert review Guerrilla testing Same day user testing Lab based user testing Focus groups and 1:1 interviews Community user groups Remote usability/summative testing Online research panels Online omnibus surveying User needs are at the heart of all digital by default services, and to meet them you will need to test your services and assumptions with real users. The most important step for a team is to identify the needs of its users. These form the basis for the development of a service, and is the first principle for building a digital by default service. User needs You will be using research to test your assumptions throughout the design, build and operation of a service. This introduction sets out a few of the basics of user research. Learn more about user research These techniques may be useful when conducting user research As you conduct or procure specific rounds of user research you will want to ensure these are conducted in a consistent way, delivering results in a useful format. Learn how to write user research briefs","description":"User needs are at the heart of all digital by default services, and to meet them you will need to test your services and assumptions with real users.","link":"/service-manual/user-centred-design/know-your-users.html"},{"title":"Print forms","indexable_content":"Don’t make paper forms unless you have to Do as much as possible online Start from scratch The context of a form may be other forms (not the web) Digital to paper and back to digital again Test your forms with real users If you have to choose between hiring a writer and a designer, choose a writer Forms are not an alternative channel Templates and examples Download template form getting rid of any duplication pre-populating forms with information entered online making sure a user only prints the forms they require eliminating unnecessary steps and fields (if a field is marked as optional, consider whether you need to collect that information at all) numbering the forms so it was clear in which order they needed to be completed clearly indicating when questions are optional (‘skip this question if…’) improving the forms in response to user testing giving an overview of the application process at the start labelling pages and continuation sheets clearly explaining jargon inline add familiarity by showing some of the information the user has just entered in large font – their name, for example create a transcript of the information entered using the digital tool, with a signature box at the bottom. if your digital output has to match an existing paper form, you can still add a cover sheet with user’s input (for familiarity) clearly explain the next step: for example, give the address to send the form to, or give a web address to continue using the digital tool; if you give a URL, include a short ID string which logs the user in and takes them to the relevant step the digital and paper counterparts should have a similar look and feel InDesign CS6 (.indd) file InDesign CS4 (.idml) file PDF file (not editable) Occasionally, digital services will need paper forms at some stage in the transaction. Sometimes an application may require one or more ‘wet signatures’ to validate it, or print alternatives may be part of your assisted digital support. For the most part you can approach these as design challenges like any other – the Design Principles are a great way to start this work – but there are some specific things you’ll want to bear in mind. The most important thing to remember is that print forms should only be made when absolutely necessary. For example, the legislation governing Lasting Powers of Attorney (LPA) requires ‘wet signatures’ for the application to be valid. Until this legislation changes, a print form has to be part of the final service. Aim for as much of the transaction as possible to be completed online. Users should have to print as little as possible in order to complete an application, which means: In lots of cases, paper forms will exist for services being redesigned. The launch of the digital service gives you an opportunity to improve them. When doing so, don’t just base your designs on the existing forms. Look at what information you need to carry out the transaction and try to eliminate unnecessary steps.   Performance data should be available for the existing forms. This will tell you about the most common options people select and the kinds of mistakes they make. This will be useful information while you design the new forms. Links, smart answers, search boxes and drop downs are not available in print publications. If a transaction requires multiple forms you will need to make sure the design and layout helps users navigate them as easily as their online counterparts.  In the Lasting Power of Attorney application forms, this meant:   Your paper forms are a small but important part of a digital process. Typically you’ll be switching to paper forms to collect a signature. There are some ways you can make this transition easier: You should test your paper forms with real users. For complex forms, plan for 3 rounds of lab testing, making improvements to the form between each round. You’ll gain valuable insight by attending the user research in person. Get an introduction to user research and find out about all the different kinds of testing. Templates (based on the Lasting Power of Attorney application) created by teams across government will help you make easy-to-complete forms, but lots of the ‘heavy-lifting’ will be in making specialist terms and language accessible to the majority of users. Where specialist terms are unavoidable, or necessary, you’ll need people who understand how users read and how best to write for government services when working on the design. Skilled content designers and copywriters will make forms – whether online or in print – simpler and clearer, improving the completion rate and user satisfaction in the process. They complement the digital transaction and should be easy to process by staff supporting the digital service. Their performance also has an impact on the KPIs you gather for digital services. Contact us if you’re starting work on print forms. We can send you the LPA form pack to use as a template. (The redesigned LPA forms will be made public soon by the Ministry of Justice as part of a formal consultation.) If you get the error ‘There is no application set to open the document…’, you don’t have InDesign installed. You’ll need to install a trial or buy a subscription.","description":"Occasionally, digital services will need paper forms at some stage in the transaction. Sometimes an application may require one or more ‘wet signatures’ to validate it, or print alternatives may be part of your assisted digital support.","link":"/service-manual/user-centred-design/print-forms.html"},{"title":"Buttons","indexable_content":"Button styles Button colours Disabling buttons Types of button Writing button text Example Example Primary actions Secondary actions Warning actions Launch button Common actions Compound actions Primary actions move the user on to the next part of the transaction Avoid having multiple primary actions on a single page Secondary actions change the current view They don’t move users to the next step There can be multiple secondary actions per page They should be less prominent than the primary action Actions that have irreversible effects should look ‘scary’ Keep them away from the other actions Make the action reversible / ‘undo’-able If that isn’t possible to implement, ask the user for confirmation before continuing with the action Use to start a transaction Let users know if they’ll be taken to a different website verbs and an active voice clear, informative and succinct language sentence case technical terms – eg use ‘delete’, not ‘form reset’, use one of the common actions below instead of ‘submit’ lots of words – eye tracking shows users are less likely to read long labels Buttons should be used to signify actions that the user can perform. Buttons can be made of links, inputs and button tags. CSS should be used to separate the technical implementation of a button from its style so it looks the same whatever tag is used, like this:      Button tag     Link tag      Buttons should have a default colour (like green). Alternative colours should be possible but only to indicate different actions.      Primary action     Secondary action     Warning action    As with the default browser style, buttons will need a disabled style to indicate when the disabled attribute is set.      Button tag     Link tag           Next step         Save         Delete account         Get started      on the HMRC website    Do use: Don’t use: A simple guide is to see if the text passes the ‘Would you like to … ? /  I would like to …’ test. Many of these can be used in place of ‘Submit’, which is a technical term that you should avoid. Sometimes you want a single button to perform more than one action like ‘Save and quit’. It’s worth trying to avoid this situation but if you can’t, use common sense. If one of the actions is obvious or not important to know, don’t mention it. For example, if a button saves the current state and moves the user to the next screen, don’t use ‘Save and next’, just use ‘Next’, because users will assume the former. When in doubt, test with real users.","description":"Buttons should be used to signify actions that the user can perform.","link":"/service-manual/user-centred-design/resources/buttons.html"},{"title":"CAPTCHA","indexable_content":"Why you shouldn’t use them Alternatives to CAPTCHA Further reading Usability – they put the burden of detecting bots on the user rather than the system. As CAPTCHAs are designed to be hard to read and understand, this makes the service much more difficult to use. Accessibility – they are inaccessible by design. This effectively makes the service unusable by people with certain disabilities. Even CAPTCHAs that provide audio versions do not completely resolve this issue. Privacy – 3rd party CAPTCHA services set cookies, collect analytics and can track users across multiple sites. This introduces significant privacy concerns Performance – use of a 3rd party CAPTCHA service ties your performance to theirs. If their service goes offline, so does access to your service Security – the security of your service is tied to that of the 3rd party. If they are compromised, your service and its users may also be rate and connection limiting use of honey pots protective monitoring CAPTCHA and the BBC Ticketmaster ditches CAPTCHA for something simpler CAPTCHA stands for Completely Automated Public Turing test to tell Computers and Humans Apart. They are used to prevent bots (automated software) from completing a form or accessing a system and usually take the form of jumbled up text for the user to decipher and enter before submitting a form. CAPTCHAs introduce significant problems to online services: Additionally, if a 3rd party CAPTCHA service is used, there are further problems to consider: Many of the risks that CAPTCHAs are aimed to mitigate can be addressed in other ways: It’s important to note that even with a CAPTCHA in place bots will still get through due to advances in computer imaging and the use of CAPTCHA farms. A combination of different approaches generally gives the best results.","description":"CAPTCHA stands for Completely Automated Public Turing test to tell Computers and Humans Apart. They are used to prevent bots (automated software) from completing a form or accessing a system and usually take the form of jumbled up text for the user to decipher and enter before submitting a form.","link":"/service-manual/user-centred-design/resources/captcha.html"},{"title":"Colour palettes","indexable_content":"Semantic colour names Standard colour palette Greys Government department colours Text Backgrounds and borders Purple Mauve Fuschia Pink Baby Pink Red Mellow Red Orange Brown Yellow Grass Green Green Turquoise Light Blue HM Government Treasury Cabinet Office Department for Education Department for Transport Home Office Department of Health (NHS) Ministry of Justice Ministry of Defence Foreign and Commonwealth Office Department for Communities and Local Government Department for Energy and Climate Change Department for Culture Media and Sport Department for Environment Food and Rural Affairs Department for Work and Pensions Department for Business, Innovation and Skills Department for International Development Government Equalities Office Attorney General's Office Scotland Office Wales Office $text-colour ($black) $secondary-text-colour ($grey-1) $link-colour (#005ea5) $link-visited-colour (#2e8aca) $link-active-colour (#2e8aca) $link-hover-colour (#2e8aca) $border-colour ($grey-2) $panel-colour ($grey-3) $canvas-colour ($grey-4) $highlight-colour ($grey-4) $page-colour ($white) Sass: $purpleHex: #2e358b Sass: $purple-50Hex: #9799c4 Sass: $purple-25Hex: #d5d6e7 Sass: $mauveHex: #6f72af Sass: $mauve-50Hex: #b7b9d7 Sass: $mauve-25Hex: #e2e2ef Sass: $fuschiaHex: #912b88 Sass: $fuschia-50Hex: #c994c3 Sass: $fuschia-25Hex: #e9d4e6 Sass: $pinkHex: #d53880 Sass: $pink-50Hex: #eb9bbe Sass: $pink-25Hex: #f6d7e5 Sass: $baby_pinkHex: #f499be Sass: $baby-pink-50Hex: #faccdf Sass: $baby-pink-25Hex: #fdebf2 Sass: $redHex: #b10e1e Sass: $red-50Hex: #d9888c Sass: $red-25Hex: #efcfd1 Sass: $mellow-redHex: #df3034 Sass: $mellow-red-50Hex: #ef9998 Sass: $mellow-red-25Hex: #f9d6d6 Sass: $orangeHex: #f47738 Sass: $orange-50Hex: #fabb96 Sass: $orange-25Hex: #fde4d4 Sass: $brownHex: #b58840 Sass: $brown-50Hex: #dac39c Sass: $brown-25Hex: #f0e7d7 Sass: $yellowHex: #ffbf47 Sass: $yellow-50Hex: #ffdf94 Sass: $yellow-25Hex: #fff2d3 Sass: $grass-greenHex: #85994b Sass: $grass-green-50Hex: #c2cca3 Sass: $grass-green-25Hex: #e7ebda Sass: $greenHex: #006435 Sass: $green-50Hex: #7fb299 Sass: $green-25Hex: #cce0d6 Sass: $turquoiseHex: #28a197 Sass: $turquoise-50Hex: #95d0cb Sass: $turquoise-25Hex: #d5ecea Sass: $light-blueHex: #2b8cc4 Sass: $light-blue-50Hex: #96c6e2 Sass: $light-blue-25Hex: #d5e8f3 #0B0C0C, $black #6F777B, $grey-1 #BFC1C3, $grey-2 #DEE0E2, $grey-3 #F8F8F8, $grey-4 #FFFFFF, $white HM GovernmentSass: $hm-governmentHex: #0076c0 TreasurySass: $treasuryHex: #af292e Cabinet OfficeSass: $cabinet-officeHex: #0078ba Department for EducationSass: $department-for-educationHex: #003a69 Department for TransportSass: $department-for-transportHex: #006c56 Home OfficeSass: $home-officeHex: #9325b2 Department of Health (NHS)Sass: $department-of-healthHex: #00ad93 Ministry of JusticeSass: $ministry-of-justiceHex: #231f20 Ministry of DefenceSass: $ministry-of-defenceHex: #4d2942 Foreign and Commonwealth OfficeSass: $foreign-and-commonwealth-officeHex: #003e74 Department for Communities and Local GovernmentSass: $department-for-communities-and-local-governmentHex: #00857e Department for Energy and Climate ChangeSass: $department-of-energy-climate-changeHex: #009ddb Department for Culture Media and SportSass: $department-for-culture-media-sportHex: #d40072 Department for Environment Food and Rural AffairsSass: $department-for-environment-food-and-rural-affairsHex: #898700 Department for Work and PensionsSass: $department-for-work-and-pensionsHex: #00beb7 Department for Business, Innovation and SkillsSass: $department-for-business-innovation-and-skillsHex: #003479 Department for International DevelopmentSass: $department-for-international-developmentHex: #002878 Government Equalities OfficeSass: $government-equalities-officeHex: #9325b2 Attorney General's OfficeSass: $attorney-generals-officeHex: #9f1888 Scotland OfficeSass: $scotland-officeHex: #002663 Wales OfficeSass: $wales-officeHex: #a33038 This is the standard GOV.UK colour palette. We recommend you use the Sass variables where possible in case the colour values are updated. The variables are defined in _colours.scss in the GOV.UK Frontend Toolkit. $text-colour ($black) $secondary-text-colour ($grey-1) $link-colour (#005ea5) $link-visited-colour (#2e8aca) $link-active-colour (#2e8aca) $link-hover-colour (#2e8aca) $border-colour ($grey-2) $panel-colour ($grey-3) $canvas-colour ($grey-4) $highlight-colour ($grey-4) $page-colour ($white) Sass: $purple Hex: #2e358b Sass: $purple-50 Hex: #9799c4 Sass: $purple-25 Hex: #d5d6e7 Sass: $mauve Hex: #6f72af Sass: $mauve-50 Hex: #b7b9d7 Sass: $mauve-25 Hex: #e2e2ef Sass: $fuschia Hex: #912b88 Sass: $fuschia-50 Hex: #c994c3 Sass: $fuschia-25 Hex: #e9d4e6 Sass: $pink Hex: #d53880 Sass: $pink-50 Hex: #eb9bbe Sass: $pink-25 Hex: #f6d7e5 Sass: $baby_pink Hex: #f499be Sass: $baby-pink-50 Hex: #faccdf Sass: $baby-pink-25 Hex: #fdebf2 Sass: $red Hex: #b10e1e Sass: $red-50 Hex: #d9888c Sass: $red-25 Hex: #efcfd1 Sass: $mellow-red Hex: #df3034 Sass: $mellow-red-50 Hex: #ef9998 Sass: $mellow-red-25 Hex: #f9d6d6 Sass: $orange Hex: #f47738 Sass: $orange-50 Hex: #fabb96 Sass: $orange-25 Hex: #fde4d4 Sass: $brown Hex: #b58840 Sass: $brown-50 Hex: #dac39c Sass: $brown-25 Hex: #f0e7d7 Sass: $yellow Hex: #ffbf47 Sass: $yellow-50 Hex: #ffdf94 Sass: $yellow-25 Hex: #fff2d3 Sass: $grass-green Hex: #85994b Sass: $grass-green-50 Hex: #c2cca3 Sass: $grass-green-25 Hex: #e7ebda Sass: $green Hex: #006435 Sass: $green-50 Hex: #7fb299 Sass: $green-25 Hex: #cce0d6 Sass: $turquoise Hex: #28a197 Sass: $turquoise-50 Hex: #95d0cb Sass: $turquoise-25 Hex: #d5ecea Sass: $light-blue Hex: #2b8cc4 Sass: $light-blue-50 Hex: #96c6e2 Sass: $light-blue-25 Hex: #d5e8f3 #0B0C0C, $black #6F777B, $grey-1 #BFC1C3, $grey-2 #DEE0E2, $grey-3 #F8F8F8, $grey-4 #FFFFFF, $white Sass: $hm-government Hex: #0076c0 Sass: $treasury Hex: #af292e Sass: $cabinet-office Hex: #0078ba Sass: $department-for-education Hex: #003a69 Sass: $department-for-transport Hex: #006c56 Sass: $home-office Hex: #9325b2 Sass: $department-of-health Hex: #00ad93 Sass: $ministry-of-justice Hex: #231f20 Sass: $ministry-of-defence Hex: #4d2942 Sass: $foreign-and-commonwealth-office Hex: #003e74 Sass: $department-for-communities-and-local-government Hex: #00857e Sass: $department-of-energy-climate-change Hex: #009ddb Sass: $department-for-culture-media-sport Hex: #d40072 Sass: $department-for-environment-food-and-rural-affairs Hex: #898700 Sass: $department-for-work-and-pensions Hex: #00beb7 Sass: $department-for-business-innovation-and-skills Hex: #003479 Sass: $department-for-international-development Hex: #002878 Sass: $government-equalities-office Hex: #9325b2 Sass: $attorney-generals-office Hex: #9f1888 Sass: $scotland-office Hex: #002663 Sass: $wales-office Hex: #a33038","description":"This is the standard GOV.UK colour palette. We recommend you use the Sass variables where possible in case the colour values are updated. The variables are defined in _colours.scss in the GOV.UK Frontend Toolkit.","link":"/service-manual/user-centred-design/resources/colour-palettes.html"},{"title":"Creating accessible PDFs","indexable_content":"In Microsoft Word In Adobe Acrobat Before publication Acknowledgements Use headings Use lists Create a table of contents Use readable body text Use good colour contrast Use data tables Provide text descriptions Set the document language Check the tag tree Check the tab order Check the reading order Check the reflow order Check text descriptions Remove empty tags Set decorative content Check data tables Check active links Check high contrast Display document title Full Adobe accessibility check Quick screen reader check Use NVDA Use VoiceOver from the top of the PDF (with the Numlock off), use Numpad 0 + Numpad 2 to read the PDF from top to bottom and check the reading order use the tab key (repeatedly) to move through the PDF and check the tab order use the h key (repeatedly) to move through the PDF and check the heading structure use the g key (repeatedly) to move through the PDF and check for text descriptions from the top of the PDF use a double finger down swipe, or Control + Option + A to read the PDF from top to bottom and check the reading order use the tab key (repeatedly) to move through the PDF and check the tab order The best way to create an accessible PDF is to create an accessible source document. Well structured Microsoft Word documents make good source documents for conversion to PDF. When a source document is converted into PDF it is tagged. The PDF tag tree reflects the structure of the document, and it’s this structure that assistive technologies like screen readers use to navigate the document. Use the styles and features available in Word to format your content and give it structure. This will make it easier to convert your source document into PDF because it lays the groundwork for the PDF tag tree. Use the heading styles in Word to create a logical document structure. Don’t increase the size of text or make it bold to create the appearance of headings. Treat your document like a book: It should have one title (level one heading) and multiple chapters (level two headings). Within each chapter there may be multiple sections (level three headings) and sub sections (level four headings). Use the list styles in Word to group together related items. If the items follow a specific sequence, use a numbered list instead. Don’t use punctuation or other markers to create the illusion of a list. If your document is longer than a few pages, use Word to automatically create a table of contents based on your heading structure. Don’t use lists and links to manually create a table of contents. Use left aligned text (unless the language of your document is read right to left). Don’t use justified text in your document. Choose a sans serif font and use the styles in Word to set it as the default, with a minimum size of 12pt. If you need to include footnotes or other text of a smaller size, increase the size of the body text to 14pt rather than reducing the size of any text below 12pt. Don’t use chunks of italicised or capitalised text, and don’t underline text unless it’s a link. Use foreground/background colours for text that have a good contrast ratio. The 4.5:1 ratio recommended by the Web Content Accessibility Guidelines 2.0 is a good minimum. Don’t use colour or shape as the only way to identify something in your document. Use text labels or descriptions instead. Use tables with column headings to display data. Don’t use tables to make cosmetic changes to the layout of the document. Use Word to add text descriptions to all important images in the document. Make sure the text description includes all the information contained within, or conveyed by, the image. Use Adobe Acrobat Pro to convert your Word document into PDF. Use the Convert to PDF option under the Adobe menu in Microsoft Word to do this. This will ensure that Acrobat picks up the accessibility you have built into your source document. Set the language of the document. Go to File > Properties > Advanced and select a language from the Language menu. If the PDF is written in Welsh, type CY into the box. All content must be tagged, marked as an artefact (background content), or removed from the tag tree. Use the Tags panel to review and edit the tag tree. If the PDF was converted from a well structured Word document, the tag tree should require little editing. If the PDF contains form fields, use Advanced > Accessibility > Touch up reading order to check they can be navigated with the tab key in a logical order. If the tab order needs improving, use the Order panel to drag and drop the fields into the correct order. Use the Tags panel to review and edit the reading order of the PDF. Don’t rely on the visual order of the PDF. The reading order is based on the structure of the PDF tag tree, which may not match the visual content order. Use View > Zoom > Reflow then check that the PDF still has a logical reading order. Note: It can sometimes be difficult to guarantee a logical reflow order for PDfs with complex content. Go to Advanced > Accessibility > Touch up reading order and check that all images have text descriptions. If the text descriptions were present in the source Word document and the Convert to PDF option was used, the text descriptions should already be present in the PDF. Remove empty tags from the tag tree. Use the Tags panel to highlight and delete any empty tags from the tag tree. Tag decorative content elements as artefacts. Use Advanced > Accessibility > Touch up reading order to select a decorative element, and use the Background button to make the element an artefact. Use the Tags panel to check the structure of data tables. The <table>, <tr> and <td> tags should be used to give data tables the proper structure. Use the Tags panel to check that links are active. Active links should be tagged with the <link> tag. Use File > Preference > Accessibility to set a high contrast colour scheme, and check the PDF remains readable. It may not be possible to make high contrast mode work in all PDFs, in which case you should be prepared to make a high contrast version available on request. Display the document title instead of the file name. Go to File > Properties > Initial view and select Document title from the Show drop down box. Once all the above steps have been taken, the PDF should be checked before it is published. Go to Advanced > Accessibility and select Full check. The PDF should pass the full check for WCAG 2.0 Level AA without any warnings. Ask a screen reader user to read through the PDF. If no-one is available to do this, use one of the following options instead. Non Visual Desktop Access (NVDA) is a free open source screen reader for Windows. It can be installed to the desktop or run from a portable USB thumb drive. With NVDA running, open the PDF and use the following commands to check the PDF: These commands will also work with the JAWS screen reader from Freedom Scientific. VoiceOver is the integrated screen reader with Mac OS X and all iOS devices. In Mac OS X turn VoiceOver on (or off again) using Command + F5. With VoiceOver running open the PDF and use the following commands to check the PDF: VoiceOver does not provide shortcut keys for navigating PDFs by headings or graphics. Thanks to the Department for Work and Pensions (DWP) for enabling us to incorporate its accessibility best practice guidance into this document.","description":"The best way to create an accessible PDF is to create an accessible source document. Well structured Microsoft Word documents make good source documents for conversion to PDF. When a source document is converted into PDF it is tagged. The PDF tag tree reflects the structure of the document, and it’s this structure that assistive technologies like screen readers use to navigate the document.","link":"/service-manual/user-centred-design/resources/creating-accessible-PDFs.html"},{"title":"Forms","indexable_content":"Please check the form Examples in this page Examples on GOV.UK User interface patterns Aligning controls in a column Aligning controls in a row Pre-checked radio buttons and checkboxes Wrapping controls in a label tag Label positioning Hidden labels Styling text input fields Fieldsets and legends Hints Buttons Validation messages Nested fieldsets Example you already know the answer because you were given it previously there is a good business reason to steer users towards a particular answer, for instance ‘Contact me by email’ may be preferable to ‘Contact me by phone’ to help manage call centre workload there is a strong ‘common case’ bias towards a particular answer selecting none is a valid option (you should avoid using radio buttons for this as they can’t be unchecked) you want an unbiased opinion without leading the user there is a legal requirement for the user to make a choice using a light grey background to make them stand out equally on a white page or coloured panel applying an inset border style as users are accustomed to typing into ‘holes’ cut in to the interface Re-type your email address Select at least one area of interest Contact page Freedom of information request form Search results page 404 page Report a problem form (click ‘Is there anything wrong with this page?’ at the bottom to view the form) Licence finder Trade Tariff Like other components of web pages, forms should be created following the principles of progressive enhancement. Browsers have default styling for forms. This is usually shared with the styling of the operating system user interface (UI), making it familiar to users. Ensure that any styling you add does not remove any of the native, highly accessible functionality offered by these defaults. The HTML5 specification should be consulted for guidance on creating the HTML. This is more important for forms than other parts of HTML as some types of user will depend on proper use of the language. For example it is important each form element has a label describing it otherwise screenreaders will not be able to identify it properly. In the forms created so far on GOV.UK certain patterns have emerged as solutions to common UI problems. What follows is a guide to those patterns. The basic pattern for grouping controls is in a column with one row for each control and its label.           Job offers          Networking          Business opportunities        You might occasionally need to arrange your group of controls in a row, for instance if they have short labels and there are only a few options.               Male              Female            You may want to pre-check radio buttons if: Do not pre-check radio buttons if: In the examples above the controls are wrapped in their associated label tag. The default behaviour of clicking a label will move focus to its associated form element. By wrapping a form element in its label you increase the size of the area users need to click to interact with that element to whatever size you make the label. There are valid cases for top, left or right alignment of labels. The position of an element’s label does not effect how screenreaders announce it to users. The table below (from a great article on form design in Smashing Magazine) outlines the relative advantages of each approach: All form elements other than submission buttons should have a label. For situations where a label would not fit into the visual interface the label should be hidden from view. Hiding labels should be done with care as by doing so you are taking information away from the user. Labels should never be hidden by using display: none as this will also remove them from the document flow meaning they will not be recognised by screenreaders. In the example below the second input box has a hidden label. It is associated with the first input box visually by its position. For non-visual users it is important the label is present to provide this information.          Street                   Street line two                   Town/City                   Postcode          When styling form elements you should try to achieve the same goals as the default styling through our own CSS. You can achieve this for input fields by: As explained in their HTML5 specification section fieldsets are used to break up forms into logical sections.              First name                           Last name              There are times when you might want to treat a set of form controls like they were a single, compound control (like a date of birth selector). One way to do this is with a nested fieldset.            Full name                           Day               Day                Month               Month                Year               Year Hints for help with interactions can be placed above or below the relevant control.          Telephone         Include your country code                 Code         The three numbers on the back of the card                 Password         Make it at least six characters long          By default buttons should be horizontally left-aligned beneath the form inputs (not necessarily left aligned with the labels, and not right aligned on the page). The primary action should be the first button in the group.            Full name                        See the separate page on buttons for more detailed guidance. Summarise any validation errors at the top of your page like this: Each link should jump the user down to the corresponding form control, where the control should display a red validation message. The red summary visually connects the summary to the error messages in the form and aids quick scanning of the form for errors. To see all the examples above in a single form, check out the registration form example. The CSS for those styles is derived from this Sass file. Examples of forms currently in use across GOV.UK are:","description":"Like other components of web pages, forms should be created following the principles of progressive enhancement.","link":"/service-manual/user-centred-design/resources/forms.html"},{"title":"Grids","indexable_content":"Using the mixin Simple example Regular grids Irregular grids Browser support HTML Sass Example 5,3 ratio example 2,2,1 ratio example 2,1 ratio example when a simple linear layout will work when a data table is more appropriate for full page layouts (it’s not that kind of grid) Item 1 Item 2 Item 3 Item 4 Item 5 Item 6 Item 7 Item 8 Item 1 Item 2 Item 1 Item 2 Item 3 Item 4 Item 1 Item 2 Item 3 Item 4 Item 5 Item 1 Item 2 Item 3 Item 4 Item 5 Item 6 Item 7 Item 8 Item 1 Item 2 Item 3 Item 4 Item 1 Item 2 Item 3 Item 4 Item 5 Item 6 Item 1 Item 2 Item 3 Item 4 Use this mixin if you need to arrange content in a grid or split part of a page into columns. You might want to do this for an image gallery, product catalogue or home page layout. It’s particularly useful if you don’t want to explicitly represent rows or columns in the markup. Don’t use this The grid mixin accepts the following arguments: The mixin is tag-agnostic, so the elements can be list items, divs, paragraphs or anything else. Avoid applying border effects to the elements as this will make the width calculation incorrect. Style the contents of those elements instead. At mobile screen sizes the grid elements switch to being full-width. Item 1 Item 2 Item 3 Item 4 Item 5 Item 6 Item 7 Item 8 You can create grids of equally sized elements by passing in a single value representing the number of elements in a row. The following examples are for demonstration purposes only and not ones we’d ever recommend. Item 1 Item 2 Item 1 Item 2 Item 3 Item 4 Item 1 Item 2 Item 3 Item 4 Item 5 Item 1 Item 2 Item 3 Item 4 Item 5 Item 6 Item 7 Item 8 You can create grids of unequally sized elements by passing in an array representing the relative widths of the elements in a row. Item 1 Item 2 Item 3 Item 4 Item 1 Item 2 Item 3 Item 4 Item 5 Item 6 Item 1 Item 2 Item 3 Item 4 This Sass is supported perfectly by Chrome, Firefox, Safari and Internet Explorer 9 and higher. In Internet Explorer 7 and 8, regular grids are fine but irregular grids use first-child rather than nth-child. For multiple rows you’ll need to pass in a $max-rows variable representing the maximum number of rows in the grid. In Internet Explorer 6, all grid elements take up the entire width of the page.","description":"Use this mixin if you need to arrange content in a grid or split part of a page into columns. You might want to do this for an image gallery, product catalogue or home page layout. It’s particularly useful if you don’t want to explicitly represent rows or columns in the markup.","link":"/service-manual/user-centred-design/resources/grids.html"},{"title":"Header and footer","indexable_content":"Headers Footers Templates and styles Design patterns Pattern 1: No title Pattern 2: Title Pattern 3: Title and navigation Search Soliciting user feedback Using the GOV.UK logotype open a blank email in the user’s email client, using the mailto URL scheme toggle the appearance of an embedded web form take the user to a dedicated feedback web form on another page within your service    ALPHA: This is a prototype     BETA: This is a trial service     ALPHA: This is a prototype – your feedback will help us to improve it. Find out more     BETA: This is a trial service – your feedback will help us to improve it. Find out more  Users need to know if your service is in alpha or beta. Many will not recognise these terms or know what they mean so it’s up to you to make them aware that a service is not officially live and they should use it at their own risk. That means labelling the service clearly and explaining what it means for them. Your service must be clearly labelled as alpha or beta using one of the three design patterns described below. So far, we have identified three patterns that fit the situations we have encountered, we may update this list as the exemplar programme matures and we have strong evidence on how they perform. Each design pattern also has a banner below the title, which should briefly explain the stage of the project – ‘alpha’ or ‘beta’ – and that it is a trial service. The text for the banners should be as follows: ALPHA: This is a prototype BETA: This is a trial service Services that aren’t a transaction funnel may optionally choose to link to a feedback survey and description of alpha or beta: ALPHA: This is a prototype – your feedback will help us to improve it. Find out more BETA: This is a trial service – your feedback will help us to improve it. Find out more Services should only add these links when it is acceptable for a user to be taken away from their existing flow. In all cases the GOV.UK logotype must link to www.gov.uk. This is simply the GOV.UK header as is, without a title. This is relevant if your service is small and straightforward, taking place over only a few screens. You should also use this on the first page of your service if you are using the title of the service as the main heading the body.  This is the pattern that will be used in most cases. Your service will not need any internal navigation, but will need a title to identify itself. The service title should link to the service start page unless there is a reason why this would not work.  If you service needs to show navigation, then this should sit below the service title. The service title should link to the service start page.  In this case “Your tax account” is not a transactional funnel, so it can also include links to feedback and information. If your service requires a search function this should sit in the header. The search form should be clearly labelled, indicating that it is only searching your service and not the entire GOV.UK domain. You should try to not complicate the interface by having multiple search boxes on a single page, though you may need to use in-page filters. These should be clearly and accessibly labelled and given appropriate ARIA attributes. They should also be visually differentiated from the main search box. Users can provide data that is invaluable for developing and improving your service. You should ask for feedback by providing a link to your service’s feedback mechanism from the ‘alpha’/’beta’ banner. The link could: You can only use the GOV.UK logotype if your service is currently available on GOV.UK. If the service is on a temporary domain then you must use an alternative header that does not show the GOV.UK logotype or crown. The logotype must link back to GOV.UK. The footer of your service should follow the design patterns used by the GOV.UK footer. It should include links to secondary information for your service that would allow users to contact you directly or provide feedback about your service (like email addresses, phone numbers or contact forms). The footer should also display the appropriate copyright and licence notices. If you need access to the templates and styles, please make a request using the GOV.UK support form.","description":"Users need to know if your service is in alpha or beta. Many will not recognise these terms or know what they mean so it’s up to you to make them aware that a service is not officially live and they should use it at their own risk. That means labelling the service clearly and explaining what it means for them.","link":"/service-manual/user-centred-design/resources/header-footer.html"},{"title":"Form example -- Registration","indexable_content":"There was a problem submitting the form Example Enter your first name Enter your last name Re-type your email address Select at least one area of interest  Email  Telephone  Post This example form incorporates most of the basic user interface (UI) patterns for forms and lets you play with different label alignments. For a detailed breakdown of each element and how to code and style it, see our forms design pattern page. Click the label alignment options in the snippet below to see how they affect the layout.      Label alignment:      top |     left |     right    Please try the following:              Title             Mr.Mrs.MissMs.Dr.Other              1. Enter your first name             First name                           2. Enter your last name             Last name                               Day                 Day                  Month                 Month                  Year                 Year               Male              Female                         Enter email                           3. Re-type your email address             Re-type email                           Telephone             Include your country code                         Street                           Street line two                           Town/City                           Postcode                           Write a few short words about yourself                            Make this biography public                         4. Select at least one area of interest              Job offers              Networking              Business opportunities                       ","description":"This example form incorporates most of the basic user interface (UI) patterns for forms and lets you play with different label alignments.","link":"/service-manual/user-centred-design/resources/registration-form.html"},{"title":"Sass repositories","indexable_content":"Categories of Sass mixins Responsive design Cross browser Further reading GOV.UK typography, colours and image assets Mixins for responsive designs Mixins for targeting old versions of Internet Explorer Mixins for cross browser CSS Sass is a language for creating stylesheets that lets us share blocks of code and techniques. GDS has created a repository called the GOV.UK Frontend Toolkit to simplify the creation of services with a consistent look and feel. The toolkit is available as a gem for easy inclusion in Ruby projects. The files in the gem can be categorised into four main parts: The first part is the bit that gives all GOV.UK projects a familiar look. There are a collection of pre-defined font sizes that we use on GOV.UK. There is a mixin for each one, for example heading-26. These also include a standard amount of whitespace around the text to help with vertical rhythm on the page, spacing things out nicely. The second is a way to develop sites that are able to respond to different sized displays. The third is an easy way of writing Internet Explorer specific CSS in our stylesheets without using hacks. The fourth is a way to keep browser specific styles out of our projects. We encapsulate new or non-standardised CSS into mixins. In this way we can easily update all the instances of a new CSS property without having to do a search and replace across all our projects. It is generally advised to write your markup with a mobile first attitude. That is, add desktop styles to an otherwise narrow screen stylesheet. In this way you only add styles for desktop and don’t reset desktop styles for a mobile device. The are two main types of cross browser CSS that we are concerned with. Firstly using different techniques to achieve a consistent effect and secondly using vendor prefixes to apply consistent behaviour for newer features. For example: @extend %contain-floats uses a cross-browser technique to ensure that the element wraps all the floated elements within it. It is not a property that normally exists in CSS but is something we often need to do and don’t want to use different techniques everywhere. It gives us consistency across our code. @include border-radius is designed to use the different border radius implementations (-moz-border-radius, -webkit-border-radius etc) to create consistent presentation across different browsers. The README.md file in the GOV.UK Frontend Toolkit has more information.","description":"Sass is a language for creating stylesheets that lets us share blocks of code and techniques. GDS has created a repository called the GOV.UK Frontend Toolkit to simplify the creation of services with a consistent look and feel. The toolkit is available as a gem for easy inclusion in Ruby projects.","link":"/service-manual/user-centred-design/resources/sass-repositories.html"},{"title":"Shared asset libraries","indexable_content":"Why we do this Where to find our code govuk_template govuk_frontend_toolkit Yahoo recommends minimising HTTP requests to improve performance All used libraries are kept at known versions, which guarantees compatibility and reduces the risk of security vulnerabilities through external server compromise govuk_template govuk_frontend_toolkit Shared asset libraries are helpful for using the same frontend and branding on multiple services. When building services around patterns and consistency, it’s important to share your frontend assets so that they can be easily reused as required. There are two more benefits to this approach: The templates on GOV.UK are constantly changing as we react to user feedback and evolving best practice, so the best place to find them is by looking at the code we publish: The GOV.UK template is a project that provides the GOV.UK header and footer, as well as associated assets. It generates a variety of output formats and you can extend it with more depending on the language your service is written in. GOV.UK’s static consumes the Ruby version of the template application, which in turn provides shared resources like footer links across the various frontend apps that run GOV.UK. The Performance Platform’s Node.js application uses the `` version of the template. You should be able to become a consumer of the template in exactly the same way these two projects are, by adding it to your application’s dependencies. The service manual article on Sass repositories has more information on this repository. GDS is continuously improving GOV.UK, which means that template and asset code changes regularly. All services on GOV.UK are expected to keep their templates and assets updated. How you do this will depend on how your frontend is implemented, but if you include the template and the toolkit as dependencies in your application it should be relatively easy to update as GDS publishes new versions. Please contact the GDS team for help or advice.","description":"Shared asset libraries are helpful for using the same frontend and\nbranding on multiple services.","link":"/service-manual/user-centred-design/resources/shared-asset-libraries.html"},{"title":"Typography","indexable_content":"The GOV.UK typeface Using New Transport Colour contrast Type size Further reading A blog post from Ben Terrett on why we chose New Transport Shared asset libraries Services should use clear, easy to read type, with consistent styles and a clear hierarchy of information. GOV.UK uses the typeface New Transport, created especially for government use. This typeface is embedded in the GOV.UK CSS and is served to browsers via a WOFF file (or an EOT file for Internet Explorer 8). The font is hinted to display well on all browsers. Older browsers do not receive the typeface. We currently use two weights of New Transport: Light and Bold. Italics should not be used. The number of different type sizes on a page should be kept to the minimum, and only one typeface/font should be used on each website. New Transport is not licensed for use outside of the GOV.UK domain. When your service goes live you’ll be given access to the typeface. If the service uses numbers in columns or tables, you should change these to use the tabular numbers version of New Transport. This replaces the standard numbers with new versions that have a fixed width. The main noticeable difference is a base on the character 1. GDS has used this on the Performance Platform and Trade Tariff. Text must have enough contrast against the background colour to be readable. This should be tested to conform with our accessibility requirements. Generally we use type in #0B0C0C against a white or light grey background. Links should be blue and underlined, as mentioned in our guidance on colour palettes. Type should be large enough to be easily read. This is generally larger than many current websites: 36px for headlines, 19px for body text. This can be included using default styles in Sass from the GOV.UK Frontend Toolkit. These include line height spacing that works across browsers.","description":"Services should use clear, easy to read type, with consistent styles and a clear hierarchy of information.","link":"/service-manual/user-centred-design/resources/typography.html"},{"title":"Writing for transactions","indexable_content":"Writing help text Transaction start pages Transaction end pages Further reading General guidance Asking for personal details Inline help Progressive disclosure Pop-up help window Describe the service Set expectations Use sentence case Address the user directly Follow the style guide Don’t use ‘please’ on labels – but give polite, clear, short instructions Don’t use colons after labels Don’t add ‘your’ to labels, for example: ‘Your name’, ‘Your address’, unless you have a page with multiple people where you need to differentiate – for example: ‘your name’, ‘partner name’ ‘First name’ not ‘Christian name’ ‘Middle names’ ‘Last name’ not ‘Family name’ ‘Age’ ‘Date of birth’ (not ‘DOB’) that they have successfully completed the transaction what further actions they need to perform, and when what further actions they can expect from the service, and when who they should contact with any queries or complaints about any related information and services Microcopy is the term given to the short words or phrases used during transactions, for things like buttons, form labels, help text, alerts and questions. Your first strategy when it comes to help text is to design a service that’s so intuitive it doesn’t need any. For this reason it helps to stick to interface design conventions where possible. Avoid innovation for its own sake – the real innovation is an easy to use government service. Sometimes though, users need a little help. Here are some ways of providing it.          Telephone         Include your country code        Use this to provide examples for unfamiliar information requests or formats. This refers to help that appear on a page when the user interacts with a link, but remains hidden otherwise. It’s useful for delivering important help to some users, without distracting or confusing everyone else. For this reason it’s particularly useful for dealing with edge case user scenarios. This is the nuclear option and should only be considered as a last resort. Popups or lightboxes should ONLY be used for delivering help relating to the concepts or terminology involved in a service or transaction. If you’re using one to explain how to use a service then you need to go back and make the interface more intuitive. The start point for any GOV.UK transaction should be a page on the GOV.UK domain. Users should not be able to jump to a later page in the service via some other means (eg Google). The design of the start page will be determined by the nature of the service and its audience. All start pages should meet the following goals: The page should include the name of the service, expressed as an action if possible (‘Renew your passport’, Claim for disability allowance’, ‘Book a driving test’ etc). If you need to, include a very brief description of the service. People will tend to base their expectations of what a transaction involves on their experience of other digital transactions, for example, online shopping and banking. If your service has features that do not match these expectations then you should inform your users of this as early on as possible, ideally on the start page. Users should be told about any financial costs or long waiting periods involved. If they’ll be asked to provide relatively obscure information let them know so they can get it ready. If there are specific eligibility requirements for the service let people know. If the eligibility requirements are complex you should consider using the GOV.UK smart answer format to help people understand whether they’re eligible. It’s better to ask a few questions up front (and explain why you’re doing this) than to let people invest time and effort in a transaction only to discover part way through that they’re not eligible to use it. The end point of any transaction should be a page on the GOV.UK domain. These pages should let the user know: Information on designing forms that work on the ‘Forms that work’ site","description":"Microcopy is the term given to the short words or phrases used during transactions,\nfor things like buttons, form labels, help text, alerts and questions.","link":"/service-manual/user-centred-design/resources/writing-for-transactions.html"},{"title":"What your service should look like","indexable_content":"Designing your service Using the GOV.UK logo Typography Pictures and icons Header and footer Alpha Beta Live The service standard states that your service should be built “with the same look, feel and tone as GOV.UK”. The first place to start is by reading our design principles. These will help you to understand how we approach design. You should keep these in mind when designing and building your service, always thinking about how to make the service simpler, clearer and faster. At alpha stage, the important thing is to make something and make it quickly, before testing it with real users. You should follow the design principles and should spend your time working on the user journey, interface and experience, but you don’t need to worry about making everything pixel perfect in your prototype. At beta the main priority is creating a simpler, clearer, faster service. You should concentrate on making the user journey and experience as simple and clear as possible. However, you should  be moving towards a finalised look and feel, how fast you achieve this depends on the size of your design team and how much time you can dedicate to it. Before your service goes live it must look and feel like GOV.UK.  You should be using the correct typography, header and footer and if necessary should have followed the guidance on pictures and icons. Throughout the process of creating your service you should refer to the design principles, think of the user and design for simplicity. The user experience across all services on GOV.UK should be consistent. Your service is on GOV.UK if it’s either at www.gov.uk/myservice, myservice.service.gov.uk or myblog.blog.gov.uk. At the point of launch it should look like GOV.UK and should have the GOV.UK logotype, including the crown. You can still follow the guidance if your service isn’t on GOV.UK, but your site shouldn’t identify as being part of GOV.UK and shouldn’t use the crown or the GOV.UK logotype in the header. This blog post talks about a good example of a site that follows our design patterns without using the logo. Services should use clear, easy to read type, with consistent styles and a clear hierarchy of information. You’ll be given access to the New Transport typeface if your service is on GOV.UK. Read our full guidance on typography. In most cases your service will not need to use pictures. You should only use icons if they are clear, simple and help to convey information more clearly than text alone could. You should use the standard template for your header and footer if your service is on GOV.UK. Read our guidance on headers and footers","description":"The service standard states that your service should be built “with the same look, feel and tone as GOV.UK”.","link":"/service-manual/user-centred-design/service-look-and-feel.html"},{"title":"User-centred design in alpha and beta","indexable_content":"Designers and researchers are essential Researching in design iterations Build Measure Learn Fortnightly user research (eg ‘Testing Tuesday’) Guerrilla research Other research methods Analysis Share your findings Communicating Before the session During the session Affinity sorting Actions Prioritisation Video Showcase Consider your more distant stakeholders Publish the design as it stands Retrospectives keep your team concentrating on real user needs help teams design products which are prioritised by user needs help teams iterate products in response to user feedback they’re sharing all the knowledge that’s been gathered about users, so any decisions made on the product design will be influenced by real user insights the researcher can provide more information and more regular feedback — helping the entire team to iterate and prioritise, and create the best possible product they’ll continually create experiments that allow them to test and prove whether or not design approaches create products that users find understandable and desirable build measure learn it’s usually quite quick to create prototypes of design concepts in code doing so will usually gain more helpful insights from users than paper prototypes allows time for making tweaks and small changes reduces the risk of technical failures in the research sessions have people interact with the design you have been working on (usability testing) explore any specific issues you‘d like to gather more information on through a depth interview (one-on-one interviews that produce deep insights on user needs and behaviours) your own offices a research laboratory (get one from a commercial supplier if there’s not one readily available) define your research questions — what do you want to learn from the round of user testing? prepare some theories about your design, eg changing the words on the button will encourage people to read the page more carefully decide how you’ll know when these theories have been proved or disproved, eg you’ll know this is true because you’ll observe people taking more time to read the page decide the type/demographic of users you want to talk to     age       location       suitability to task       other factors      age location suitability to task other factors start recruitment at least a week before testing (GDS recommend that you find participants through a recruitment agency) prepare a discussion guide that explains how the sessions should be run and the important research outcomes create a participant release form that authorises the recording of the session send a schedule for the day to project team members, inviting and strongly encouraging them to attend sessions arrange for live streaming of the sessions online via a service such as GoToMeeting, so team members who can’t attend in person can still watch it ready to be seen by users ready to be tested end-to-end usable and accessible from the testing venue (eg firewall restrictions, screen resolutions, compatible browsers) make sure the session is being recorded make sure that the live stream of the session is available for external viewers keep research theories at the top of your mind so you don’t forget any important topics write down notes on post-its of important observations     use yellow, super-sticky post-it notes for observations       only 1 observation per post it note       use a marker pen and write in capital letters (easier to read when writing quickly)       if you’re not sure if it’s important, still write it down      use yellow, super-sticky post-it notes for observations only 1 observation per post it note use a marker pen and write in capital letters (easier to read when writing quickly) if you’re not sure if it’s important, still write it down get a written transcript of the session (either during the session or have it transcribed at a later time) analysing sharing communicating have a clear understanding of what you have learned know what the implications on design will be be able to make sure that important observations are not lost gather all the post-it note observations created during the testing sessions and stick them up on a big wall group the observations into similar themes create findings — a statement that summarises the observations — for each group, and write it down on a post-it note, sticking it on top of the group try to address the theories and decide if you have enough information to state a theory is now proven or disproven a theory needs further qualitative/quantitative testing gather findings confirm things that people have seen form a consensus on the findings remember what you discovered see how a particular theme or feature has developed over the course of the design iterations shared document blog story wall (eg Trello) research catalogue showcasing/presenting regular updates always publishing the design having retrospectives the research and design work being done what has been learnt from research how research is changing the design of the service inviting them to your showcases publishing a blog maintaining a shared document sending out regular update emails, using screenshots from the prototype to help people see what has been tested Carry out user research during every stage of your project. Do it continuously through each stage — don’t leave it as something that happens at the beginning and end of phases. Doing user research continuously will: Each team needs to have a designer and a researcher working together, both having an active interest in the design and user insight. This isn’t about a user researcher ‘testing’ the work of the designer — having designers and researchers work together means that: Make sure that you have a team member leading on research that sits with the team. They may also do other work (design, write content etc), but are also responsible for running research at least every 2 weeks. Don’t spend more than 2 weeks working on design without testing it with real end users. Each 2-week iteration should comprise of 3 stages:  With every iteration, create materials to be included in the next round of research. In the alpha and beta phases, these will typically be prototypes that vary in accuracy — from paper prototypes through to working prototypes in code. Use existing GDS code and other open source frameworks as: Accept that you’ll design and build a number of prototypes to test with end users, and that these prototypes will be thrown away. This gives the team freedom to explore a range of different concepts and gain a better understanding of what works best for end users. Allow time for these experiments particularly in the early stages of the project. In the early stages of your project the prototype will probably look unfinished and the design will still require a lot of work. Stick to the ‘test every 2 weeks’ rule, instead of waiting until you’re completely happy with the design or have something that feels finished. Try to make sure that the prototype is completed at least 24 hours before your testing sessions begin, as it: There are many ways to create ‘experiments’ that will help you to answer questions about your product, your end users and your design ideas. Your researcher will help choose the best research methods for answering each question. With their help, you’ll probably end up using a number of different techniques on your project. Whatever your project, be sure to do fortnightly user research. In these sessions you’re able to: Carry out research sessions with users every 2 weeks on the same day. Doing user testing at predictable and regular intervals means that your team members will be able to schedule time to watch sessions. A good approach is to schedule 5 one-on-one interviews for that day, each interview being 1 hour in duration. (Schedule a 6th participant as a ‘spare’ in case some participants don’t attend earlier in the day or you have a poorly recruited participant). Conduct these sessions in either: Make sure that you have both facilities booked and participants recruited as early in your project as possible for the expected duration of the alpha and beta phases. To prepare for these sessions, you need to: Prepare your prototype for the sessions by making sure that it’s: In each user testing session: A day of research should be followed by a period of analysis before making any significant design changes. The analysis stage is described in detail below. You can use the time between more formal testing to conduct some guerrilla-style testing, getting some initial feedback on your revised designs. Guerrilla testing normally involves taking your prototype into a coffee shop or other public location and finding volunteers who’ll give you some quick feedback. Guerrilla testing participants aren’t necessarily representative of your target audience, but they can provide a quick sense-check in between formal testing sessions. There are many other research methods you can use to supplement your ‘in person’ qualitative research and to address specific research questions. Using a different technique can provide better insight into a particular research question, or validate insights from fortnightly research with a larger audience. Here‘s a list of some techniques you can use and information on how and when you should use them. Now it’s time to see what you can learn from your user testing and research. Make the best possible use of this information by: It’s important to properly debrief after spending a day doing user testing as you’ll have seen a lot of people and a lot of different reactions. Although it takes time to analyse sessions in detail, doing so means that you’ll: To do this correctly, you need to: At this stage you’re aiming to create a full set of insights (things that you’ve observed and learned), so don’t start designing solutions just yet. You should allow several hours for this analysis process. Work with anyone that’s seen the user testing session to analyse observations, as it will help to:  Decide what you’re going to do about each finding, if anything, and when you’ve proven a theory, make sure that it’s recorded and shared with the project team. If a finding needs further action, determine what is required to address it in the next design iteration. Write this on an orange post-it note and stick it on top of the finding. When you’ve decided on what actions to take, use a prioritisation method (like dot voting) to decide how you’ll spend your efforts for the next iteration. Put user testing findings and actions in a place that’s easily accessible to the project team, as it’s likely that you’ll come back to the findings to: You can document your findings for others to see in many different ways, like using a: It’s also useful to keep a record of what you tested with users, eg if you’ve prototyped in code, keep a folder in your repository for each round. You may also like to keep screenshots of the prototype. GDS intend to create a format for sharing research findings between projects across government. While GDS work on the best way to do this, keep your research findings in an accessible shared format so findings can be shared and learned from in the future. The findings should, wherever possible, be easily understood with no need for an explanation. Move any actions onto your story wall so that the whole team can see what you’re working on and who’s working on it. Use videos to demonstrate findings and store the session videos in a safe place, ideally where they can be found and watched by team members. If you store the videos on a public service (like Vimeo), make sure they are adequately protected. You need to discuss the project and its progress with others. You can do this by: Just as a product team often presents a showcase at the end of each iteration, so should the research and design team. This will make sure the wider project team has an understanding of: This is an opportunity for members of the project team to catch up if they haven’t been able to attend user testing sessions, as well as an opportunity to ask questions. Depending on how your team works, you might put your presentation into the project team’s showcase or you may have a showcase just for research and design. There may be other people beyond your more immediate project team who are interested in the outcomes of your work. You can communicate and/or work together with these people by: Make it possible for your team to always access the latest version of the prototype, but make sure it’s appropriately protected. Team members may need to check something or use the prototype to talk about the project with their own stakeholders. Put a message across the top of the prototype to remind people that it’s a work in progress. It’s important to reflect on your progress at regular intervals. Schedule a retrospective where research and design team members have the chance to discuss the process openly and can suggest changes to the process. Assign owners and due dates to the actions to make sure that change occurs.","description":"Carry out user research during every stage of your project. Do it continuously through each stage — don’t leave it as something that happens at the beginning and end of phases.","link":"/service-manual/user-centred-design/user-centred-design-alpha-beta.html"},{"title":"User needs","indexable_content":"Defining user needs How we define user needs Work out your proposition User stories Further reading Gather the evidence Frame the need correctly You can’t do it all at once Existing content. Use web analytics to show that content that already exists is being regularly accessed Search terms. Use search logs to show that people are expressing a need for content or functionality by searching for it on the website Customer facing staff. Talk to people who work on service desks or in call centres. These people talk to users regularly and are able to tell you what the most pressing and real user needs are. User research. You may have previously commissioned user research that can help you understand what your users needs are. You can also commission from your research team a quick user research study that will help you understand and validate user needs by talking directly to a sample of users. it’s something that only the government does there’s clear demand for it from users (ie through search and traffic logs) or the government is legally obliged to provide it it’s something that people can do or it’s something people need to know before they can do something that’s regulated by/related to the government it’s something the government provides, does or pays for it’s inherent to a person’s or an organisation’s rights and obligations it’s straightforward advice that helps people to comply with their statutory obligations (eg what records you should keep for your HMRC tax return) or provides certain kinds of advice and support to businesses, but excludes general life or business advice that is provided by third parties Define the user need Write the user story from the perspective of the user Define the acceptance criteria Solution is explored and delivered by the service team What was the evidence? Users’ information needs and analytics Using search data to meet user needs Exploring user needs What we know about the users of Inside Government Meeting the needs of businesses Introducing the Needotron: working out the shape of the product Deep understanding of your users’ needs is crucial for building a successful digital service. Any thinking about a service, whether online or offline, must start with the question: what is the user need? Defining a user need must be strict and honest. For GDS, it’s the need the user has of government, not the need of government to impart information to the user. That’s an important distinction, because it means that you’ll be able to more measure the success of your services and iteratively improve them to meet the needs of the people who’ll make use of them. You should be able to provide evidence that a user need actually exists, and starting from the potential evidence areas can be a good way to start creating a list of user needs. Good evidence points can include: The way you describe each user need can have a big impact on how successfully you ultimately meet the real need of the user. Especially for your most important user needs it’s important to make sure you have framed the need correctly – that is, described it using words that actually reflect what the user’s trying to do and aren’t just loosely describing the kind of content you think they might need. For example, when thinking about tax codes, consider the solution you might design for this user need: ‘as a user I want information about how tax codes are calculated’. Now consider the solution you might design for this user need: ‘as a user I want to know if I’m on the right tax code’.  It can be difficult to get the framing right from analytics data alone. Especially for your most important user needs, be sure to do some qualitative research (talking directly to users) and talk to frontline staff to make sure the wording properly reflects the real user need. You need to set the boundaries of what your service can and should offer to prevent a bottomless list of user needs. This is especially true of a government website like GOV.UK. As well as identifying user needs, we identified which of those needs the government must answer. This led to the ‘It’s in if…’ criteria: Similar criteria may help you determine which needs you build a service to meet. Expressing a user need mustn’t imply the solution. The user need should be expressed as a user story, so the service team can discuss and explore possible solutions. Read more detailed guidance about writing user stories.","description":"Deep understanding of your users’ needs is crucial for building a successful digital service.","link":"/service-manual/user-centred-design/user-needs.html"},{"title":"Community user groups","indexable_content":"How research communities work Where and how you might use it When not to use Participants rapid speed (questions are answered in real time, research team can react rapidly to internal demands) cost effective when up and running flexibility availability rich outputs (visual content such as video is regularly used and is impactful) deep insights raises profile of research internally when insight is relevant and timely An online research community is a website that allows pre-selected respondents to interact with each other and complete a series of tasks. The types of community differ, and vary in openness, permanency and size. A typical community will be private, often have between 300-500 members and focus on building relationships between participants. This type of community offers a great source of deep insight over long periods of time. The tasks given to a research community differ in format and can include; online forums, blogs, polls, surveys, instant chat and more. Tasks are usually creative, and ask different questions relating to the research objective/s. Research communities can be used for a wide range of projects such as audience understanding, diary studies, concept development or simply idea generation. They allow a business to get close to users and offer meaningful dialogue between an organisation and its customers. The key benefits are: Communities require constant management. Participants need to be kept engaged, which means daily interaction, new questions and inventive approaches to maintain their interest. The community needs constant moderation, usually by a research manager or third party research agency. The quality of community members is paramount. Their feedback needs to be quality checked and panel attrition needs to be monitored on an on-going basis to ensure fresh ideas. Participants can be obtained using public postings on websites, email groups and social media, or via third party sample companies.","description":"An online research community is a website that allows pre-selected respondents to interact with each other and complete a series of tasks.","link":"/service-manual/user-centred-design/user-research/community-user-groups.html"},{"title":"Discussion Guides","indexable_content":"Where and how you might use them Example Discussion guides are used in order to ensure that focus groups and 1:1 interviews cover the required topics, and information is obtained from the sessions that will address the needs of the research. In preparation for a focus group or 1:1 interview it is helpful to generate a list of questions that address the information that you’re interested in obtaining from the sessions. These questions should be open ended, and structured in a manner that will help elicit information from respondents in a sensible flow. As suggested by the name, discussion guides should be used as a guide to the discussion, and in comparison with a structured questionnaire, questions and areas for coverage should not read out verbatim. This enables the discussion to be led by respondents’ own experience. Typically each section of the guide would include time guidance to ensure all areas can be covered and it should also indicate when stimulus is being used, and specify participants’ tasks. Writing a discussion guide should be an iterative process, and once the initial draft has been written it is helpful to get input from other people on the project team. A discussion guide written for testing that was conducted with BIS experts for the development of GOV.UK (PDF, 157kb).","description":"Discussion guides are used in order to ensure that focus groups and 1:1 interviews cover the required topics, and information is obtained from the sessions that will address the needs of the research.","link":"/service-manual/user-centred-design/user-research/discussion-guides.html"},{"title":"Ethnographic research","indexable_content":"How ethnographic research works Where and how you might use it When not to use Participants Timescales Ethnographic research usually involves observing target users in their natural, real-world setting,  rather than in the artificial environment of a lab or focus group. The aim is to gather insight into how people live; what they do; how they use things; or what they need in their everyday or professional lives. Ethnographic research relies on techniques such as observation, video diaries, photographs, contextual interviews, and analysis of artefacts such as for example devices, tools or paper forms that might be used as part of a person’s job. Observations can be made at home, at work, or in leisure environments. People can be studied with their family, on their own, with work colleagues, or as part of a group of friends. Often one participant may be recruited, but several more may be studied as part of that person’s family or friends. Data collection can range from a 4-5 hour contextual interview, through to following a participant for several days, or even a longitudinal study over several weeks or months to investigate, for example, how a particular product or service might be used over time. It doesn’t necessarily involve ‘full immersion’ in a person’s life: it can involve a depth interview in a person’s home or it might involve a person simply maintaining their own video diary over a period of time. Ethnographic research can provide extremely rich insight into ‘real life’ behaviour, and can be used to identify new or currently unmet user needs. This approach is most valuable at the beginning of a project when there is a need to understand real end user needs, or to understand the constraints of using a new product or service by a particular audience. Ethnographic research can provide a significant amount of qualitative data, and analysis can be time consuming. NOTE: The term ‘ethnographic’ can be misused, it’s currently a bit of a ‘buzzword’ with some agencies who may not fully understand the approach.  It is recommended that a specialist agency is used, who can demonstrate successful case studies (collecting and analysing the data). In principle, anyone could participate in this type of research. As with any user research, the recruitment of suitable participants is key. The full implications of the research should be fully explained to potential participants, as some may not feel comfortable with this level of intrusion in their lives. Depending on the study needs and the approach, but 6-8 weeks from briefing to results can provide rich insight. It may take time to build trust with participants, and the analysis period needs to be sufficient to be thorough. Ethnographic research can be expensive and time consuming, but this depends on the needs of a particular project. The benefits derived can be extremely valuable.","description":"Ethnographic research usually involves observing target users in their natural, real-world setting,  rather than in the artificial environment of a lab or focus group. The aim is to gather insight into how people live; what they do; how they use things; or what they need in their everyday or professional lives.","link":"/service-manual/user-centred-design/user-research/ethnographic-research.html"},{"title":"Expert reviews","indexable_content":"How expert reviews work Where and how you might use it Weaknesses and when not to use Timescales Cognitive walkthrough Expert reviews – also known as heuristic evaluations – are low cost usability methods that don’t involve participation of real end users. An ‘expert’ usability evaluator can assess a product (or web site) against a known set of ‘heuristics’, or usability guidelines (best practice). An alternative approach is to conduct a ‘cognitive walkthrough’ against specific use cases or scenarios. Ideally two usability experts will conduct the review independently, and then meet to discuss and agree the findings before making recommendations to the service manager. A list of widely accepted (although not necessarily validated) heuristics are provided by Jakob Nielsen or an alternative set can be found in the International Standard ISO 9241. A ‘cognitive walkthrough’ is a usability inspection that aims to identify usability issues by focusing on how easy it is for users to accomplish specific tasks with the system (or website). This method starts with identifying the user goals, then conducting a task analysis to specify the sequence of steps or actions required to achieve each task. The usability expert, along with designers and developers, then walks through these identified steps to assess the extent to which a user can achieve their goal. Both of these approaches can be used to evaluate an existing product or website, or as a quick, low cost method of evaluating a product in development. These approaches can be considered to be ‘better than nothing’, but will never provide the same quality insight as testing with real end users. However, a competent usability specialist will often identify issues that are subsequently seen in traditional user testing or in actual use. Some agencies conduct expert reviews as the first step of all usability projects. The fundamental weakness of any expert review is that it doesn’t involve use by real end users. Some people may therefore consider this to be purely opinion, but input from an experienced usability expert is better than no user testing at all. For a simple website application this could be turned around in 1-2 days.","description":"Expert reviews – also known as heuristic evaluations – are low cost usability methods that don’t involve participation of real end users. An ‘expert’ usability evaluator can assess a product (or web site) against a known set of ‘heuristics’, or usability guidelines (best practice). An alternative approach is to conduct a ‘cognitive walkthrough’ against specific use cases or scenarios.","link":"/service-manual/user-centred-design/user-research/expert-review.html"},{"title":"Focus groups, mini groups, and 1:1 interviews","indexable_content":"How they work Where and how you might use them Weaknesses and when not to use Participants Focus groups, mini groups, and 1:1 interviews involve unstructured interviews or group discussion. A focus group is normally made up of 6-12 people, although sometimes mini groups are favoured (4-5 people) as they can lead to a greater depth of discussion. 1:1 interviews are conducted by a moderator with a single respondent, and sometimes these are conducted over the phone. Interviews and discussion groups are both facilitated by a trained moderator using a specially designed topic guide in order to ensure the discussion is focused and keeps on topic. Sessions normally last 1-2 hours and will often involve participants being shown stimulus – eg communications materials to inform discussion, and sessions may include interactive techniques eg role play scenarios. Focus groups and 1:1 interviews are useful techniques for exploring and mapping reasons for attitudes and behaviour, understanding how target audiences approach issues or may be encouraged to change. They also enable participants reactions to be monitored, and the moderator to probe interesting issues when necessary. Dedicated viewing facilities are often used that enable sessions to be recorded, and interested parties can observe the sessions (often via a two way mirror) in a separate room to the participants. This often results in the research having more credibility as interested parties will have been able to view the sessions, and witness the findings first hand – this also means key issues can be acted upon quicker as observers can feed straight back to their teams. Some sessions take place in community settings, encouraging less confident/harder-to-reach audiences to attend. Focus groups can also be conducted online. The number of groups is often dictated by the budget available, but the average project will have 4 to 6 groups, with larger ones having between 10 and 20. Besides budget, other factors that influence the number of groups used are the number of potential users groups that need to be covered, and the number of geographic areas. Focus groups and 1:1 interviews can be expensive compared with other types of research, while the small samples size means that findings are not statistically significant. There is, therefore, a risk of generalising across audience groups. Analysis of the sessions can also be time consuming. Moderators also need to build a rapport with respondents. If he or she fails to do this, and can’t control the group adequately, it can result in the sessions being dominated by one or two participants, and biased data being collected. Participants can be obtained from the general population and hard to reach groups.","description":"Focus groups, mini groups, and 1:1 interviews involve unstructured interviews or group discussion.","link":"/service-manual/user-centred-design/user-research/focus-groups-mini-groups-interviews.html"},{"title":"Guerrilla testing","indexable_content":"How does guerrilla testing work Where and how you might use it Weaknesses and when not to use Number and types of participants GDS Example always ask permission first to speak with people outline briefly the purpose of the research reassure them about confidentiality keep it simple and quick consider the location and set up carefully eg a busy train station may have the footfall but people might be in too much of a hurry to spare the time providing incentives for audience participation is not required or necessary (however, depending on where you are running your sessions chocolates are often a welcome ‘thank you’ for peoples’ time) Guerrilla user testing is a low cost method of user testing. The term ‘guerrilla’ refers to its ‘out in the wild’ style, in the fact that it can be conducted anywhere eg cafe, library, train station etc, essentially anywhere where there is significant footfall. Guerrilla testing works well to quickly validate how effective a design is on its intended audience, whether certain functionality works in the way it is supposed to, or even establishing whether a brand or proposition is clear. This approach is quick and relatively easy to set up. Participants are not recruited but are ‘approached’ by those persons conducting the sessions. The sessions themselves are short, typically between 15-30 minutes and are loosely structured around specific key research objectives. The output is typically ‘qualitative’ so insight is often rich and detailed. Anyone on the service team can conduct ‘guerrilla testing’ on their site or service but often the best scenario is for a researcher to run the sessions with the designer or developer. The researcher can help with defining the tasks, moderating the sessions as well as provide a level of ‘objectivity’ by not being the person who designed or built what is being evaluated. Involving the designer / developer in the sessions enables them to see first hand ‘real’ people interacting with their product, where there are areas for improvement and how they might go about resolving any issues. This approach also does away with any lengthy reporting back. Insights can be observed, taken away and fed back into the design process almost immediately. However, a brief summary with key findings and recommendations can be written up as a more formal record. It is a method that suits the ‘agile framework’ well. Guerrilla testing can be used throughout the service lifecycle. As it is cheap to set up, run and report back on, it is a method that can be used frequently. There are a few logistics that should be taken into consideration before conducting any guerrilla testing; Remember, whenever recording sessions you must seek permission from the participant first. Provide them with a written consent form for them to sign. The key weakness of guerrilla testing as a research method it that is not statistically robust and participants may not always match your ‘target’ audience in terms of skills, expertise, knowledge. This can vary from between 6-12 participants in any given round of guerrilla testing. It is very much dependent on where and when those sessions are conducted. 16 teenagers evaluated the National Citizen Service website. Sessions were conducted in the canteen of a further education college in Nottingham. Usability and editorial findings were discovered and quickly fed back to the development teams. The whole process took 2 days.","description":"Guerrilla user testing is a low cost method of user testing. The term ‘guerrilla’ refers to its ‘out in the wild’ style, in the fact that it can be conducted anywhere eg cafe, library, train station etc, essentially anywhere where there is significant footfall.","link":"/service-manual/user-centred-design/user-research/guerrilla-testing.html"},{"title":"Lab-based user testing","indexable_content":"How lab-based user testing works Where and how you might use it Weaknesses and when not to use Number and types of participants efficiency accuracy recall emotional response User testing is a ‘qualitative’ research method, used to gauge how easy and intuitive a (product, service, website) is to use and whether it supports the needs of its intended audience. From a traditional perspective, user testing measures how well participants respond in the key areas of: This approach is combined with more ‘qualitative’ techniques to help understand the users’ motivations and attitudes as well. A product or service is usually assessed by asking small samples of the target audience to complete specific but realistic tasks in one to one sessions. The facilitator observes how well the participants are able to complete those tasks, noting down the areas or features that cause the participant problems. Often the facilitator will ask participants to think aloud whilst completing those tasks in order to understand their decision processes better. User testing can be conducted on low-fi assets (eg paper prototypes, wireframes) and hi-fi assets, including working prototypes and live digital services. It’s most effective when conducted early within the lifecycle of a product but can be conducted towards the end of the service lifecycle to validate any usability improvements. A lab environment can provide a more formal setting for conducting sessions, with most spaces offering viewing facilities enabling stakeholders and key team members to watch the sessions in real time. Usability testing is best suited to finding the big issues, essentially the problems that affect users trying to perform tasks. However, it is qualitative in nature and is not statistically robust due to the small participant numbers, per round. The lab environment may also be considered a weakness. It means testing is conducted in a very different space to that a user will commonly be in. This method is not an appropriate for understanding user behaviour or user needs eg what they might want from a product or service. Between 5-8 participants, per round of user testing is sufficient to highlight the majority of usability issues. The key is to test often.","description":"User testing is a ‘qualitative’ research method, used to gauge how easy and intuitive a (product, service, website) is to use and whether it supports the needs of its intended audience.","link":"/service-manual/user-centred-design/user-research/lab-based-user-testing.html"},{"title":"A/B and multivariate experiments","indexable_content":"Further reading This article in Wired shows how A/B experiments were used to good effect in Obama’s election campaign. This article in eConsultancy shows how multivariate experiments were used to improve conversion rates at LoveFilm. We interviewed Craig Sullivan, an industry expert on conversion optimisation. He explains when he uses A/B and multivariate experiments in the design process.","description":"We interviewed Craig Sullivan, an industry expert on conversion optimisation. He explains when he uses A/B and multivariate experiments in the design process.","link":"/service-manual/user-centred-design/user-research/multivariate-testing.html"},{"title":"Online Omnibus surveying","indexable_content":"Where and how you might use it Weaknesses and when not to use Timescales Online Omnibus surveys are an effective way of interviewing a representative number of people, in a short period of time, and for a relatively low cost. Omnibus surveys of this type use an online panel to gather the sample and, as with regular offline Omnibus surveys, costs are kept down by collating questions on a variety of subjects from a number of clients. Omnibus panels are ideal when you have key questions that you want answering, and need to reach a representative number of people quickly and cheaply. An omnibus survey is not appropriate when there are too many questions required. This would result in the survey being too long (combined with questions from other clients). An online Omnibus panel can be conducted relatively quickly, with most companies running surveys twice a week with data delivered 4 days after survey goes in to field. Some companies offer an ‘on demand’ service where surveys can start any time, although a minimum number of questions is normally required.","description":"Online Omnibus surveys are an effective way of interviewing a representative number of people, in a short period of time, and for a relatively low cost. Omnibus surveys of this type use an online panel to gather the sample and, as with regular offline Omnibus surveys, costs are kept down by collating questions on a variety of subjects from a number of clients.","link":"/service-manual/user-centred-design/user-research/online-omnibus-survey.html"},{"title":"Online research panels","indexable_content":"Where and how you might use them When not to use Timescales An online panel is a collection of pre-recruited research participants who have agreed to take part in online research over a period of time. Members of the panels are incentivised to take part, and normally rewarded through vouchers, or points that have a monetary value. As members of the panel are pre-recruited it means that they can be easily targeted by demographic, ownership and lifestyle information. As well as regular consumer panels, some companies also run business panels. Online panels are used to target representative samples of people easily, cheaply and quickly. They can also be useful to target hard to reach groups, who would otherwise be impossible to reach within a realistic timeframe. Respondents completing research in the shortest amount of time is a common problem, as this indicates that they have not given full consideration to the research that they are taking part in. Most panel companies reduce this prospect by monitoring how respondents complete the research (by time completion, and by the way they answer certain questions) and remove any suspicious respondents from their database. In addition to this, most panel companies will ensure that their respondents only complete four to six surveys a year. As the sample is ‘on tap’ online panels can be used to turn round projects very quickly. When used for remote user testing, fieldwork is normally complete within three to four days.","description":"An online panel is a collection of pre-recruited research participants who have agreed to take part in online research over a period of time. Members of the panels are incentivised to take part, and normally rewarded through vouchers, or points that have a monetary value.\nAs members of the panel are pre-recruited it means that they can be easily targeted by demographic, ownership and lifestyle information. As well as regular consumer panels, some companies also run business panels.","link":"/service-manual/user-centred-design/user-research/online-research-panels.html"},{"title":"Remote usability/summative testing (quantitative)","indexable_content":"How remote usability testing works Where and how you might use it Weaknesses and when not to use Number and types of participants Examples produces results quickly is relatively cheap compared with conducting face to face user testing at scale produces statistically significant results removes moderator bias Remote usability testing takes place outside the lab with users participating in their own home, using their own computers, and with no third party moderator present. Each session usually includes tasks being given to see how users interact with the website/online service and are followed up with a series of questions about their perceptions and how easy it was to complete the tasks. As there is no moderator present, special software is used in order to record the user’s interactions with the website/online service. While traditional lab testing focuses of gathering rich and detailed information, remote usability testing aims to test with large numbers of users and produce statistically significant results. It is recommended that remote testing is not conducted in isolation, and face to face testing is also completed. Remote usability testing can be used to test both website content, and online services. Testing content normally involves people completing task based on the online content, while online services are normally tested by asking users to complete a task using the online tool or transaction (Jobseekers Allowance, Driving Licence etc). Success is measured on whether the user can complete the tasks/transaction. Benchmarks on new and existing products are gathered so that completion rates (and other success measures) be collected and performance monitored. The key advantage of remote usability testing are that it: Although remote usability provides testing with large numbers of people, findings can lack depth as they focus on what people do, and not why they have done it. When testing new products the tasks can seem artificial, and this is increased when testing is conducted with a panel of users, instead of real users on a live site. Remote usability testing should not be used in isolation and it is recommended that it is used in conjunction with face to face testing with real users. In order to test new products that are still in development it is necessary to engage an online panel in order to recruit participants. Existing products and services can be testing via the live website. It is recommended that 400 responses are collected so that results are robust. An example from the second round of Inside Government usability testing (PDF, 2MB)","description":"Remote usability testing takes place outside the lab with users participating in their own home, using their own computers, and with no third party moderator present.","link":"/service-manual/user-centred-design/user-research/remote-usability.html"},{"title":"Same day user testing (online qualitative)","indexable_content":"How same day user testing works Where and how you might use it Weaknesses and when not to use Number and types of participants fits with an agile way of working produces quick results can be relatively cheap compared with conducting face to face user testing removes moderator bias Like standard remote usability testing, same day user testing also takes place outside the lab with users participating in their own home, using their own computers, and with no third party moderator present. Where standard remote usability testing focuses on large numbers, and providing statistically significant findings, this type of testing enables you to get rich, qualitative data from respondents that would normally only be possible by observing users in person. As there is no moderator present, each session is recorded via screen capture software, that records the whole session, including the user talking about what they are doing, why they are doing it, and how they feel about it etc. All interviews are conducted in one hour, with the video sessions, and the answers to the follow up questions made available immediately. Analysis is then conducted in house. Rapid 1:1 testing can be used to test both website content, and online services. Each session involves participants completing a series of online tasks to see how they use the website or tool to find information and/or complete transactions. Each participant is then asked a few follow up questions focusing on overall satisfaction with the site and the experience it provided. The key advantages of remote usability testing are that it: Although this type of remote usability testing can provide depth of content, the lack of moderation can result in respondents veering off topic and the analysis be quite lengthy. Also, respondents can sometimes appear to be ‘professional testers’, and therefore can perform tasks with greater ease than other users. Testing is normally conducted using the online panel that is available through the panel company. As with regular lab based testing, it is recommended that you test with 8-12 users.","description":"Like standard remote usability testing, same day user testing also takes place outside the lab with users participating in their own home, using their own computers, and with no third party moderator present.","link":"/service-manual/user-centred-design/user-research/same-day-user-testing.html"},{"title":"Survey sampling methodologies","indexable_content":"What to consider when sampling Further reading When conducting quantitative research it is essential that all findings are statistically valid, so that there is confidence in the findings and inferences can be made from the sample to the population. This process of collecting information from a sample is referred to as ‘sampling’ and enables researchers to understand the views, and needs of a user base, without interviewing the whole user population. Consideration needs to be given to how the sample is collected (who, how, where, when) and the size of the sample collected for each study. A large sample size can be more accurate and provide greater confidence in the data collected, however a large sample is not required for all research projects. The sample method chosen should consider the size and scale of the project, the sub-group analysis required, and balance the robustness of the approach that with the time, money, and resource available to ensure its fit for purpose. Sampling can be a complex process to understand, but on most occasions a sample of 400-500 will be sufficient for most in-house studies – again this is dependent on the level of subgroup analysis needed and also the penetration of the target group in the population. The greater the number of population groups that need to be analysed may increase the sample size needed, as typically for findings to be statistically valid a minimum of 100 in a subgroup is required. A larger overall sample of 1000+ is normally required if findings need to be nationally representative. This guide to sampling (PDF, 885kb) was written by the National Audit Office, and although it was first published in 2001, still provides a helpful introduction to sampling methodologies.","description":"When conducting quantitative research it is essential that all findings are statistically valid, so that there is confidence in the findings and inferences can be made from the sample to the population.","link":"/service-manual/user-centred-design/user-research/sampling-methodologies.html"},{"title":"Sentiment analysis","indexable_content":"How sentiment analysis works Weaknesses and when not to be used Sentiment analysis is a method used to analyse high volumes of verbatim comments from users in order to help easily understand the attitude and tone of users’ comment. The method uses tailored software to analyse user comments and structure them in a manner that can be used to understand what user ‘feel’ towards a product or service. This enables positive and negative comments to be grouped so that actions can be assigned to resolve problems and issues raised by users. Some of the free tools available can group comments very broadly, and not enable the level of granularity that enables the analysis to be useful and actionable. Furthermore, comments can sometimes be analysed incorrectly. This is especially the case when slang is used, or phrases are not meant literally (when negative words and phrases are meant positively, or positive comment are meant ironically). This is a notoriously difficult technique for anything beyond broad statements, and requires a very large sample size both of seed (already analysed) material and of commentary from each user. It’s not much use on twitter comments, for example, because they’re so short.","description":"Sentiment analysis is a method used to analyse high volumes of verbatim comments from users in order to help easily understand the attitude and tone of users’ comment.","link":"/service-manual/user-centred-design/user-research/sentiment-analysis.html"},{"title":"Survey design","indexable_content":"What to consider when creating a survey GDS Example Further reading Questions Common survey question types introduction – purpose of the survey, why it’s important that people take part. screener questions KPI questions – questions that benchmark performance follow up demographic questions – additional respondent information eg working/not working, salary, location etc Male Female Prefer not to answer Was visiting the site for more than one reason Information I was looking for was spread over more than one area of the site Couldn’t find what I was looking for in one area, so went into other areas Stumbled into other interesting areas of the site Just browsing other areas of the site Other (please specify) Strongly agree Agree Neither agree nor disagree Disagree Strongly disagree Don’t know The abundance of free survey tools makes it cheap and easy for user research teams to produce their own surveys. However, thought still needs to be given to the survey design, understanding the goal(s) of the survey , and how the results will be used. In order to increase the effectiveness of the survey, it is important to include an introduction that explains the purpose of the survey to potential respondents. Make it clear what the survey will be used for, and how it will help improve the service. As it’s unlikely that it will be possible to include monetary incentives, it’s important the introduction is worded in a manner that increases the likelihood that people will want to take part. It is also recommended that people are informed of how long the survey will take to complete as this will reduce drop out rate (people who start the survey but don’t finish). Screener questions should be placed at the beginning of surveys to ensure that the correct people take the survey eg demographic information, reason for visiting etc. Quotas may also be set so that you can control the number of people taking the survey that fall into a specific demographic group or audience type. A typical user survey will be structured in the following manner: Surveys are normally composed of the following types of questions: Single response (these type of questions allow the respondent to select just one answer) eg: Q. What is your gender? Multiple choice questions (respondents can select more than one answer) eg: Q. Why did you visit more than one area of the site today (please tick all that apply)? Open ended questions (respondents can write a text response) Q. You rated ‘ease of using the site’ as fair or poor. Please tell us why you gave this rating? Scale questions (respondents are asked if they agree/disagree with a statement) Q. The site was easy to use Once you have decided on the questions that you want to be included, it is necessary to set up the question logic so that respondents are routed through the questionnaire correctly. It is recommended that all surveys are tested thoroughly to ensure the correct data data is collected. GOV.UK Survey (PDF, 213KB) University of Leeds – Guide to design of questionnaires","description":"The abundance of free survey tools makes it cheap and easy for user research teams to produce their own surveys. However, thought still needs to be given to the survey design, understanding the goal(s) of the survey , and how the results will be used.","link":"/service-manual/user-centred-design/user-research/survey-design.html"},{"title":"User research briefs","indexable_content":"What it should contain Example See also A Research brief is a document written to explain research requirements and enable research to be procured via third party agencies. A research brief will outline the research objectives, and why the work is required. Agencies respond to the brief and recommend suitable ways to conduct the work, and address the research objectives. A brief should contain the following information: Background – this should include a background of the organisation that wants to conduct the research, what relevant research has been conducted previously (or is being conducted), and the overall need for the piece of research that’s being procured. Business objectives - this should include an explanation of the business objectives that the research is addressing. This is essential as it explains why the research is required by the business. Research objectives - the research objectives should outline what the research is trying to achieve. Target Audience/Participants - this should identify who you want to talk to, and focus on the target audience for the product/service. It is also common to include key demographic information, and where, geographically, the research is to be conducted. Preferred Approach/Methodology - it is recommended that research agencies consider the objective of the research and recommend the most suitable methodology, however, it is common practice for in-house research teams to provide an idea of how they think the research could be conducted. Cost - this section should state the budget available for the research. It is suggested that research agencies should be asked to provide a range of price options, so that insight teams can choose the most appropriate approach for their needs. Deliverables - this should outline how the research is to be delivered eg presentation, workshop etc. Timings - this should specify when the research is to be delivered. Project team - this section should specify who is on the project team and their contact details. IDA research brief (PDF, 177KB) Working with specialist suppliers","description":"A Research brief is a document written to explain research requirements and enable research to be procured via third party agencies. A research brief will outline the research objectives, and why the work is required.","link":"/service-manual/user-centred-design/user-research/user-research-briefs.html"},{"title":"User research surveys","indexable_content":"How surveys work Weaknesses and when not to use Participants Online web surveys Email survey Face to face interviews Telephone interviews don’t allow much time or context for considered replies leave room for questions to be misinterpreted have a completion bias – one common criticism is that they are only completed by people who are either very satisfied or very dissatisfied Surveys contain closed ended questions with fixed responses such as ‘yes’, ‘no’, ‘very satisfied’ to ‘very dissatisfied’, ‘excellent’ to ‘poor’ etc, and open ended questions that allow respondents to provide written responses. Surveys can be conducted either face to face by a trained moderator or through self-completion (online, postal etc) in order to quantify thoughts/beliefs, behaviours, ownership etc. Surveys can be a relatively cheap and quick way of gathering and quantifying public opinion and monitoring change over time. Surveys can be used to size a market to find out how many people own a product, or take part in an activity eg how many people use the Internet in the UK. They can be also used to understand usage and attitudes towards a product or activity eg how often people use the Internet, how they access it, why the use it etc. Complex analysis can be also be conducted to understand key drivers of attitude/behaviour, segmentation of audiences, and trade offs between different opinions etc. Surveys provide a snapshot of opinion, and are a useful method for monitoring uptake, usage and attitudes over time. Self-completion surveys used to obtain information from web users. Surveys are activated when a user interacts with a page – this can either be through arriving/leaving a page, or clicking on a link. Software is used to control the  proportion of users who are invited to take a specific survey. In order to increase completion rates, online surveys are normally no longer than 5 to 10 minutes in length (approximately 20 to 25 questions). These are identical in set up and design to online web surveys, except participants are asked to complete them via an email, and not at random when they visit a website. Email surveys are often more targeted as participants can be identified by information already known about the them eg demographic, information usage etc. Online or email surveys are normally the cheapest method as these can be conducted in-house using inexpensive or free survey software, inviting people to take part from their own website, or using a database of email addresses. Like telephone interviews, face to face interviews are structured and conducted by a trained moderator. Unlike other survey methods, respondents may be shown limited stimulus – eg an advert, a leaflet cover or a website design. Face to face interviews also enable researchers to target respondents more easily by demographic, geographic and socio-economic group. Structured interviews that are conducted over the phone by a trained moderator. Telephone and face interviews are the most costly methodology and they are normally conducted by a third party agency. Although surveys can be a quick and cheap manner in which to conduct user research, they often: All surveys, regardless of methodology, should use a robust sample that is nationally representative or representative of an audience of interest. Most nationally representative surveys are 1,000 people or more, but can be significantly larger if there are a number of distinct user groups that are being targeted. It is important that the delivery channel used considers the target audience. For example in the recent Digital Landscape research an online and face-to-face methodology was used as both online and offline audiences needed to be interviewed – an online only methodology would have excluded those who are offline, and the opinions of this audience omitted from the research. It is also important to note that performing research with hard-to-reach groups can be more time consuming and costly (eg BME, people with disabilities).","description":"Surveys contain closed ended questions with fixed responses such as ‘yes’, ‘no’, ‘very satisfied’ to ‘very dissatisfied’, ‘excellent’ to ‘poor’ etc, and open ended questions that allow respondents to provide written responses.","link":"/service-manual/user-centred-design/user-research/user-research-surveys.html"},{"title":"User research tools","indexable_content":"Survey tools Remote user testing tools Card sorting tools Online focus groups Community groups Text analysis tool    Please note: Examples are illustrative, rather than recommendations, and we suggest checking G-Cloud regularly to see if any similar services are available using that framework.  There are a number of good tools that can help user research teams conduct in-house research quickly and effectively. Below is a high level overview of what these enable, for people unfamiliar with user testing. Online tools are a good way in which to conduct in-house research cheaply and effectively. Conducting research in-house enables user research teams get closer to the data, and have a good understanding of the user. This is, however, dependent on teams having adequate resource to enable analysis to be conducted properly and fed back to the relevant teams. Please note: Examples are illustrative, rather than recommendations, and we suggest checking G-Cloud regularly to see if any similar services are available using that framework. These enable the easy creation of online surveys. Features include a scripting tool that allows easy creation of different question types, the ability to add branding, advanced branching, automated reporting, automatic activation/pop script, email links etc. Some good tools are available free, however, these often have restrictions on the number of questions you can ask, the number of completes you can collect, and advance features such as API, and branding. Examples include Survey Monkey, Survey Expression, Smart Survey and Fluid Surveys. These tools enable user research teams to conduct remote usability testing in-house. These tools allow you to script the surveys easily, while also conduct the analysis to be conducted relatively simply and quickly. Most of these tools also enable you to conduct standard online surveys within the same software package. Examples include Keynote Webeffective, and User Zoom. These tools enable user research teams to conduct online card sorting in-house. Card sorting tools usually come as part of a larger survey package. Examples include User Zoom and Optimal Workshop. Enable in-house research teams to conduct focus groups online. Most also enable you to run 1:1 sessions, auto ethnography, diary studies, and micro communities. Examples include LiveMinds and VisionsLive. Enables in-house research team to manage a community of users and/or stakeholders. These normally come part of a larger online research tools package. Examples include LiveMinds and Bloomfire. Enables the analysis of large amounts of written feedback – via email, helpdesk etc. This software enables you to sort user feedback into themes, and make it easier to action. Examples include Feedback Ferret and Atlas.","description":"There are a number of good tools that can help user research teams conduct in-house research quickly and effectively. Below is a high level overview of what these enable, for people unfamiliar with user testing.","link":"/service-manual/user-centred-design/user-research/user-research-tools.html"},{"title":"Working with prototypes","indexable_content":"Building prototypes Ideas can be ugly understanding your users and their needs developing a sense of how you might serve those needs estimating the effort involved in building and running a service to do so The best way to understand a product is to try to build it. Prototyping is an essential process to get a feel for the shape and edges of a product, to begin to estimate the work involved and to provide something you can quickly test with real users. This is a vital part of a process often known as ‘product discovery’:  We built alpha.gov.uk as a prototype of what would later become the single domain www.gov.uk. It was built quickly without much concern to scalability, resilience, or any of the other considerations of a ‘real’ product, because none of those matter unless the core concept is sound. That allowed us to get feedback early and also understand some of the trickier concepts we would have to grapple with, like the fuzzy lines between different audiences, the operational processes that would be required, and so on. Prototyping can start on paper with sketches. Hand-drawn sketches of what a service might involve are a good way to begin thinking things through. We encourage everyone to get to running code as quickly as possible. It’s only when you start working in the same medium your users will be using (for online services that’s generally a web browser, but it may also be via an API (application programming interface)) that you can really understand the experience you need to provide. The smart answer format for GOV.UK began as a series of paper sketches refined over a week by a small team. That process gave them a good sense of the boundaries of the problem they were trying to solve.  As quickly as possible we prototyped the format using HTML and javascript so that it could be experienced in a web browser. That revealed more constraints, such as the fact that users might expect to be able to go back and amend an answer without realising that would change their whole journey through the format. This allowed us to quickly adjust the user interface to be clearer for its users before we started the work of building out the full system. Running code also forces you to think about your integrations with other services and how they might work – do you need to send email? integrate with an existing database? and so on. A prototype will rarely actually include these integrations but having a clear picture of them is vital if you’re going to understand the real effort involved in building and operating the service.","description":"The best way to understand a product is to try to build it. Prototyping is an essential process to get a feel for the shape and edges of a product, to begin to estimate the work involved and to provide something you can quickly test with real users.","link":"/service-manual/user-centred-design/working-with-prototypes.html"}]
